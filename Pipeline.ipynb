{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn import preprocessing\n",
    "from scipy.io import loadmat\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Model, activations, layers\n",
    "from keras.layers import Dense, Reshape, Concatenate, GlobalAveragePooling1D, Embedding, LayerNormalization\n",
    "from keras.layers import MultiHeadAttention, Dropout, Add, Permute, BatchNormalization, Conv1D, Flatten\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import random\n",
    "\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_IV_LOCATION_LIST = [\n",
    "    ['-', '-', '-', 'FP1', 'FPZ', 'FP2', '-', '-', '-'],\n",
    "    ['-', '-', '-', 'AF3', '-', 'AF4', '-', '-', '-'],\n",
    "    ['F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8'],\n",
    "    ['FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8'],\n",
    "    ['T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8'],\n",
    "    ['TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8'],\n",
    "    ['P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8'],\n",
    "    ['-', 'PO7', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'PO8', '-'],\n",
    "    ['-', '-', 'CB1', 'O1', 'OZ', 'O2', 'CB2', '-', '-']\n",
    "]\n",
    "\n",
    "\n",
    "channels_division = {\n",
    "#                      1: [['-','-','-'],['-','-','-'],['F7', 'F5', 'F3']], \n",
    "#                      2: [['FP1', 'FPZ', 'FP2'],['AF3', '-', 'AF4'],['F1', 'FZ', 'F2']],\n",
    "#                      3: [['-','-','-'],['-','-','-'],['F4', 'F6', 'F8'],\n",
    "                     4: [['FT7', 'FC5', 'FC3'],['T7', 'C5', 'C3'],['TP7', 'CP5', 'CP3']],\n",
    "                     5: [['FC1', 'FCZ', 'FC2'],['C1', 'CZ', 'C2'],['CP1', 'CPZ', 'CP2']],\n",
    "                     6: [['FC4', 'FC6', 'FT8'],['C4', 'C6', 'T8'],['CP4', 'CP6', 'TP8']],\n",
    "#                      7: [['P7', 'P5', 'P3'],['-', 'PO7', 'PO5'],['-', '-', 'CB1']],\n",
    "                     8: [['P1', 'PZ', 'P2'],['PO3', 'POZ', 'PO4'],['O1', 'OZ', 'O2']],\n",
    "#                      9: [['P4', 'P6', 'P8'],['PO6', 'PO8', '-'],['CB2', '-', '-']]\n",
    "                    }\n",
    "\n",
    "labels_exp_1=['1','2','3','0','2','0','0','1','0','1','2','1','1','1','2','3','2','2','3','3','0','3','0','3']\n",
    "labels_exp_2=['2','1','3','0','0','2','0','2','3','3','2','3','2','0','1','1','2','1','0','3','0','1','3','1']\n",
    "labels_exp_3=['1','2','2','1','3','3','3','1','1','2','1','0','2','3','3','0','2','3','0','0','2','0','1','0']\n",
    "\n",
    "test_indexes_1 = sorted([23,22,21,20,17,16,13,12])\n",
    "test_indexes_2 = sorted([23,22,21,20,19,18,16,12])\n",
    "test_indexes_3 = sorted([23,22,21,20,17,16,13,10])\n",
    "\n",
    "train_indexes_1 = [i for i in range(24) if i not in test_indexes_1]\n",
    "train_indexes_2 = [i for i in range(24) if i not in test_indexes_2]\n",
    "train_indexes_3 = [i for i in range(24) if i not in test_indexes_3]\n",
    "\n",
    "channel_order = ['FP1','FPZ','FP2','AF3','AF4','F7','F5','F3','F1','FZ','F2','F4','F6','F8','FT7','FC5','FC3','FC1',\n",
    "                 'FCZ','FC2','FC4','FC6','FT8','T7','C5','C3','C1','CZ','C2','C4','C6','T8','TP7','CP5','CP3','CP1',\n",
    "                 'CPZ','CP2','CP4','CP6','TP8','P7','P5','P3','P1','PZ','P2','P4','P6','P8','PO7','PO5','PO3','POZ',\n",
    "                 'PO4','PO6','PO8','CB1','O1','OZ','O2','CB2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = ''\n",
    "dir_paths =['eeg_feature_smooth/1/','eeg_feature_smooth/2/','eeg_feature_smooth/3/']\n",
    "\n",
    "channel_group=1\n",
    "freq_bands=[0,1,2,3,4]\n",
    "\n",
    "train_indexes_1 = [i for i in range(24)]\n",
    "train_indexes_2 = [i for i in range(24)]\n",
    "train_indexes_3 = [i for i in range(24)]\n",
    "\n",
    "def get_data(channel_group=10, freq_bands=[0,1,2,3,4], repeat = 1):\n",
    "    file_label=[]\n",
    "\n",
    "    ch_list = channels_division[channel_group]\n",
    "    channels = [[channel_order.index(ch_) for ch_ in ch] for ch in ch_list]\n",
    "    channels = [j for sub in channels for j in sub]\n",
    "\n",
    "    train_x = []\n",
    "    test_x = []\n",
    "    train_y = []\n",
    "    test_y = []\n",
    "\n",
    "    for directory in sorted(dir_paths):\n",
    "        trial_num = int(directory.split('/')[-2])\n",
    "        files = [f for f in os.listdir(os.path.join(base_dir, directory)) if os.path.isfile(os.path.join(base_dir, directory, f))]\n",
    "        f=1\n",
    "\n",
    "        for file in sorted(files):\n",
    "            subject_num = int(file.split('_')[0])\n",
    "            df = loadmat(directory+file)\n",
    "            columns=list(df.keys())\n",
    "\n",
    "\n",
    "            i=3\n",
    "            subset=[]\n",
    "\n",
    "            while i<99:\n",
    "                subset.append(columns[i])\n",
    "                i=i+4\n",
    "\n",
    "            if directory == 'eeg_feature_smooth/1/':\n",
    "                indexes = sorted(train_indexes_1)\n",
    "            if directory == 'eeg_feature_smooth/2/':\n",
    "                indexes = sorted(train_indexes_2)\n",
    "            if directory == 'eeg_feature_smooth/3/':\n",
    "                indexes = sorted(train_indexes_3)\n",
    "            \n",
    "            for r in range(1):\n",
    "                for i in indexes:\n",
    "                    col = subset[i]\n",
    "                    trial = []\n",
    "                    for freq in freq_bands:\n",
    "                        frequency = []\n",
    "                        for chn in channels:\n",
    "                            a1= df[col][chn,:,freq]\n",
    "                            padded_array1 = np.zeros((64, ))\n",
    "                            shape1 = np.shape(a1)\n",
    "                            padded_array1[:shape1[0],] = a1\n",
    "                            frequency.append(padded_array1)\n",
    "\n",
    "                        if r == 0:\n",
    "                            trial.append(frequency)\n",
    "                        else:\n",
    "                            data = list(range(0, len(frequency)))\n",
    "                            random.shuffle(data)\n",
    "                            k1, k2 = data[:2]\n",
    "                            frequency[k1], frequency[k2] = frequency[k2], frequency[k1]\n",
    "                            trial.append(frequency)\n",
    "#                         trial.append(frequency)\n",
    "\n",
    "                    train_x.append(trial)\n",
    "\n",
    "                if directory == 'eeg_feature_smooth/1/':\n",
    "                    labels = [labels_exp_1[j] for j in indexes]\n",
    "                    train_y.extend(labels)\n",
    "                if directory == 'eeg_feature_smooth/2/':\n",
    "                    labels = [labels_exp_2[j] for j in indexes]\n",
    "                    train_y.extend(labels)\n",
    "                if directory == 'eeg_feature_smooth/3/':\n",
    "                    labels = [labels_exp_3[j] for j in indexes]\n",
    "                    train_y.extend(labels)\n",
    "    \n",
    "            if directory == 'eeg_feature_smooth/1/':\n",
    "                indexes = sorted(test_indexes_1)\n",
    "            if directory == 'eeg_feature_smooth/2/':\n",
    "                indexes = sorted(test_indexes_2)\n",
    "            if directory == 'eeg_feature_smooth/3/':\n",
    "                indexes = sorted(test_indexes_3)\n",
    "\n",
    "            for i in indexes:\n",
    "                col = subset[i]\n",
    "                trial = []\n",
    "                for freq in freq_bands:\n",
    "                    frequency = []\n",
    "                    for chn in channels:\n",
    "                        a1= df[col][chn,:,freq]\n",
    "                        padded_array1 = np.zeros((64, ))\n",
    "                        shape1 = np.shape(a1)\n",
    "                        padded_array1[:shape1[0],] = a1\n",
    "                        frequency.append(padded_array1)\n",
    "\n",
    "                    trial.append(frequency)\n",
    "\n",
    "                test_x.append(trial)\n",
    "\n",
    "            if directory == 'eeg_feature_smooth/1/':\n",
    "                labels = [labels_exp_1[j] for j in indexes]\n",
    "                test_y.extend(labels)\n",
    "            if directory == 'eeg_feature_smooth/2/':\n",
    "                labels = [labels_exp_2[j] for j in indexes]\n",
    "                test_y.extend(labels)\n",
    "            if directory == 'eeg_feature_smooth/3/':\n",
    "                labels = [labels_exp_3[j] for j in indexes]\n",
    "                test_y.extend(labels)\n",
    "    \n",
    "    train_x = np.array(train_x).transpose((0,1,2,3))  # (0,3,1,2))\n",
    "    train_x = np.array(train_x).transpose((0,1,2,3))  # (0,3,1,2))\n",
    "    \n",
    "    return np.array(train_x), np.array(train_y) #, np.array(test_x), np.array(test_y)\n",
    "\n",
    "central_x, central_Y = get_data(channel_group=4, freq_bands=freq_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 5, 9, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "central_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "freq_bands = [0,1,2,3]\n",
    "\n",
    "central_x, central_y = get_data(channel_group=4, freq_bands=freq_bands)\n",
    "\n",
    "rtemporal_x, rtemporal_y = get_data(channel_group=4, freq_bands=freq_bands)\n",
    "\n",
    "parietal_x, parietal_y = get_data(channel_group=4, freq_bands=freq_bands)\n",
    "\n",
    "occipital_x, occipital_y = get_data(channel_group=4, freq_bands=freq_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Electrode Patch encoder\n",
    "class LinearEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(LinearEmbedding, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        self.expand = expand\n",
    "        # Create class token\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        class_token = w_init(shape=(1, projection_dim), dtype=\"float32\")\n",
    "        self.class_token = tf.Variable(initial_value=class_token, trainable=True)\n",
    "        # Dense layer for linear transformation of electrode patches (Map to constant size De)\n",
    "        self.projection = Dense(projection_dim)\n",
    "        # Embedding layer for positional embeddings\n",
    "        self.position_embedding = Embedding(input_dim=num_patches+1, output_dim=projection_dim)\n",
    "        self.dropout = Dropout(0.3)  # 0.1\n",
    "\n",
    "    def call(self, patch, *kwargs):\n",
    "        # For brain-region-level spatial learning\n",
    "        batch = tf.shape(patch)[0]\n",
    "        class_token = tf.tile(self.class_token, multiples=[batch, 1])\n",
    "        class_token = tf.reshape(class_token, (batch, 1, self.projection_dim))\n",
    "        patches_embed = self.projection(patch)\n",
    "        patches_embed = self.dropout(patches_embed)\n",
    "        patches_embed = tf.concat([class_token, patches_embed], 1)\n",
    "        # calculate position embeddings\n",
    "        positions = tf.range(start=0, limit=self.num_patches + 1, delta=1)\n",
    "        positions_embed = self.position_embedding(positions)\n",
    "        # Add positions to patches\n",
    "        encoded = patches_embed + positions_embed\n",
    "        return encoded\n",
    "\n",
    "# MLP\n",
    "class MLP(layers.Layer):\n",
    "    def __init__(self, hidden_states, output_states, dropout=dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dense1 = Dense(hidden_states, activation=tf.nn.gelu)  #gelu\n",
    "        self.dense2 = Dense(output_states, activation=tf.nn.gelu)  #gelu\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def call(self, x, *kwargs):\n",
    "        hidden = self.dense1(x)\n",
    "        dr_hidden = self.dropout(hidden)\n",
    "        output = self.dense2(dr_hidden)\n",
    "        dr_output = self.dropout(output)\n",
    "        return dr_output\n",
    "\n",
    "# Transformer Encoder Block\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, model_dim, num_heads=k, msa_dimensions=Dh):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.layernormalization1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=msa_dimensions, dropout=dropout_rate)\n",
    "        self.layernormalization2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = MLP(hidden_states=model_dim * 4, output_states=model_dim)\n",
    "\n",
    "    def call(self, x, *kwargs):\n",
    "        # layer normalization 1.\n",
    "        x1 = self.layernormalization1(x)  # encoded_patches\n",
    "        # create a multi-head attention layer.\n",
    "        attention_output = self.attention(x1, x1)\n",
    "        # skip connection 1.\n",
    "        x2 = Add()([attention_output, x])  # encoded_patches\n",
    "        # layer normalization 2.\n",
    "        x3 = self.layernormalization2(x2)\n",
    "        # mLP.\n",
    "        x3 = self.mlp(x3)\n",
    "        # skip connection 2.\n",
    "        y = Add()([x3, x2])\n",
    "        return y\n",
    "\n",
    "#  Transformer Encoder Block x L Repeat\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, model_dim, num_blocks):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.blocks = [TransformerEncoderBlock(model_dim, num_blocks) for _ in range(num_blocks)]\n",
    "\n",
    "    def call(self, x, *kwargs):\n",
    "        # create a [batch_size, projection_dim] tensor.\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dr = 64  # embedding dimension (brain - region level)\n",
    "Lr = 8  # no of encoder (brain - region level)\n",
    "dropout_rate = 0.4  # Dropout rate\n",
    "k = 16  # num of heads in MSA 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1536)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 4, 9, 64)]   0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4, 9, 64)]   0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 4, 9, 64)]   0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4, 9, 64)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 3, 8, 16)     4112        ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 3, 8, 16)    64          ['conv2d[0][0]',                 \n",
      " alization)                                                       'conv2d[1][0]',                 \n",
      "                                                                  'conv2d[2][0]',                 \n",
      "                                                                  'conv2d[3][0]']                 \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 384)          0           ['batch_normalization[1][0]']    \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 384)          0           ['batch_normalization[2][0]']    \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 384)          0           ['batch_normalization[3][0]']    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 384)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 384)          0           ['flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1536)         0           ['flatten[0][0]',                \n",
      "                                                                  'dropout[0][0]',                \n",
      "                                                                  'dropout[1][0]',                \n",
      "                                                                  'dropout[2][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 1536)        6144        ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1536)         0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          196736      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128)         512         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            516         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 208,084\n",
      "Trainable params: 204,724\n",
      "Non-trainable params: 3,360\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def HierarchicalTransformer():\n",
    "    N = 9\n",
    "    timesteps = 64\n",
    "    d=4\n",
    "    \n",
    "    conv = layers.Conv2D(16, (2,2), activation = 'relu')\n",
    "    batch_norm = BatchNormalization()\n",
    "    dropout = Dropout(0.5)\n",
    "\n",
    "    # 4. Brain region (Central)\n",
    "    input_1 = keras.Input(shape=(d, 9, timesteps))\n",
    "    electrode_patch_c = conv(input_1)\n",
    "    electrode_patch_c = batch_norm(electrode_patch_c)\n",
    "    electrode_patch_c = Flatten()(electrode_patch_c)\n",
    "    output_c = (electrode_patch_c)\n",
    "\n",
    "    # 5. Brain region (Right Temporal)\n",
    "    input_2 = keras.Input(shape=(d, 9, timesteps))\n",
    "    electrode_patch_rt = conv(input_2)\n",
    "    electrode_patch_rt = batch_norm(electrode_patch_rt)\n",
    "    electrode_patch_rt = Flatten()(electrode_patch_rt)\n",
    "    output_rt = dropout(electrode_patch_rt)\n",
    "\n",
    "    # 6. Brain region (Left Parietal)\n",
    "    input_3 = keras.Input(shape=(d, 9, timesteps))\n",
    "    electrode_patch_lp = conv(input_3)\n",
    "    electrode_patch_lp = batch_norm(electrode_patch_lp)\n",
    "    electrode_patch_lp = Flatten()(electrode_patch_lp)\n",
    "    output_lp = dropout(electrode_patch_lp)\n",
    "\n",
    "    # 8. Brain region (Right Parietal)\n",
    "    input_4 = keras.Input(shape=(d, 9, timesteps))\n",
    "    electrode_patch_rp = conv(input_4)\n",
    "    electrode_patch_rp = batch_norm(electrode_patch_rp)\n",
    "    electrode_patch_rp = Flatten()(electrode_patch_rp)\n",
    "    output_rp = dropout(electrode_patch_rp)\n",
    "\n",
    "    xl = Concatenate(axis=1)([output_c, output_rt, output_lp, output_rp]) # 2\n",
    "    \n",
    "    print(xl.shape)\n",
    "    \n",
    "    outputs_br = LinearEmbedding(8, Dr)(xl)   # (None, 10, Dr)\n",
    "    outputs_br = TransformerEncoder(Dr, Lr)(outputs_br)\n",
    "    class_token_output = outputs_br[:, 0, :]  # (None, Dr)\n",
    "    brain_regions_embeddings = Flatten()(class_token_output)\n",
    "    brain_regions_embeddings = BatchNormalization()(brain_regions_embeddings)\n",
    "    brain_regions_embeddings = Dropout(dropout_rate)(brain_regions_embeddings)\n",
    "    brain_regions_embeddings = Dense(128, activation='relu')(brain_regions_embeddings)\n",
    "    brain_regions_embeddings = BatchNormalization()(brain_regions_embeddings)\n",
    "    brain_regions_embeddings = Dropout(dropout_rate)(brain_regions_embeddings)\n",
    "    \n",
    "    prediction = Dense(4, activation=activations.softmax)(brain_regions_embeddings)\n",
    "    \n",
    "    hslt = Model(inputs=[input_1, input_2, input_3, input_4],\n",
    "                 outputs=prediction)\n",
    "\n",
    "    return hslt\n",
    "\n",
    "\n",
    "model = HierarchicalTransformer()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross-validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1536)\n",
      "Epoch 1/300\n",
      "87/87 [==============================] - 2s 11ms/step - loss: 1.6350 - accuracy: 0.3785 - val_loss: 1.3966 - val_accuracy: 0.3704\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.2814 - accuracy: 0.4769 - val_loss: 1.3475 - val_accuracy: 0.4398\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.1792 - accuracy: 0.5093 - val_loss: 1.0144 - val_accuracy: 0.5880\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0260 - accuracy: 0.5602 - val_loss: 1.2940 - val_accuracy: 0.4213\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9329 - accuracy: 0.5961 - val_loss: 1.8059 - val_accuracy: 0.4028\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.9812 - accuracy: 0.5856 - val_loss: 0.9865 - val_accuracy: 0.6019\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9399 - accuracy: 0.6227 - val_loss: 1.0805 - val_accuracy: 0.4954\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8897 - accuracy: 0.6285 - val_loss: 0.9439 - val_accuracy: 0.6019\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9122 - accuracy: 0.6215 - val_loss: 0.9761 - val_accuracy: 0.5972\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8262 - accuracy: 0.6528 - val_loss: 1.1212 - val_accuracy: 0.5602\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9123 - accuracy: 0.6319 - val_loss: 1.2569 - val_accuracy: 0.4907\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8894 - accuracy: 0.6285 - val_loss: 0.7392 - val_accuracy: 0.6944\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8257 - accuracy: 0.6528 - val_loss: 0.7376 - val_accuracy: 0.6806\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7706 - accuracy: 0.6701 - val_loss: 0.9607 - val_accuracy: 0.6667\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8287 - accuracy: 0.6597 - val_loss: 1.0901 - val_accuracy: 0.6065\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8202 - accuracy: 0.6632 - val_loss: 1.2968 - val_accuracy: 0.4907\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7930 - accuracy: 0.6562 - val_loss: 0.6972 - val_accuracy: 0.7037\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7878 - accuracy: 0.6562 - val_loss: 0.6780 - val_accuracy: 0.6759\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7787 - accuracy: 0.6644 - val_loss: 0.6693 - val_accuracy: 0.7037\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7440 - accuracy: 0.6725 - val_loss: 0.6934 - val_accuracy: 0.6991\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7271 - accuracy: 0.6806 - val_loss: 0.7275 - val_accuracy: 0.6667\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7237 - accuracy: 0.6898 - val_loss: 0.8822 - val_accuracy: 0.5370\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7150 - accuracy: 0.6701 - val_loss: 0.6538 - val_accuracy: 0.6852\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7340 - accuracy: 0.6678 - val_loss: 0.6671 - val_accuracy: 0.6991\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7812 - accuracy: 0.6701 - val_loss: 0.9481 - val_accuracy: 0.6019\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8518 - accuracy: 0.6296 - val_loss: 0.7080 - val_accuracy: 0.6574\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8086 - accuracy: 0.6354 - val_loss: 0.6807 - val_accuracy: 0.6991\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7416 - accuracy: 0.6690 - val_loss: 1.1085 - val_accuracy: 0.6435\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8220 - accuracy: 0.6435 - val_loss: 0.7018 - val_accuracy: 0.6713\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7261 - accuracy: 0.6690 - val_loss: 0.7433 - val_accuracy: 0.6944\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7619 - accuracy: 0.6516 - val_loss: 1.0166 - val_accuracy: 0.5833\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7348 - accuracy: 0.6759 - val_loss: 0.8808 - val_accuracy: 0.5602\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7159 - accuracy: 0.6933 - val_loss: 0.6321 - val_accuracy: 0.6667\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6910 - accuracy: 0.6979 - val_loss: 0.6402 - val_accuracy: 0.6991\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.7323 - accuracy: 0.6632 - val_loss: 0.6256 - val_accuracy: 0.7083\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6855 - accuracy: 0.6898 - val_loss: 8.0602 - val_accuracy: 0.2870\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6976 - accuracy: 0.7025 - val_loss: 0.6434 - val_accuracy: 0.6991\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6787 - accuracy: 0.6968 - val_loss: 1.0534 - val_accuracy: 0.5417\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6896 - accuracy: 0.6806 - val_loss: 0.6634 - val_accuracy: 0.6944\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6612 - accuracy: 0.6898 - val_loss: 0.5702 - val_accuracy: 0.7130\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6947 - accuracy: 0.6875 - val_loss: 0.5975 - val_accuracy: 0.7222\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6968 - accuracy: 0.6863 - val_loss: 0.9436 - val_accuracy: 0.6204\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6737 - accuracy: 0.6840 - val_loss: 0.6999 - val_accuracy: 0.6713\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6827 - accuracy: 0.7002 - val_loss: 0.9467 - val_accuracy: 0.6435\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6714 - accuracy: 0.6817 - val_loss: 0.6014 - val_accuracy: 0.7454\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6753 - accuracy: 0.7083 - val_loss: 0.6436 - val_accuracy: 0.7083\n",
      "Epoch 47/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6827 - accuracy: 0.6933 - val_loss: 0.7094 - val_accuracy: 0.6806\n",
      "Epoch 48/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7000 - accuracy: 0.6863 - val_loss: 0.6142 - val_accuracy: 0.7037\n",
      "Epoch 49/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6782 - accuracy: 0.6933 - val_loss: 0.7858 - val_accuracy: 0.6991\n",
      "Epoch 50/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6933 - accuracy: 0.6887 - val_loss: 0.7251 - val_accuracy: 0.6991\n",
      "Epoch 51/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6652 - accuracy: 0.6887 - val_loss: 0.5410 - val_accuracy: 0.7315\n",
      "Epoch 52/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8147 - accuracy: 0.6296 - val_loss: 1.0867 - val_accuracy: 0.6343\n",
      "Epoch 53/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6725 - accuracy: 0.6910 - val_loss: 0.5693 - val_accuracy: 0.7176\n",
      "Epoch 54/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6777 - accuracy: 0.6991 - val_loss: 0.6665 - val_accuracy: 0.7269\n",
      "Epoch 55/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6606 - accuracy: 0.7049 - val_loss: 0.8508 - val_accuracy: 0.7454\n",
      "Epoch 56/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6758 - accuracy: 0.7025 - val_loss: 0.5828 - val_accuracy: 0.6852\n",
      "Epoch 57/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6579 - accuracy: 0.6968 - val_loss: 0.8619 - val_accuracy: 0.7083\n",
      "Epoch 58/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6376 - accuracy: 0.7176 - val_loss: 0.6213 - val_accuracy: 0.7083\n",
      "Epoch 59/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6409 - accuracy: 0.7118 - val_loss: 0.6829 - val_accuracy: 0.6991\n",
      "Epoch 60/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7108 - accuracy: 0.6817 - val_loss: 0.5837 - val_accuracy: 0.7407\n",
      "Epoch 61/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6488 - accuracy: 0.6921 - val_loss: 0.5359 - val_accuracy: 0.7315\n",
      "Epoch 62/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6645 - accuracy: 0.6956 - val_loss: 0.6033 - val_accuracy: 0.7222\n",
      "Epoch 63/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6475 - accuracy: 0.7014 - val_loss: 0.6279 - val_accuracy: 0.6852\n",
      "Epoch 64/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6749 - accuracy: 0.6794 - val_loss: 0.5572 - val_accuracy: 0.6759\n",
      "Epoch 65/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6507 - accuracy: 0.6933 - val_loss: 0.5140 - val_accuracy: 0.7037\n",
      "Epoch 66/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7008 - accuracy: 0.6968 - val_loss: 0.5273 - val_accuracy: 0.7130\n",
      "Epoch 67/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6587 - accuracy: 0.6852 - val_loss: 0.7847 - val_accuracy: 0.6343\n",
      "Epoch 68/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6668 - accuracy: 0.7118 - val_loss: 0.7121 - val_accuracy: 0.6667\n",
      "Epoch 69/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6560 - accuracy: 0.6863 - val_loss: 0.7223 - val_accuracy: 0.7222\n",
      "Epoch 70/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5972 - accuracy: 0.7292 - val_loss: 0.5829 - val_accuracy: 0.7130\n",
      "Epoch 71/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6435 - accuracy: 0.6991 - val_loss: 0.5711 - val_accuracy: 0.7222\n",
      "Epoch 72/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6450 - accuracy: 0.7164 - val_loss: 0.7127 - val_accuracy: 0.6806\n",
      "Epoch 73/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6321 - accuracy: 0.7118 - val_loss: 0.6829 - val_accuracy: 0.6713\n",
      "Epoch 74/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6812 - accuracy: 0.6944 - val_loss: 0.5605 - val_accuracy: 0.6944\n",
      "Epoch 75/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7001 - accuracy: 0.6817 - val_loss: 22.6523 - val_accuracy: 0.2407\n",
      "Epoch 76/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6855 - accuracy: 0.6771 - val_loss: 0.5609 - val_accuracy: 0.6991\n",
      "Epoch 77/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6323 - accuracy: 0.7234 - val_loss: 0.6387 - val_accuracy: 0.6944\n",
      "Epoch 78/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6919 - accuracy: 0.6817 - val_loss: 0.5795 - val_accuracy: 0.7083\n",
      "Epoch 79/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6397 - accuracy: 0.7118 - val_loss: 0.5838 - val_accuracy: 0.7315\n",
      "Epoch 80/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6248 - accuracy: 0.7049 - val_loss: 0.5973 - val_accuracy: 0.7130\n",
      "Epoch 81/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6405 - accuracy: 0.6898 - val_loss: 0.7313 - val_accuracy: 0.6574\n",
      "Epoch 82/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6480 - accuracy: 0.6921 - val_loss: 0.7130 - val_accuracy: 0.6574\n",
      "Epoch 83/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6287 - accuracy: 0.7141 - val_loss: 0.6976 - val_accuracy: 0.7269\n",
      "Epoch 84/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5758 - accuracy: 0.7292 - val_loss: 0.6084 - val_accuracy: 0.7407\n",
      "Epoch 85/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6135 - accuracy: 0.7106 - val_loss: 0.6020 - val_accuracy: 0.7083\n",
      "Epoch 86/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6595 - accuracy: 0.6991 - val_loss: 0.5500 - val_accuracy: 0.6944\n",
      "Epoch 87/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6445 - accuracy: 0.7153 - val_loss: 0.5094 - val_accuracy: 0.7593\n",
      "Epoch 88/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6546 - accuracy: 0.6944 - val_loss: 0.5866 - val_accuracy: 0.7130\n",
      "Epoch 89/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6313 - accuracy: 0.7072 - val_loss: 0.5997 - val_accuracy: 0.6991\n",
      "Epoch 90/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6256 - accuracy: 0.7072 - val_loss: 0.6434 - val_accuracy: 0.6898\n",
      "Epoch 91/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6302 - accuracy: 0.6991 - val_loss: 0.6480 - val_accuracy: 0.7361\n",
      "Epoch 92/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6454 - accuracy: 0.7060 - val_loss: 0.6664 - val_accuracy: 0.6806\n",
      "Epoch 93/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6515 - accuracy: 0.7037 - val_loss: 0.5530 - val_accuracy: 0.7361\n",
      "Epoch 94/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6211 - accuracy: 0.7245 - val_loss: 0.5430 - val_accuracy: 0.7269\n",
      "Epoch 95/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6224 - accuracy: 0.7130 - val_loss: 0.5511 - val_accuracy: 0.7500\n",
      "Epoch 96/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6546 - accuracy: 0.7199 - val_loss: 0.5580 - val_accuracy: 0.6852\n",
      "Epoch 97/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6747 - accuracy: 0.7014 - val_loss: 1.1477 - val_accuracy: 0.5185\n",
      "Epoch 98/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6736 - accuracy: 0.7002 - val_loss: 0.7555 - val_accuracy: 0.6667\n",
      "Epoch 99/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6269 - accuracy: 0.6991 - val_loss: 0.6819 - val_accuracy: 0.7315\n",
      "Epoch 100/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6575 - accuracy: 0.7072 - val_loss: 0.6236 - val_accuracy: 0.6759\n",
      "Epoch 101/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6829 - accuracy: 0.7049 - val_loss: 0.5636 - val_accuracy: 0.7269\n",
      "Epoch 102/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6768 - accuracy: 0.6991 - val_loss: 0.7223 - val_accuracy: 0.6806\n",
      "Epoch 103/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6896 - accuracy: 0.6863 - val_loss: 0.5338 - val_accuracy: 0.7176\n",
      "Epoch 104/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6320 - accuracy: 0.7025 - val_loss: 0.5820 - val_accuracy: 0.7037\n",
      "Epoch 105/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5817 - accuracy: 0.7431 - val_loss: 0.8612 - val_accuracy: 0.6157\n",
      "Epoch 106/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6357 - accuracy: 0.7106 - val_loss: 0.6524 - val_accuracy: 0.7222\n",
      "Epoch 107/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6498 - accuracy: 0.7141 - val_loss: 0.9869 - val_accuracy: 0.4630\n",
      "Epoch 108/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6396 - accuracy: 0.7083 - val_loss: 0.6923 - val_accuracy: 0.6944\n",
      "Epoch 109/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6058 - accuracy: 0.7118 - val_loss: 0.5744 - val_accuracy: 0.6713\n",
      "Epoch 110/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6837 - accuracy: 0.7072 - val_loss: 0.5471 - val_accuracy: 0.6806\n",
      "Epoch 111/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6196 - accuracy: 0.7245 - val_loss: 0.5428 - val_accuracy: 0.7176\n",
      "Epoch 112/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5907 - accuracy: 0.7280 - val_loss: 0.5232 - val_accuracy: 0.7222\n",
      "Epoch 113/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6619 - accuracy: 0.6933 - val_loss: 0.5519 - val_accuracy: 0.6944\n",
      "Epoch 114/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6202 - accuracy: 0.7095 - val_loss: 0.5725 - val_accuracy: 0.7269\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6268 - accuracy: 0.6944 - val_loss: 0.5251 - val_accuracy: 0.6944\n",
      "Epoch 116/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6056 - accuracy: 0.7106 - val_loss: 0.5968 - val_accuracy: 0.6944\n",
      "Epoch 117/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6063 - accuracy: 0.7106 - val_loss: 0.5279 - val_accuracy: 0.7222\n",
      "Epoch 118/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6311 - accuracy: 0.7280 - val_loss: 0.5083 - val_accuracy: 0.7361\n",
      "Epoch 119/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6284 - accuracy: 0.7095 - val_loss: 0.4900 - val_accuracy: 0.7315\n",
      "Epoch 120/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6174 - accuracy: 0.7095 - val_loss: 0.4979 - val_accuracy: 0.7546\n",
      "Epoch 121/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6037 - accuracy: 0.7118 - val_loss: 0.5382 - val_accuracy: 0.7176\n",
      "Epoch 122/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5881 - accuracy: 0.7338 - val_loss: 0.5332 - val_accuracy: 0.7315\n",
      "Epoch 123/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5967 - accuracy: 0.7257 - val_loss: 0.6123 - val_accuracy: 0.6944\n",
      "Epoch 124/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6048 - accuracy: 0.7118 - val_loss: 0.5478 - val_accuracy: 0.6852\n",
      "Epoch 125/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6277 - accuracy: 0.6968 - val_loss: 0.6200 - val_accuracy: 0.7037\n",
      "Epoch 126/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6512 - accuracy: 0.6944 - val_loss: 8.3036 - val_accuracy: 0.4769\n",
      "Epoch 127/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6312 - accuracy: 0.7211 - val_loss: 0.5874 - val_accuracy: 0.7222\n",
      "Epoch 128/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6029 - accuracy: 0.6944 - val_loss: 0.5510 - val_accuracy: 0.7037\n",
      "Epoch 129/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6023 - accuracy: 0.7049 - val_loss: 0.6081 - val_accuracy: 0.6991\n",
      "Epoch 130/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6519 - accuracy: 0.6875 - val_loss: 0.5934 - val_accuracy: 0.7083\n",
      "Epoch 131/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5757 - accuracy: 0.7326 - val_loss: 0.5054 - val_accuracy: 0.6944\n",
      "Epoch 132/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6469 - accuracy: 0.7002 - val_loss: 0.4952 - val_accuracy: 0.7269\n",
      "Epoch 133/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5638 - accuracy: 0.7234 - val_loss: 0.6824 - val_accuracy: 0.7037\n",
      "Epoch 134/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6633 - accuracy: 0.7095 - val_loss: 0.5543 - val_accuracy: 0.7315\n",
      "Epoch 135/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6144 - accuracy: 0.7072 - val_loss: 0.9724 - val_accuracy: 0.6435\n",
      "Epoch 136/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6804 - accuracy: 0.6910 - val_loss: 0.5874 - val_accuracy: 0.6944\n",
      "Epoch 137/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6744 - accuracy: 0.7014 - val_loss: 1.6156 - val_accuracy: 0.2778\n",
      "Epoch 138/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8388 - accuracy: 0.6076 - val_loss: 0.7345 - val_accuracy: 0.6852\n",
      "Epoch 139/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7723 - accuracy: 0.6400 - val_loss: 0.6212 - val_accuracy: 0.7176\n",
      "Epoch 140/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6913 - accuracy: 0.6794 - val_loss: 0.6688 - val_accuracy: 0.6898\n",
      "Epoch 141/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7608 - accuracy: 0.6354 - val_loss: 0.7258 - val_accuracy: 0.6759\n",
      "Epoch 142/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7908 - accuracy: 0.6424 - val_loss: 0.6456 - val_accuracy: 0.6667\n",
      "Epoch 143/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7319 - accuracy: 0.6690 - val_loss: 0.6163 - val_accuracy: 0.6759\n",
      "Epoch 144/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7206 - accuracy: 0.6794 - val_loss: 0.6111 - val_accuracy: 0.6852\n",
      "Epoch 145/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7800 - accuracy: 0.6574 - val_loss: 0.6604 - val_accuracy: 0.6713\n",
      "Epoch 146/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7504 - accuracy: 0.6574 - val_loss: 0.6232 - val_accuracy: 0.6713\n",
      "Epoch 147/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7400 - accuracy: 0.6794 - val_loss: 0.6275 - val_accuracy: 0.6574\n",
      "Epoch 148/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6897 - accuracy: 0.6759 - val_loss: 0.6631 - val_accuracy: 0.6852\n",
      "Epoch 149/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6602 - accuracy: 0.7072 - val_loss: 0.6518 - val_accuracy: 0.6667\n",
      "Epoch 150/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6922 - accuracy: 0.6794 - val_loss: 0.6002 - val_accuracy: 0.7222\n",
      "Epoch 151/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6107 - accuracy: 0.7269 - val_loss: 0.5313 - val_accuracy: 0.7130\n",
      "Epoch 152/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6088 - accuracy: 0.7049 - val_loss: 0.5851 - val_accuracy: 0.7130\n",
      "Epoch 153/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5920 - accuracy: 0.7257 - val_loss: 0.5221 - val_accuracy: 0.7269\n",
      "Epoch 154/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5775 - accuracy: 0.7106 - val_loss: 0.5734 - val_accuracy: 0.7222\n",
      "Epoch 155/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6192 - accuracy: 0.7257 - val_loss: 0.5304 - val_accuracy: 0.7593\n",
      "Epoch 156/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5977 - accuracy: 0.7188 - val_loss: 0.5793 - val_accuracy: 0.7083\n",
      "Epoch 157/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6007 - accuracy: 0.7130 - val_loss: 0.5310 - val_accuracy: 0.7176\n",
      "Epoch 158/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6346 - accuracy: 0.6979 - val_loss: 0.5723 - val_accuracy: 0.7037\n",
      "Epoch 159/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5742 - accuracy: 0.7315 - val_loss: 0.5827 - val_accuracy: 0.7083\n",
      "Epoch 160/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5697 - accuracy: 0.7211 - val_loss: 0.4930 - val_accuracy: 0.7269\n",
      "Epoch 161/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5887 - accuracy: 0.7211 - val_loss: 0.5649 - val_accuracy: 0.7083\n",
      "Epoch 162/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6229 - accuracy: 0.6921 - val_loss: 0.5775 - val_accuracy: 0.6944\n",
      "Epoch 163/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6687 - accuracy: 0.6979 - val_loss: 0.4947 - val_accuracy: 0.7083\n",
      "Epoch 164/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5972 - accuracy: 0.7095 - val_loss: 0.4943 - val_accuracy: 0.7222\n",
      "Epoch 165/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5980 - accuracy: 0.7130 - val_loss: 0.5134 - val_accuracy: 0.7315\n",
      "Epoch 166/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5848 - accuracy: 0.7361 - val_loss: 0.5086 - val_accuracy: 0.7222\n",
      "Epoch 167/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5938 - accuracy: 0.7222 - val_loss: 1.4584 - val_accuracy: 0.6065\n",
      "Epoch 168/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6112 - accuracy: 0.7118 - val_loss: 0.5122 - val_accuracy: 0.7130\n",
      "Epoch 169/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6131 - accuracy: 0.7083 - val_loss: 0.5071 - val_accuracy: 0.7130\n",
      "Epoch 170/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5763 - accuracy: 0.7269 - val_loss: 0.4959 - val_accuracy: 0.7269\n",
      "Epoch 171/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5902 - accuracy: 0.7176 - val_loss: 0.5463 - val_accuracy: 0.7083\n",
      "Epoch 172/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5685 - accuracy: 0.7222 - val_loss: 0.5051 - val_accuracy: 0.7176\n",
      "Epoch 173/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5751 - accuracy: 0.7477 - val_loss: 0.4864 - val_accuracy: 0.7315\n",
      "Epoch 174/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5959 - accuracy: 0.7407 - val_loss: 0.5127 - val_accuracy: 0.6944\n",
      "Epoch 175/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5976 - accuracy: 0.7257 - val_loss: 0.4980 - val_accuracy: 0.7222\n",
      "Epoch 176/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6091 - accuracy: 0.7025 - val_loss: 0.5538 - val_accuracy: 0.6898\n",
      "Epoch 177/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6412 - accuracy: 0.7176 - val_loss: 0.5960 - val_accuracy: 0.6898\n",
      "Epoch 178/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5945 - accuracy: 0.7234 - val_loss: 0.5919 - val_accuracy: 0.6991\n",
      "Epoch 179/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6016 - accuracy: 0.7303 - val_loss: 0.5411 - val_accuracy: 0.7222\n",
      "Epoch 180/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5954 - accuracy: 0.7234 - val_loss: 0.5429 - val_accuracy: 0.7222\n",
      "Epoch 181/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6099 - accuracy: 0.7257 - val_loss: 0.5178 - val_accuracy: 0.7176\n",
      "Epoch 182/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5991 - accuracy: 0.7176 - val_loss: 0.5230 - val_accuracy: 0.7222\n",
      "Epoch 183/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5561 - accuracy: 0.7326 - val_loss: 0.5004 - val_accuracy: 0.7130\n",
      "Epoch 184/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6618 - accuracy: 0.7141 - val_loss: 0.5388 - val_accuracy: 0.7130\n",
      "Epoch 185/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5976 - accuracy: 0.7176 - val_loss: 0.5102 - val_accuracy: 0.7176\n",
      "Epoch 186/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6131 - accuracy: 0.7106 - val_loss: 0.5269 - val_accuracy: 0.7222\n",
      "Epoch 187/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6162 - accuracy: 0.6921 - val_loss: 0.5232 - val_accuracy: 0.7454\n",
      "Epoch 188/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5749 - accuracy: 0.7257 - val_loss: 0.5169 - val_accuracy: 0.7222\n",
      "Epoch 189/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6113 - accuracy: 0.7326 - val_loss: 0.5499 - val_accuracy: 0.7176\n",
      "Epoch 190/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5989 - accuracy: 0.7257 - val_loss: 0.5649 - val_accuracy: 0.6898\n",
      "Epoch 191/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5831 - accuracy: 0.7326 - val_loss: 0.5449 - val_accuracy: 0.7037\n",
      "Epoch 192/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6213 - accuracy: 0.7130 - val_loss: 0.5718 - val_accuracy: 0.6991\n",
      "Epoch 193/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5907 - accuracy: 0.7257 - val_loss: 0.5357 - val_accuracy: 0.7083\n",
      "Epoch 194/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5760 - accuracy: 0.7303 - val_loss: 0.5532 - val_accuracy: 0.7083\n",
      "Epoch 195/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5898 - accuracy: 0.7188 - val_loss: 0.5143 - val_accuracy: 0.7176\n",
      "Epoch 196/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5706 - accuracy: 0.7338 - val_loss: 0.5968 - val_accuracy: 0.7222\n",
      "Epoch 197/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5844 - accuracy: 0.7292 - val_loss: 0.6797 - val_accuracy: 0.7176\n",
      "Epoch 198/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5939 - accuracy: 0.7350 - val_loss: 0.5042 - val_accuracy: 0.7083\n",
      "Epoch 199/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5585 - accuracy: 0.7384 - val_loss: 0.4970 - val_accuracy: 0.7361\n",
      "Epoch 200/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5764 - accuracy: 0.7164 - val_loss: 0.5211 - val_accuracy: 0.7222\n",
      "Epoch 201/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5460 - accuracy: 0.7234 - val_loss: 0.4955 - val_accuracy: 0.7130\n",
      "Epoch 202/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5397 - accuracy: 0.7315 - val_loss: 0.4975 - val_accuracy: 0.7315\n",
      "Epoch 203/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5651 - accuracy: 0.7350 - val_loss: 0.5075 - val_accuracy: 0.7037\n",
      "Epoch 204/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5805 - accuracy: 0.7303 - val_loss: 0.4895 - val_accuracy: 0.7361\n",
      "Epoch 205/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5591 - accuracy: 0.7222 - val_loss: 0.5357 - val_accuracy: 0.6898\n",
      "Epoch 206/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5737 - accuracy: 0.7315 - val_loss: 0.4868 - val_accuracy: 0.7269\n",
      "Epoch 207/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5804 - accuracy: 0.7199 - val_loss: 0.6119 - val_accuracy: 0.6343\n",
      "Epoch 208/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6749 - accuracy: 0.6887 - val_loss: 0.5157 - val_accuracy: 0.7315\n",
      "Epoch 209/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6088 - accuracy: 0.7188 - val_loss: 0.5161 - val_accuracy: 0.7222\n",
      "Epoch 210/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5521 - accuracy: 0.7431 - val_loss: 0.4991 - val_accuracy: 0.7130\n",
      "Epoch 211/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5384 - accuracy: 0.7384 - val_loss: 0.4934 - val_accuracy: 0.7176\n",
      "Epoch 212/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5685 - accuracy: 0.7292 - val_loss: 0.4871 - val_accuracy: 0.7037\n",
      "Epoch 213/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5371 - accuracy: 0.7419 - val_loss: 0.5001 - val_accuracy: 0.7315\n",
      "Epoch 214/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5755 - accuracy: 0.7245 - val_loss: 0.5812 - val_accuracy: 0.6713\n",
      "Epoch 215/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5867 - accuracy: 0.7292 - val_loss: 0.5420 - val_accuracy: 0.7037\n",
      "Epoch 216/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5622 - accuracy: 0.7211 - val_loss: 0.5526 - val_accuracy: 0.7222\n",
      "Epoch 217/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6233 - accuracy: 0.7072 - val_loss: 0.5680 - val_accuracy: 0.7037\n",
      "Epoch 218/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5306 - accuracy: 0.7488 - val_loss: 0.5302 - val_accuracy: 0.7176\n",
      "Epoch 219/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5843 - accuracy: 0.7083 - val_loss: 0.4988 - val_accuracy: 0.7315\n",
      "Epoch 220/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5644 - accuracy: 0.7257 - val_loss: 0.5006 - val_accuracy: 0.7130\n",
      "Epoch 221/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5445 - accuracy: 0.7384 - val_loss: 0.5016 - val_accuracy: 0.7176\n",
      "Epoch 222/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5664 - accuracy: 0.7141 - val_loss: 0.5421 - val_accuracy: 0.7361\n",
      "Epoch 223/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5841 - accuracy: 0.7199 - val_loss: 0.5222 - val_accuracy: 0.7315\n",
      "Epoch 224/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5775 - accuracy: 0.7106 - val_loss: 0.5318 - val_accuracy: 0.7176\n",
      "Epoch 225/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5675 - accuracy: 0.7315 - val_loss: 0.4957 - val_accuracy: 0.7176\n",
      "Epoch 226/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5916 - accuracy: 0.7014 - val_loss: 0.5524 - val_accuracy: 0.7315\n",
      "Epoch 227/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5794 - accuracy: 0.7350 - val_loss: 0.5020 - val_accuracy: 0.7407\n",
      "Epoch 228/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5713 - accuracy: 0.7303 - val_loss: 0.4908 - val_accuracy: 0.7454\n",
      "Epoch 229/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5513 - accuracy: 0.7269 - val_loss: 0.4918 - val_accuracy: 0.7407\n",
      "Epoch 230/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5914 - accuracy: 0.7199 - val_loss: 0.4901 - val_accuracy: 0.7500\n",
      "Epoch 231/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5643 - accuracy: 0.7222 - val_loss: 0.4918 - val_accuracy: 0.7407\n",
      "Epoch 232/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5709 - accuracy: 0.7118 - val_loss: 0.4923 - val_accuracy: 0.7222\n",
      "Epoch 233/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5626 - accuracy: 0.7199 - val_loss: 0.5537 - val_accuracy: 0.7176\n",
      "Epoch 234/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5378 - accuracy: 0.7546 - val_loss: 0.5017 - val_accuracy: 0.7176\n",
      "Epoch 235/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5354 - accuracy: 0.7338 - val_loss: 0.5003 - val_accuracy: 0.7222\n",
      "Epoch 236/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5238 - accuracy: 0.7523 - val_loss: 0.5135 - val_accuracy: 0.7269\n",
      "Epoch 237/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5342 - accuracy: 0.7373 - val_loss: 0.4909 - val_accuracy: 0.7176\n",
      "Epoch 238/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5213 - accuracy: 0.7373 - val_loss: 0.4961 - val_accuracy: 0.7222\n",
      "Epoch 239/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5386 - accuracy: 0.7373 - val_loss: 0.4923 - val_accuracy: 0.7269\n",
      "Epoch 240/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5538 - accuracy: 0.7303 - val_loss: 0.4905 - val_accuracy: 0.7269\n",
      "Epoch 241/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5457 - accuracy: 0.7338 - val_loss: 0.4879 - val_accuracy: 0.7269\n",
      "Epoch 242/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5252 - accuracy: 0.7338 - val_loss: 0.4868 - val_accuracy: 0.7083\n",
      "Epoch 243/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5486 - accuracy: 0.7419 - val_loss: 0.4887 - val_accuracy: 0.7315\n",
      "Epoch 244/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5446 - accuracy: 0.7164 - val_loss: 0.4866 - val_accuracy: 0.7315\n",
      "Epoch 245/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5122 - accuracy: 0.7465 - val_loss: 0.4861 - val_accuracy: 0.7361\n",
      "Epoch 246/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5533 - accuracy: 0.7141 - val_loss: 0.4743 - val_accuracy: 0.6991\n",
      "Epoch 247/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5553 - accuracy: 0.7222 - val_loss: 0.5012 - val_accuracy: 0.7130\n",
      "Epoch 248/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5502 - accuracy: 0.7234 - val_loss: 0.4948 - val_accuracy: 0.7176\n",
      "Epoch 249/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5160 - accuracy: 0.7569 - val_loss: 0.4917 - val_accuracy: 0.7407\n",
      "Epoch 250/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5429 - accuracy: 0.7512 - val_loss: 0.4760 - val_accuracy: 0.7269\n",
      "Epoch 251/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5511 - accuracy: 0.7130 - val_loss: 0.4873 - val_accuracy: 0.7222\n",
      "Epoch 252/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5567 - accuracy: 0.7419 - val_loss: 0.4742 - val_accuracy: 0.7222\n",
      "Epoch 253/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5623 - accuracy: 0.7442 - val_loss: 0.4764 - val_accuracy: 0.7269\n",
      "Epoch 254/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5445 - accuracy: 0.7234 - val_loss: 0.4783 - val_accuracy: 0.7130\n",
      "Epoch 255/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5384 - accuracy: 0.7477 - val_loss: 0.4714 - val_accuracy: 0.7315\n",
      "Epoch 256/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5374 - accuracy: 0.7500 - val_loss: 0.4907 - val_accuracy: 0.7269\n",
      "Epoch 257/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5207 - accuracy: 0.7535 - val_loss: 0.4887 - val_accuracy: 0.7176\n",
      "Epoch 258/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5282 - accuracy: 0.7303 - val_loss: 0.5120 - val_accuracy: 0.7222\n",
      "Epoch 259/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5505 - accuracy: 0.7315 - val_loss: 0.5049 - val_accuracy: 0.7269\n",
      "Epoch 260/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5775 - accuracy: 0.7326 - val_loss: 0.4716 - val_accuracy: 0.7315\n",
      "Epoch 261/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5318 - accuracy: 0.7500 - val_loss: 0.4727 - val_accuracy: 0.7315\n",
      "Epoch 262/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5670 - accuracy: 0.7188 - val_loss: 0.4784 - val_accuracy: 0.7315\n",
      "Epoch 263/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5457 - accuracy: 0.7396 - val_loss: 0.4754 - val_accuracy: 0.7269\n",
      "Epoch 264/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5534 - accuracy: 0.7326 - val_loss: 0.4840 - val_accuracy: 0.7222\n",
      "Epoch 265/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5375 - accuracy: 0.7523 - val_loss: 0.4856 - val_accuracy: 0.7222\n",
      "Epoch 266/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5476 - accuracy: 0.7303 - val_loss: 0.4837 - val_accuracy: 0.7222\n",
      "Epoch 267/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5340 - accuracy: 0.7315 - val_loss: 0.4841 - val_accuracy: 0.7269\n",
      "Epoch 268/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5491 - accuracy: 0.7373 - val_loss: 0.4816 - val_accuracy: 0.7176\n",
      "Epoch 269/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5170 - accuracy: 0.7558 - val_loss: 0.4930 - val_accuracy: 0.7361\n",
      "Epoch 270/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5688 - accuracy: 0.7384 - val_loss: 0.4984 - val_accuracy: 0.7315\n",
      "Epoch 271/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5062 - accuracy: 0.7558 - val_loss: 0.4754 - val_accuracy: 0.7269\n",
      "Epoch 272/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5344 - accuracy: 0.7407 - val_loss: 0.4818 - val_accuracy: 0.7315\n",
      "Epoch 273/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5608 - accuracy: 0.7315 - val_loss: 0.4847 - val_accuracy: 0.7361\n",
      "Epoch 274/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5174 - accuracy: 0.7477 - val_loss: 0.4826 - val_accuracy: 0.7222\n",
      "Epoch 275/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5442 - accuracy: 0.7384 - val_loss: 0.4964 - val_accuracy: 0.7269\n",
      "Epoch 276/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5450 - accuracy: 0.7419 - val_loss: 0.5005 - val_accuracy: 0.7315\n",
      "Epoch 277/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5433 - accuracy: 0.7442 - val_loss: 0.5053 - val_accuracy: 0.7222\n",
      "Epoch 278/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5290 - accuracy: 0.7465 - val_loss: 0.4938 - val_accuracy: 0.7222\n",
      "Epoch 279/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5296 - accuracy: 0.7338 - val_loss: 0.4994 - val_accuracy: 0.7130\n",
      "Epoch 280/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5024 - accuracy: 0.7373 - val_loss: 0.4947 - val_accuracy: 0.7315\n",
      "Epoch 281/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5221 - accuracy: 0.7535 - val_loss: 0.4938 - val_accuracy: 0.7222\n",
      "Epoch 282/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5314 - accuracy: 0.7581 - val_loss: 0.4797 - val_accuracy: 0.7315\n",
      "Epoch 283/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5267 - accuracy: 0.7558 - val_loss: 0.4788 - val_accuracy: 0.7269\n",
      "Epoch 284/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5510 - accuracy: 0.7442 - val_loss: 0.4792 - val_accuracy: 0.7222\n",
      "Epoch 285/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5157 - accuracy: 0.7407 - val_loss: 0.4776 - val_accuracy: 0.7176\n",
      "Epoch 286/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5441 - accuracy: 0.7257 - val_loss: 0.4718 - val_accuracy: 0.7222\n",
      "Epoch 287/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5779 - accuracy: 0.7234 - val_loss: 0.4742 - val_accuracy: 0.7361\n",
      "Epoch 288/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5270 - accuracy: 0.7512 - val_loss: 0.4793 - val_accuracy: 0.7130\n",
      "Epoch 289/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5428 - accuracy: 0.7512 - val_loss: 0.4795 - val_accuracy: 0.7361\n",
      "Epoch 290/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5300 - accuracy: 0.7431 - val_loss: 0.4764 - val_accuracy: 0.7269\n",
      "Epoch 291/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5715 - accuracy: 0.7477 - val_loss: 0.4782 - val_accuracy: 0.7315\n",
      "Epoch 292/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5550 - accuracy: 0.7269 - val_loss: 0.4755 - val_accuracy: 0.7361\n",
      "Epoch 293/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5563 - accuracy: 0.7350 - val_loss: 0.4810 - val_accuracy: 0.7269\n",
      "Epoch 294/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5262 - accuracy: 0.7465 - val_loss: 0.4773 - val_accuracy: 0.7315\n",
      "Epoch 295/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5207 - accuracy: 0.7465 - val_loss: 0.4717 - val_accuracy: 0.7176\n",
      "Epoch 296/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5301 - accuracy: 0.7454 - val_loss: 0.4767 - val_accuracy: 0.7269\n",
      "Epoch 297/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5276 - accuracy: 0.7361 - val_loss: 0.4789 - val_accuracy: 0.7269\n",
      "Epoch 298/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5432 - accuracy: 0.7303 - val_loss: 0.4765 - val_accuracy: 0.7315\n",
      "Epoch 299/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5172 - accuracy: 0.7315 - val_loss: 0.4794 - val_accuracy: 0.7315\n",
      "Epoch 300/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5241 - accuracy: 0.7419 - val_loss: 0.4812 - val_accuracy: 0.7130\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "(None, 1536)\n",
      "Epoch 1/300\n",
      "87/87 [==============================] - 2s 11ms/step - loss: 1.6563 - accuracy: 0.3727 - val_loss: 2.3819 - val_accuracy: 0.2639\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.3209 - accuracy: 0.4630 - val_loss: 1.0683 - val_accuracy: 0.5231\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1532 - accuracy: 0.5220 - val_loss: 1.1262 - val_accuracy: 0.4213\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0168 - accuracy: 0.5694 - val_loss: 1.5576 - val_accuracy: 0.3657\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8816 - accuracy: 0.6424 - val_loss: 1.0681 - val_accuracy: 0.6574\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7992 - accuracy: 0.6678 - val_loss: 0.7866 - val_accuracy: 0.6204\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8016 - accuracy: 0.6655 - val_loss: 1.2440 - val_accuracy: 0.5741\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8351 - accuracy: 0.6562 - val_loss: 1.1461 - val_accuracy: 0.5231\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8611 - accuracy: 0.6412 - val_loss: 0.9177 - val_accuracy: 0.5972\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7225 - accuracy: 0.6921 - val_loss: 1.1873 - val_accuracy: 0.5370\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7894 - accuracy: 0.6806 - val_loss: 0.8959 - val_accuracy: 0.5972\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7467 - accuracy: 0.6887 - val_loss: 0.8417 - val_accuracy: 0.6065\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.7334 - accuracy: 0.6921 - val_loss: 0.7251 - val_accuracy: 0.6898\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7274 - accuracy: 0.6771 - val_loss: 0.9622 - val_accuracy: 0.5787\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6869 - accuracy: 0.6910 - val_loss: 1.3141 - val_accuracy: 0.6250\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8315 - accuracy: 0.6435 - val_loss: 0.8916 - val_accuracy: 0.6343\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7357 - accuracy: 0.6887 - val_loss: 0.9883 - val_accuracy: 0.6481\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7257 - accuracy: 0.6794 - val_loss: 0.6534 - val_accuracy: 0.6713\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7280 - accuracy: 0.6944 - val_loss: 2.2135 - val_accuracy: 0.3981\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7535 - accuracy: 0.6852 - val_loss: 0.8180 - val_accuracy: 0.6528\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7791 - accuracy: 0.6933 - val_loss: 0.9384 - val_accuracy: 0.5926\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7419 - accuracy: 0.6840 - val_loss: 0.7502 - val_accuracy: 0.6620\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7375 - accuracy: 0.6887 - val_loss: 0.9787 - val_accuracy: 0.6204\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6878 - accuracy: 0.7049 - val_loss: 0.8438 - val_accuracy: 0.6296\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7173 - accuracy: 0.6956 - val_loss: 0.8518 - val_accuracy: 0.6852\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7868 - accuracy: 0.6470 - val_loss: 1.2878 - val_accuracy: 0.5093\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.7070 - accuracy: 0.6933 - val_loss: 0.7150 - val_accuracy: 0.7037\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6929 - accuracy: 0.6921 - val_loss: 0.8679 - val_accuracy: 0.6019\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6900 - accuracy: 0.7049 - val_loss: 0.6252 - val_accuracy: 0.7037\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6715 - accuracy: 0.7049 - val_loss: 0.7065 - val_accuracy: 0.6574\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6497 - accuracy: 0.7199 - val_loss: 0.8909 - val_accuracy: 0.5880\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.6875 - val_loss: 0.9132 - val_accuracy: 0.6250\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6907 - accuracy: 0.7002 - val_loss: 0.6932 - val_accuracy: 0.6620\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6557 - accuracy: 0.7083 - val_loss: 0.9450 - val_accuracy: 0.5324\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7458 - accuracy: 0.6852 - val_loss: 1.2309 - val_accuracy: 0.5231\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6757 - accuracy: 0.6944 - val_loss: 0.7387 - val_accuracy: 0.6296\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8014 - accuracy: 0.6725 - val_loss: 0.7737 - val_accuracy: 0.6620\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6983 - accuracy: 0.6875 - val_loss: 0.9071 - val_accuracy: 0.6111\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6830 - accuracy: 0.6968 - val_loss: 0.6140 - val_accuracy: 0.6852\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6657 - accuracy: 0.7060 - val_loss: 0.6921 - val_accuracy: 0.6806\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6307 - accuracy: 0.7373 - val_loss: 0.6290 - val_accuracy: 0.6620\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6627 - accuracy: 0.7106 - val_loss: 0.6677 - val_accuracy: 0.6667\n",
      "Epoch 43/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6300 - accuracy: 0.7106 - val_loss: 0.5672 - val_accuracy: 0.6806\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6224 - accuracy: 0.7176 - val_loss: 0.7186 - val_accuracy: 0.6806\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7010 - accuracy: 0.6840 - val_loss: 0.6545 - val_accuracy: 0.7130\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6289 - accuracy: 0.6991 - val_loss: 0.5856 - val_accuracy: 0.6944\n",
      "Epoch 47/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6108 - accuracy: 0.7257 - val_loss: 0.6282 - val_accuracy: 0.6852\n",
      "Epoch 48/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6308 - accuracy: 0.7141 - val_loss: 1.5332 - val_accuracy: 0.5093\n",
      "Epoch 49/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6544 - accuracy: 0.6910 - val_loss: 0.6750 - val_accuracy: 0.6667\n",
      "Epoch 50/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6477 - accuracy: 0.7072 - val_loss: 1.1956 - val_accuracy: 0.5556\n",
      "Epoch 51/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5925 - accuracy: 0.7303 - val_loss: 0.6067 - val_accuracy: 0.6713\n",
      "Epoch 52/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6286 - accuracy: 0.7014 - val_loss: 0.6182 - val_accuracy: 0.6852\n",
      "Epoch 53/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6248 - accuracy: 0.7211 - val_loss: 0.6971 - val_accuracy: 0.7083\n",
      "Epoch 54/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6071 - accuracy: 0.7269 - val_loss: 0.5894 - val_accuracy: 0.6667\n",
      "Epoch 55/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6758 - accuracy: 0.7095 - val_loss: 0.7035 - val_accuracy: 0.6667\n",
      "Epoch 56/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6918 - accuracy: 0.6944 - val_loss: 0.7829 - val_accuracy: 0.6944\n",
      "Epoch 57/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6350 - accuracy: 0.7083 - val_loss: 0.7718 - val_accuracy: 0.6713\n",
      "Epoch 58/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6676 - accuracy: 0.6933 - val_loss: 0.6205 - val_accuracy: 0.6944\n",
      "Epoch 59/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5801 - accuracy: 0.7257 - val_loss: 0.7707 - val_accuracy: 0.6481\n",
      "Epoch 60/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5964 - accuracy: 0.7095 - val_loss: 0.5743 - val_accuracy: 0.6759\n",
      "Epoch 61/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6031 - accuracy: 0.7118 - val_loss: 0.8240 - val_accuracy: 0.6481\n",
      "Epoch 62/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6809 - accuracy: 0.6991 - val_loss: 1.0825 - val_accuracy: 0.6389\n",
      "Epoch 63/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6573 - accuracy: 0.7049 - val_loss: 0.5937 - val_accuracy: 0.6759\n",
      "Epoch 64/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5829 - accuracy: 0.7396 - val_loss: 0.5910 - val_accuracy: 0.7083\n",
      "Epoch 65/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6055 - accuracy: 0.7188 - val_loss: 0.5835 - val_accuracy: 0.6852\n",
      "Epoch 66/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6031 - accuracy: 0.7350 - val_loss: 1.7437 - val_accuracy: 0.4907\n",
      "Epoch 67/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6375 - accuracy: 0.7025 - val_loss: 0.8190 - val_accuracy: 0.6852\n",
      "Epoch 68/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6262 - accuracy: 0.7083 - val_loss: 0.6502 - val_accuracy: 0.7037\n",
      "Epoch 69/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6045 - accuracy: 0.7234 - val_loss: 0.7150 - val_accuracy: 0.6898\n",
      "Epoch 70/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6174 - accuracy: 0.7269 - val_loss: 0.5502 - val_accuracy: 0.6620\n",
      "Epoch 71/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6161 - accuracy: 0.7072 - val_loss: 0.7996 - val_accuracy: 0.6667\n",
      "Epoch 72/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6284 - accuracy: 0.7130 - val_loss: 0.9212 - val_accuracy: 0.6343\n",
      "Epoch 73/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6338 - accuracy: 0.7049 - val_loss: 1.2072 - val_accuracy: 0.5093\n",
      "Epoch 74/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7264 - accuracy: 0.6759 - val_loss: 0.7278 - val_accuracy: 0.6296\n",
      "Epoch 75/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7056 - accuracy: 0.6840 - val_loss: 0.7469 - val_accuracy: 0.6852\n",
      "Epoch 76/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6761 - accuracy: 0.7222 - val_loss: 1.8843 - val_accuracy: 0.3519\n",
      "Epoch 77/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6951 - accuracy: 0.7072 - val_loss: 0.6086 - val_accuracy: 0.6574\n",
      "Epoch 78/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6725 - accuracy: 0.7083 - val_loss: 0.6133 - val_accuracy: 0.6713\n",
      "Epoch 79/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6586 - accuracy: 0.7037 - val_loss: 0.8691 - val_accuracy: 0.5648\n",
      "Epoch 80/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6887 - accuracy: 0.6829 - val_loss: 2.0982 - val_accuracy: 0.4306\n",
      "Epoch 81/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7365 - accuracy: 0.6655 - val_loss: 0.9521 - val_accuracy: 0.6296\n",
      "Epoch 82/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6559 - accuracy: 0.7014 - val_loss: 0.5803 - val_accuracy: 0.7037\n",
      "Epoch 83/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6510 - accuracy: 0.7130 - val_loss: 0.6149 - val_accuracy: 0.6713\n",
      "Epoch 84/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6199 - accuracy: 0.7326 - val_loss: 0.6649 - val_accuracy: 0.7130\n",
      "Epoch 85/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6575 - accuracy: 0.7141 - val_loss: 0.6740 - val_accuracy: 0.6759\n",
      "Epoch 86/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6344 - accuracy: 0.7025 - val_loss: 0.5824 - val_accuracy: 0.6898\n",
      "Epoch 87/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6152 - accuracy: 0.7199 - val_loss: 0.6206 - val_accuracy: 0.6852\n",
      "Epoch 88/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6804 - accuracy: 0.6910 - val_loss: 0.6175 - val_accuracy: 0.7176\n",
      "Epoch 89/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6344 - accuracy: 0.7049 - val_loss: 0.9201 - val_accuracy: 0.6806\n",
      "Epoch 90/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6534 - accuracy: 0.7141 - val_loss: 9.3422 - val_accuracy: 0.4306\n",
      "Epoch 91/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6456 - accuracy: 0.7014 - val_loss: 0.8872 - val_accuracy: 0.6528\n",
      "Epoch 92/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6596 - accuracy: 0.7199 - val_loss: 0.7704 - val_accuracy: 0.6806\n",
      "Epoch 93/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6539 - accuracy: 0.6898 - val_loss: 0.5998 - val_accuracy: 0.7130\n",
      "Epoch 94/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6103 - accuracy: 0.7245 - val_loss: 0.6453 - val_accuracy: 0.6944\n",
      "Epoch 95/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6414 - accuracy: 0.7083 - val_loss: 0.7057 - val_accuracy: 0.6296\n",
      "Epoch 96/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6010 - accuracy: 0.7419 - val_loss: 0.5774 - val_accuracy: 0.6898\n",
      "Epoch 97/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6008 - accuracy: 0.7384 - val_loss: 0.5826 - val_accuracy: 0.6806\n",
      "Epoch 98/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7155 - accuracy: 0.7002 - val_loss: 0.7623 - val_accuracy: 0.6528\n",
      "Epoch 99/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6651 - accuracy: 0.6956 - val_loss: 0.6399 - val_accuracy: 0.6667\n",
      "Epoch 100/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5983 - accuracy: 0.7234 - val_loss: 1.0925 - val_accuracy: 0.6111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6443 - accuracy: 0.7188 - val_loss: 0.6647 - val_accuracy: 0.6667\n",
      "Epoch 102/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5566 - accuracy: 0.7569 - val_loss: 0.5934 - val_accuracy: 0.6481\n",
      "Epoch 103/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6150 - accuracy: 0.7396 - val_loss: 0.8588 - val_accuracy: 0.6111\n",
      "Epoch 104/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6596 - accuracy: 0.7106 - val_loss: 29.7240 - val_accuracy: 0.4491\n",
      "Epoch 105/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6338 - accuracy: 0.7025 - val_loss: 0.8406 - val_accuracy: 0.6620\n",
      "Epoch 106/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6135 - accuracy: 0.7118 - val_loss: 0.6108 - val_accuracy: 0.6296\n",
      "Epoch 107/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6465 - accuracy: 0.7164 - val_loss: 0.6565 - val_accuracy: 0.6620\n",
      "Epoch 108/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5857 - accuracy: 0.7407 - val_loss: 0.5917 - val_accuracy: 0.6667\n",
      "Epoch 109/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6386 - accuracy: 0.7269 - val_loss: 0.5533 - val_accuracy: 0.6574\n",
      "Epoch 110/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6227 - accuracy: 0.7014 - val_loss: 0.6607 - val_accuracy: 0.6620\n",
      "Epoch 111/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5666 - accuracy: 0.7396 - val_loss: 0.6081 - val_accuracy: 0.6944\n",
      "Epoch 112/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7666 - accuracy: 0.6481 - val_loss: 4.7138 - val_accuracy: 0.4583\n",
      "Epoch 113/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6110 - accuracy: 0.7130 - val_loss: 0.5674 - val_accuracy: 0.6574\n",
      "Epoch 114/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5918 - accuracy: 0.7211 - val_loss: 0.5382 - val_accuracy: 0.6713\n",
      "Epoch 115/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5907 - accuracy: 0.7269 - val_loss: 0.6244 - val_accuracy: 0.7269\n",
      "Epoch 116/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6011 - accuracy: 0.7269 - val_loss: 0.5249 - val_accuracy: 0.6806\n",
      "Epoch 117/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6110 - accuracy: 0.7083 - val_loss: 0.7611 - val_accuracy: 0.6528\n",
      "Epoch 118/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5800 - accuracy: 0.7465 - val_loss: 0.5748 - val_accuracy: 0.6620\n",
      "Epoch 119/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6022 - accuracy: 0.7303 - val_loss: 0.5436 - val_accuracy: 0.6806\n",
      "Epoch 120/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6056 - accuracy: 0.7014 - val_loss: 0.5412 - val_accuracy: 0.6806\n",
      "Epoch 121/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5690 - accuracy: 0.7465 - val_loss: 0.5398 - val_accuracy: 0.6759\n",
      "Epoch 122/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5889 - accuracy: 0.7338 - val_loss: 0.6369 - val_accuracy: 0.6620\n",
      "Epoch 123/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9789 - accuracy: 0.5637 - val_loss: 0.8656 - val_accuracy: 0.5694\n",
      "Epoch 124/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8372 - accuracy: 0.5995 - val_loss: 0.8137 - val_accuracy: 0.6157\n",
      "Epoch 125/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8151 - accuracy: 0.6227 - val_loss: 0.9435 - val_accuracy: 0.5741\n",
      "Epoch 126/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8069 - accuracy: 0.6181 - val_loss: 1.2162 - val_accuracy: 0.6157\n",
      "Epoch 127/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7690 - accuracy: 0.6412 - val_loss: 0.9186 - val_accuracy: 0.6157\n",
      "Epoch 128/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8161 - accuracy: 0.6377 - val_loss: 0.7566 - val_accuracy: 0.6296\n",
      "Epoch 129/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7662 - accuracy: 0.6551 - val_loss: 0.9069 - val_accuracy: 0.5926\n",
      "Epoch 130/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8110 - accuracy: 0.6238 - val_loss: 0.7385 - val_accuracy: 0.6667\n",
      "Epoch 131/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8034 - accuracy: 0.6354 - val_loss: 0.7718 - val_accuracy: 0.6343\n",
      "Epoch 132/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7842 - accuracy: 0.6586 - val_loss: 1.1393 - val_accuracy: 0.6296\n",
      "Epoch 133/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7693 - accuracy: 0.6725 - val_loss: 1.3841 - val_accuracy: 0.5509\n",
      "Epoch 134/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8673 - accuracy: 0.6204 - val_loss: 1.1225 - val_accuracy: 0.6204\n",
      "Epoch 135/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7460 - accuracy: 0.6725 - val_loss: 0.8462 - val_accuracy: 0.6389\n",
      "Epoch 136/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7536 - accuracy: 0.6667 - val_loss: 1.8964 - val_accuracy: 0.6667\n",
      "Epoch 137/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7619 - accuracy: 0.6725 - val_loss: 1.0842 - val_accuracy: 0.6204\n",
      "Epoch 138/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8435 - accuracy: 0.6644 - val_loss: 0.9372 - val_accuracy: 0.6481\n",
      "Epoch 139/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8088 - accuracy: 0.6400 - val_loss: 1.8445 - val_accuracy: 0.5926\n",
      "Epoch 140/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9177 - accuracy: 0.5926 - val_loss: 0.7501 - val_accuracy: 0.6574\n",
      "Epoch 141/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7842 - accuracy: 0.6655 - val_loss: 5.7507 - val_accuracy: 0.5185\n",
      "Epoch 142/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8241 - accuracy: 0.6505 - val_loss: 0.7159 - val_accuracy: 0.6528\n",
      "Epoch 143/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7124 - accuracy: 0.6910 - val_loss: 0.6799 - val_accuracy: 0.6667\n",
      "Epoch 144/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6667 - accuracy: 0.7083 - val_loss: 0.6508 - val_accuracy: 0.6898\n",
      "Epoch 145/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6257 - accuracy: 0.7245 - val_loss: 0.6235 - val_accuracy: 0.6713\n",
      "Epoch 146/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6419 - accuracy: 0.7176 - val_loss: 0.6175 - val_accuracy: 0.6481\n",
      "Epoch 147/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6220 - accuracy: 0.7373 - val_loss: 0.6152 - val_accuracy: 0.6759\n",
      "Epoch 148/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7055 - accuracy: 0.6840 - val_loss: 0.6804 - val_accuracy: 0.6667\n",
      "Epoch 149/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6248 - accuracy: 0.7257 - val_loss: 0.5787 - val_accuracy: 0.7176\n",
      "Epoch 150/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6503 - accuracy: 0.7106 - val_loss: 0.5849 - val_accuracy: 0.6806\n",
      "Epoch 151/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6058 - accuracy: 0.7153 - val_loss: 0.6043 - val_accuracy: 0.6389\n",
      "Epoch 152/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6182 - accuracy: 0.7211 - val_loss: 0.5519 - val_accuracy: 0.6713\n",
      "Epoch 153/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6068 - accuracy: 0.7280 - val_loss: 0.5728 - val_accuracy: 0.6944\n",
      "Epoch 154/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6067 - accuracy: 0.7269 - val_loss: 0.6322 - val_accuracy: 0.6898\n",
      "Epoch 155/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5985 - accuracy: 0.7222 - val_loss: 0.5690 - val_accuracy: 0.6667\n",
      "Epoch 156/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5742 - accuracy: 0.7396 - val_loss: 0.5394 - val_accuracy: 0.6713\n",
      "Epoch 157/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9015 - accuracy: 0.6100 - val_loss: 0.9614 - val_accuracy: 0.5833\n",
      "Epoch 158/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8103 - accuracy: 0.6505 - val_loss: 0.7714 - val_accuracy: 0.5556\n",
      "Epoch 159/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8577 - accuracy: 0.6227 - val_loss: 0.7863 - val_accuracy: 0.5648\n",
      "Epoch 160/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7887 - accuracy: 0.6748 - val_loss: 0.8053 - val_accuracy: 0.6204\n",
      "Epoch 161/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7976 - accuracy: 0.6678 - val_loss: 0.7015 - val_accuracy: 0.6389\n",
      "Epoch 162/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7371 - accuracy: 0.6956 - val_loss: 0.7202 - val_accuracy: 0.6435\n",
      "Epoch 163/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6949 - accuracy: 0.7199 - val_loss: 0.7172 - val_accuracy: 0.6389\n",
      "Epoch 164/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7036 - accuracy: 0.7060 - val_loss: 0.8884 - val_accuracy: 0.5926\n",
      "Epoch 165/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7649 - accuracy: 0.6887 - val_loss: 0.6909 - val_accuracy: 0.6343\n",
      "Epoch 166/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6931 - accuracy: 0.7049 - val_loss: 1.1918 - val_accuracy: 0.5231\n",
      "Epoch 167/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7411 - accuracy: 0.6713 - val_loss: 0.6663 - val_accuracy: 0.6574\n",
      "Epoch 168/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7331 - accuracy: 0.7049 - val_loss: 0.7307 - val_accuracy: 0.6343\n",
      "Epoch 169/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7137 - accuracy: 0.7060 - val_loss: 0.7069 - val_accuracy: 0.6481\n",
      "Epoch 170/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7328 - accuracy: 0.6910 - val_loss: 0.7313 - val_accuracy: 0.6389\n",
      "Epoch 171/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7404 - accuracy: 0.7176 - val_loss: 0.6849 - val_accuracy: 0.6435\n",
      "Epoch 172/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7364 - accuracy: 0.7049 - val_loss: 0.7026 - val_accuracy: 0.6389\n",
      "Epoch 173/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7522 - accuracy: 0.6875 - val_loss: 0.7197 - val_accuracy: 0.6620\n",
      "Epoch 174/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6754 - accuracy: 0.7176 - val_loss: 0.7019 - val_accuracy: 0.6435\n",
      "Epoch 175/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7025 - accuracy: 0.7188 - val_loss: 0.7398 - val_accuracy: 0.6111\n",
      "Epoch 176/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6962 - accuracy: 0.7083 - val_loss: 0.6706 - val_accuracy: 0.6481\n",
      "Epoch 177/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7042 - accuracy: 0.7025 - val_loss: 0.6891 - val_accuracy: 0.6343\n",
      "Epoch 178/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7226 - accuracy: 0.6979 - val_loss: 0.7269 - val_accuracy: 0.6667\n",
      "Epoch 179/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7024 - accuracy: 0.7245 - val_loss: 0.6962 - val_accuracy: 0.6574\n",
      "Epoch 180/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7008 - accuracy: 0.7060 - val_loss: 0.6627 - val_accuracy: 0.6620\n",
      "Epoch 181/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6820 - accuracy: 0.7037 - val_loss: 0.6549 - val_accuracy: 0.6296\n",
      "Epoch 182/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7043 - accuracy: 0.7002 - val_loss: 0.6488 - val_accuracy: 0.6250\n",
      "Epoch 183/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6702 - accuracy: 0.7095 - val_loss: 0.6521 - val_accuracy: 0.6481\n",
      "Epoch 184/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6883 - accuracy: 0.7199 - val_loss: 0.6713 - val_accuracy: 0.6296\n",
      "Epoch 185/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7257 - accuracy: 0.7083 - val_loss: 0.6669 - val_accuracy: 0.6620\n",
      "Epoch 186/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6789 - accuracy: 0.7037 - val_loss: 0.7543 - val_accuracy: 0.6944\n",
      "Epoch 187/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6724 - accuracy: 0.7303 - val_loss: 0.7288 - val_accuracy: 0.6806\n",
      "Epoch 188/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6928 - accuracy: 0.6956 - val_loss: 0.6659 - val_accuracy: 0.6481\n",
      "Epoch 189/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6933 - accuracy: 0.6956 - val_loss: 0.6294 - val_accuracy: 0.6389\n",
      "Epoch 190/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7229 - accuracy: 0.6898 - val_loss: 0.6577 - val_accuracy: 0.6620\n",
      "Epoch 191/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6780 - accuracy: 0.7049 - val_loss: 0.6341 - val_accuracy: 0.6713\n",
      "Epoch 192/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7023 - accuracy: 0.7130 - val_loss: 0.7035 - val_accuracy: 0.6528\n",
      "Epoch 193/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6934 - accuracy: 0.7095 - val_loss: 0.6952 - val_accuracy: 0.6435\n",
      "Epoch 194/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6493 - accuracy: 0.7338 - val_loss: 0.6776 - val_accuracy: 0.6389\n",
      "Epoch 195/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6688 - accuracy: 0.7025 - val_loss: 0.6596 - val_accuracy: 0.6435\n",
      "Epoch 196/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6646 - accuracy: 0.7222 - val_loss: 0.6545 - val_accuracy: 0.6481\n",
      "Epoch 197/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6433 - accuracy: 0.7303 - val_loss: 0.6520 - val_accuracy: 0.6157\n",
      "Epoch 198/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7257 - accuracy: 0.7025 - val_loss: 0.6478 - val_accuracy: 0.6435\n",
      "Epoch 199/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6574 - accuracy: 0.7303 - val_loss: 0.6885 - val_accuracy: 0.6991\n",
      "Epoch 200/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7238 - accuracy: 0.6887 - val_loss: 0.6680 - val_accuracy: 0.6574\n",
      "Epoch 201/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6848 - accuracy: 0.7037 - val_loss: 0.6663 - val_accuracy: 0.6667\n",
      "Epoch 202/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6628 - accuracy: 0.7338 - val_loss: 0.6524 - val_accuracy: 0.6481\n",
      "Epoch 203/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6829 - accuracy: 0.7245 - val_loss: 0.6516 - val_accuracy: 0.6528\n",
      "Epoch 204/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6539 - accuracy: 0.7326 - val_loss: 0.6468 - val_accuracy: 0.6343\n",
      "Epoch 205/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6279 - accuracy: 0.7396 - val_loss: 0.6620 - val_accuracy: 0.6713\n",
      "Epoch 206/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6382 - accuracy: 0.7407 - val_loss: 0.6389 - val_accuracy: 0.6481\n",
      "Epoch 207/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6781 - accuracy: 0.7083 - val_loss: 0.6405 - val_accuracy: 0.6528\n",
      "Epoch 208/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6712 - accuracy: 0.7199 - val_loss: 0.6547 - val_accuracy: 0.6389\n",
      "Epoch 209/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6950 - accuracy: 0.6933 - val_loss: 0.6452 - val_accuracy: 0.6343\n",
      "Epoch 210/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6681 - accuracy: 0.7234 - val_loss: 0.6430 - val_accuracy: 0.6389\n",
      "Epoch 211/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6794 - accuracy: 0.7141 - val_loss: 0.6382 - val_accuracy: 0.6343\n",
      "Epoch 212/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7014 - accuracy: 0.6991 - val_loss: 0.7986 - val_accuracy: 0.6343\n",
      "Epoch 213/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6651 - accuracy: 0.7130 - val_loss: 0.9409 - val_accuracy: 0.6157\n",
      "Epoch 214/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6424 - accuracy: 0.7303 - val_loss: 0.9917 - val_accuracy: 0.6204\n",
      "Epoch 215/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6453 - accuracy: 0.7153 - val_loss: 0.6454 - val_accuracy: 0.6667\n",
      "Epoch 216/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6505 - accuracy: 0.7199 - val_loss: 0.8150 - val_accuracy: 0.6620\n",
      "Epoch 217/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6606 - accuracy: 0.7384 - val_loss: 0.8210 - val_accuracy: 0.6296\n",
      "Epoch 218/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6594 - accuracy: 0.7292 - val_loss: 1.6528 - val_accuracy: 0.6250\n",
      "Epoch 219/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6566 - accuracy: 0.7130 - val_loss: 1.9023 - val_accuracy: 0.6481\n",
      "Epoch 220/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6889 - accuracy: 0.7141 - val_loss: 1.7850 - val_accuracy: 0.6296\n",
      "Epoch 221/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6511 - accuracy: 0.7222 - val_loss: 3.9060 - val_accuracy: 0.6528\n",
      "Epoch 222/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6334 - accuracy: 0.7373 - val_loss: 2.5589 - val_accuracy: 0.6389\n",
      "Epoch 223/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6495 - accuracy: 0.7338 - val_loss: 1.8596 - val_accuracy: 0.6343\n",
      "Epoch 224/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6450 - accuracy: 0.7211 - val_loss: 1.6740 - val_accuracy: 0.6435\n",
      "Epoch 225/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6944 - accuracy: 0.7245 - val_loss: 0.8035 - val_accuracy: 0.6204\n",
      "Epoch 226/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6791 - accuracy: 0.7234 - val_loss: 11.4936 - val_accuracy: 0.6296\n",
      "Epoch 227/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6620 - accuracy: 0.7350 - val_loss: 1.7655 - val_accuracy: 0.6528\n",
      "Epoch 228/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7097 - accuracy: 0.7002 - val_loss: 2.9336 - val_accuracy: 0.6250\n",
      "Epoch 229/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6732 - accuracy: 0.7222 - val_loss: 1.4032 - val_accuracy: 0.6759\n",
      "Epoch 230/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6474 - accuracy: 0.7326 - val_loss: 0.9201 - val_accuracy: 0.6389\n",
      "Epoch 231/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6520 - accuracy: 0.7454 - val_loss: 1.2406 - val_accuracy: 0.6759\n",
      "Epoch 232/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6602 - accuracy: 0.7211 - val_loss: 1.4925 - val_accuracy: 0.6296\n",
      "Epoch 233/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6055 - accuracy: 0.7373 - val_loss: 2.8726 - val_accuracy: 0.6389\n",
      "Epoch 234/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6248 - accuracy: 0.7419 - val_loss: 1.6873 - val_accuracy: 0.6435\n",
      "Epoch 235/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6512 - accuracy: 0.7303 - val_loss: 2.3720 - val_accuracy: 0.6389\n",
      "Epoch 236/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6457 - accuracy: 0.7280 - val_loss: 7.1424 - val_accuracy: 0.6389\n",
      "Epoch 237/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6558 - accuracy: 0.7338 - val_loss: 1.3301 - val_accuracy: 0.6389\n",
      "Epoch 238/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6693 - accuracy: 0.7141 - val_loss: 2.1833 - val_accuracy: 0.6481\n",
      "Epoch 239/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6724 - accuracy: 0.7060 - val_loss: 4.1797 - val_accuracy: 0.6157\n",
      "Epoch 240/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6858 - accuracy: 0.7130 - val_loss: 0.6884 - val_accuracy: 0.6759\n",
      "Epoch 241/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6369 - accuracy: 0.7326 - val_loss: 0.6081 - val_accuracy: 0.6620\n",
      "Epoch 242/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6557 - accuracy: 0.7164 - val_loss: 1.4726 - val_accuracy: 0.6343\n",
      "Epoch 243/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6627 - accuracy: 0.7350 - val_loss: 1.1340 - val_accuracy: 0.6759\n",
      "Epoch 244/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6578 - accuracy: 0.7303 - val_loss: 0.6246 - val_accuracy: 0.6759\n",
      "Epoch 245/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6632 - accuracy: 0.7257 - val_loss: 3.0043 - val_accuracy: 0.6528\n",
      "Epoch 246/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6658 - accuracy: 0.7176 - val_loss: 1.0132 - val_accuracy: 0.6759\n",
      "Epoch 247/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6964 - accuracy: 0.7072 - val_loss: 1.3024 - val_accuracy: 0.6481\n",
      "Epoch 248/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6173 - accuracy: 0.7535 - val_loss: 3.3128 - val_accuracy: 0.6528\n",
      "Epoch 249/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6649 - accuracy: 0.7442 - val_loss: 12.5423 - val_accuracy: 0.6574\n",
      "Epoch 250/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6952 - accuracy: 0.7060 - val_loss: 0.8611 - val_accuracy: 0.6620\n",
      "Epoch 251/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6458 - accuracy: 0.7361 - val_loss: 0.6006 - val_accuracy: 0.6759\n",
      "Epoch 252/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6579 - accuracy: 0.7245 - val_loss: 0.7786 - val_accuracy: 0.6806\n",
      "Epoch 253/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6349 - accuracy: 0.7407 - val_loss: 1.5279 - val_accuracy: 0.6667\n",
      "Epoch 254/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6346 - accuracy: 0.7419 - val_loss: 4.6599 - val_accuracy: 0.6389\n",
      "Epoch 255/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6515 - accuracy: 0.7315 - val_loss: 5.2967 - val_accuracy: 0.6528\n",
      "Epoch 256/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6584 - accuracy: 0.7257 - val_loss: 8.6822 - val_accuracy: 0.6574\n",
      "Epoch 257/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6462 - accuracy: 0.7049 - val_loss: 14.0296 - val_accuracy: 0.6620\n",
      "Epoch 258/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6430 - accuracy: 0.7303 - val_loss: 0.6086 - val_accuracy: 0.6713\n",
      "Epoch 259/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6704 - accuracy: 0.7315 - val_loss: 1.4423 - val_accuracy: 0.6667\n",
      "Epoch 260/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6117 - accuracy: 0.7303 - val_loss: 1.9869 - val_accuracy: 0.6481\n",
      "Epoch 261/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6519 - accuracy: 0.7245 - val_loss: 1.0361 - val_accuracy: 0.6343\n",
      "Epoch 262/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6353 - accuracy: 0.7303 - val_loss: 2.7714 - val_accuracy: 0.6481\n",
      "Epoch 263/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6276 - accuracy: 0.7280 - val_loss: 6.3992 - val_accuracy: 0.6481\n",
      "Epoch 264/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6663 - accuracy: 0.7234 - val_loss: 12.0360 - val_accuracy: 0.6481\n",
      "Epoch 265/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6349 - accuracy: 0.7361 - val_loss: 0.6058 - val_accuracy: 0.6528\n",
      "Epoch 266/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6885 - accuracy: 0.7211 - val_loss: 0.6013 - val_accuracy: 0.6667\n",
      "Epoch 267/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6369 - accuracy: 0.7292 - val_loss: 0.6755 - val_accuracy: 0.6667\n",
      "Epoch 268/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6437 - accuracy: 0.7350 - val_loss: 1.9705 - val_accuracy: 0.6435\n",
      "Epoch 269/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6523 - accuracy: 0.7234 - val_loss: 5.0984 - val_accuracy: 0.6620\n",
      "Epoch 270/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6522 - accuracy: 0.7269 - val_loss: 6.1089 - val_accuracy: 0.6620\n",
      "Epoch 271/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6658 - accuracy: 0.7303 - val_loss: 12.3359 - val_accuracy: 0.6435\n",
      "Epoch 272/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6157 - accuracy: 0.7350 - val_loss: 15.6501 - val_accuracy: 0.6481\n",
      "Epoch 273/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6345 - accuracy: 0.7315 - val_loss: 21.6675 - val_accuracy: 0.6481\n",
      "Epoch 274/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6803 - accuracy: 0.7153 - val_loss: 0.9977 - val_accuracy: 0.6713\n",
      "Epoch 275/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6340 - accuracy: 0.7488 - val_loss: 1.8149 - val_accuracy: 0.6481\n",
      "Epoch 276/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6548 - accuracy: 0.7211 - val_loss: 5.0758 - val_accuracy: 0.6435\n",
      "Epoch 277/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6484 - accuracy: 0.7303 - val_loss: 11.3243 - val_accuracy: 0.6389\n",
      "Epoch 278/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6165 - accuracy: 0.7454 - val_loss: 14.3960 - val_accuracy: 0.6389\n",
      "Epoch 279/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6563 - accuracy: 0.7269 - val_loss: 19.2708 - val_accuracy: 0.6435\n",
      "Epoch 280/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6335 - accuracy: 0.7280 - val_loss: 9.3663 - val_accuracy: 0.6481\n",
      "Epoch 281/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6035 - accuracy: 0.7535 - val_loss: 0.6104 - val_accuracy: 0.6667\n",
      "Epoch 282/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6131 - accuracy: 0.7500 - val_loss: 0.6045 - val_accuracy: 0.6574\n",
      "Epoch 283/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6710 - accuracy: 0.7199 - val_loss: 0.6061 - val_accuracy: 0.6620\n",
      "Epoch 284/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6613 - accuracy: 0.7407 - val_loss: 0.6116 - val_accuracy: 0.6574\n",
      "Epoch 285/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6306 - accuracy: 0.7373 - val_loss: 0.6096 - val_accuracy: 0.6759\n",
      "Epoch 286/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6560 - accuracy: 0.7326 - val_loss: 2.0138 - val_accuracy: 0.6481\n",
      "Epoch 287/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6414 - accuracy: 0.7373 - val_loss: 3.1536 - val_accuracy: 0.6389\n",
      "Epoch 288/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6377 - accuracy: 0.7477 - val_loss: 7.3325 - val_accuracy: 0.6528\n",
      "Epoch 289/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6455 - accuracy: 0.7211 - val_loss: 0.6091 - val_accuracy: 0.6759\n",
      "Epoch 290/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6401 - accuracy: 0.7384 - val_loss: 0.6179 - val_accuracy: 0.6620\n",
      "Epoch 291/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6586 - accuracy: 0.7326 - val_loss: 1.4828 - val_accuracy: 0.6435\n",
      "Epoch 292/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5933 - accuracy: 0.7535 - val_loss: 0.6102 - val_accuracy: 0.6620\n",
      "Epoch 293/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6629 - accuracy: 0.7199 - val_loss: 0.6598 - val_accuracy: 0.6574\n",
      "Epoch 294/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6287 - accuracy: 0.7211 - val_loss: 0.6075 - val_accuracy: 0.6713\n",
      "Epoch 295/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6333 - accuracy: 0.7141 - val_loss: 0.6093 - val_accuracy: 0.6667\n",
      "Epoch 296/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6308 - accuracy: 0.7269 - val_loss: 1.2583 - val_accuracy: 0.6528\n",
      "Epoch 297/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6449 - accuracy: 0.7315 - val_loss: 5.8900 - val_accuracy: 0.6435\n",
      "Epoch 298/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6283 - accuracy: 0.7303 - val_loss: 7.9775 - val_accuracy: 0.6389\n",
      "Epoch 299/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6707 - accuracy: 0.7245 - val_loss: 0.7537 - val_accuracy: 0.6620\n",
      "Epoch 300/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6226 - accuracy: 0.7407 - val_loss: 0.6505 - val_accuracy: 0.6620\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "(None, 1536)\n",
      "Epoch 1/300\n",
      "87/87 [==============================] - 2s 11ms/step - loss: 1.6529 - accuracy: 0.3727 - val_loss: 1.7647 - val_accuracy: 0.5139\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.2139 - accuracy: 0.4583 - val_loss: 2.0748 - val_accuracy: 0.3935\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0678 - accuracy: 0.5394 - val_loss: 1.9862 - val_accuracy: 0.4352\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.0002 - accuracy: 0.5671 - val_loss: 1.1844 - val_accuracy: 0.5185\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.9714 - accuracy: 0.5613 - val_loss: 0.9304 - val_accuracy: 0.6019\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8453 - accuracy: 0.6505 - val_loss: 1.3312 - val_accuracy: 0.5509\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8895 - accuracy: 0.6181 - val_loss: 0.7936 - val_accuracy: 0.6620\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8553 - accuracy: 0.6319 - val_loss: 1.1332 - val_accuracy: 0.5602\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7696 - accuracy: 0.6690 - val_loss: 1.4800 - val_accuracy: 0.5231\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7790 - accuracy: 0.6644 - val_loss: 1.0580 - val_accuracy: 0.6343\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8118 - accuracy: 0.6366 - val_loss: 0.6912 - val_accuracy: 0.6713\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7659 - accuracy: 0.6655 - val_loss: 0.7017 - val_accuracy: 0.6991\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6925 - accuracy: 0.6713 - val_loss: 0.7206 - val_accuracy: 0.6713\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7407 - accuracy: 0.6690 - val_loss: 0.9234 - val_accuracy: 0.6481\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.7651 - accuracy: 0.6782 - val_loss: 0.6158 - val_accuracy: 0.7269\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7193 - accuracy: 0.6829 - val_loss: 1.1674 - val_accuracy: 0.5231\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7294 - accuracy: 0.6910 - val_loss: 0.6039 - val_accuracy: 0.7269\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6977 - accuracy: 0.7130 - val_loss: 0.6478 - val_accuracy: 0.7269\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6708 - accuracy: 0.6991 - val_loss: 2.2255 - val_accuracy: 0.4583\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6879 - accuracy: 0.7130 - val_loss: 0.6061 - val_accuracy: 0.7176\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.7611 - accuracy: 0.6644 - val_loss: 0.5448 - val_accuracy: 0.7454\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7128 - accuracy: 0.6887 - val_loss: 0.7228 - val_accuracy: 0.6898\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7306 - accuracy: 0.6887 - val_loss: 1.0412 - val_accuracy: 0.6065\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7133 - accuracy: 0.6794 - val_loss: 0.5700 - val_accuracy: 0.7361\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7059 - accuracy: 0.7025 - val_loss: 0.7382 - val_accuracy: 0.6574\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6775 - accuracy: 0.6956 - val_loss: 0.7661 - val_accuracy: 0.6528\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6518 - accuracy: 0.7141 - val_loss: 0.9629 - val_accuracy: 0.5417\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6633 - accuracy: 0.7060 - val_loss: 0.5507 - val_accuracy: 0.7222\n",
      "Epoch 29/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6643 - accuracy: 0.7002 - val_loss: 0.6009 - val_accuracy: 0.6852\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6938 - accuracy: 0.6875 - val_loss: 0.7302 - val_accuracy: 0.6713\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7255 - accuracy: 0.6690 - val_loss: 0.7946 - val_accuracy: 0.6620\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6632 - accuracy: 0.7014 - val_loss: 0.8405 - val_accuracy: 0.6806\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6835 - accuracy: 0.6887 - val_loss: 0.6684 - val_accuracy: 0.7130\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6852 - accuracy: 0.6782 - val_loss: 1.6882 - val_accuracy: 0.5648\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6846 - accuracy: 0.6968 - val_loss: 0.5468 - val_accuracy: 0.7037\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6722 - accuracy: 0.6991 - val_loss: 1.0380 - val_accuracy: 0.5972\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6626 - accuracy: 0.6956 - val_loss: 1.3277 - val_accuracy: 0.5972\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6885 - accuracy: 0.6944 - val_loss: 0.6865 - val_accuracy: 0.6759\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6621 - accuracy: 0.7095 - val_loss: 0.6061 - val_accuracy: 0.6852\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6670 - accuracy: 0.6979 - val_loss: 0.8120 - val_accuracy: 0.7222\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6176 - accuracy: 0.7130 - val_loss: 0.6074 - val_accuracy: 0.7130\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6480 - accuracy: 0.6875 - val_loss: 0.5975 - val_accuracy: 0.6481\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6612 - accuracy: 0.7037 - val_loss: 0.6021 - val_accuracy: 0.6667\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6363 - accuracy: 0.7141 - val_loss: 0.5939 - val_accuracy: 0.7315\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6326 - accuracy: 0.6921 - val_loss: 0.6496 - val_accuracy: 0.7222\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6471 - accuracy: 0.7176 - val_loss: 1.2423 - val_accuracy: 0.6481\n",
      "Epoch 47/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6518 - accuracy: 0.7014 - val_loss: 0.6459 - val_accuracy: 0.6806\n",
      "Epoch 48/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6511 - accuracy: 0.6968 - val_loss: 0.5156 - val_accuracy: 0.7176\n",
      "Epoch 49/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6361 - accuracy: 0.7049 - val_loss: 0.5592 - val_accuracy: 0.6898\n",
      "Epoch 50/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6678 - accuracy: 0.6782 - val_loss: 0.5478 - val_accuracy: 0.6944\n",
      "Epoch 51/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6253 - accuracy: 0.7037 - val_loss: 0.6425 - val_accuracy: 0.6528\n",
      "Epoch 52/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6184 - accuracy: 0.6991 - val_loss: 0.6368 - val_accuracy: 0.6806\n",
      "Epoch 53/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6066 - accuracy: 0.7245 - val_loss: 0.6815 - val_accuracy: 0.6898\n",
      "Epoch 54/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6252 - accuracy: 0.7049 - val_loss: 0.6655 - val_accuracy: 0.6898\n",
      "Epoch 55/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6063 - accuracy: 0.7049 - val_loss: 0.5526 - val_accuracy: 0.7269\n",
      "Epoch 56/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6024 - accuracy: 0.7176 - val_loss: 0.7279 - val_accuracy: 0.6574\n",
      "Epoch 57/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5901 - accuracy: 0.7222 - val_loss: 0.6925 - val_accuracy: 0.6574\n",
      "Epoch 58/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6208 - accuracy: 0.7025 - val_loss: 0.6581 - val_accuracy: 0.7037\n",
      "Epoch 59/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6269 - accuracy: 0.7049 - val_loss: 0.7671 - val_accuracy: 0.6389\n",
      "Epoch 60/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5747 - accuracy: 0.7153 - val_loss: 0.7569 - val_accuracy: 0.6806\n",
      "Epoch 61/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6431 - accuracy: 0.6979 - val_loss: 0.6397 - val_accuracy: 0.7222\n",
      "Epoch 62/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6041 - accuracy: 0.7176 - val_loss: 0.5311 - val_accuracy: 0.7361\n",
      "Epoch 63/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6743 - accuracy: 0.6840 - val_loss: 0.6805 - val_accuracy: 0.6713\n",
      "Epoch 64/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6265 - accuracy: 0.7083 - val_loss: 0.5395 - val_accuracy: 0.7546\n",
      "Epoch 65/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6367 - accuracy: 0.6933 - val_loss: 0.5598 - val_accuracy: 0.7222\n",
      "Epoch 66/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6066 - accuracy: 0.7199 - val_loss: 0.6675 - val_accuracy: 0.6343\n",
      "Epoch 67/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6169 - accuracy: 0.7164 - val_loss: 0.6786 - val_accuracy: 0.6898\n",
      "Epoch 68/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8413 - accuracy: 0.6435 - val_loss: 0.7260 - val_accuracy: 0.6435\n",
      "Epoch 69/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6252 - accuracy: 0.7188 - val_loss: 0.4975 - val_accuracy: 0.7130\n",
      "Epoch 70/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6146 - accuracy: 0.7095 - val_loss: 0.5829 - val_accuracy: 0.6574\n",
      "Epoch 71/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6200 - accuracy: 0.7153 - val_loss: 0.5315 - val_accuracy: 0.7130\n",
      "Epoch 72/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6199 - accuracy: 0.7106 - val_loss: 0.5502 - val_accuracy: 0.6574\n",
      "Epoch 73/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6311 - accuracy: 0.7025 - val_loss: 0.5289 - val_accuracy: 0.7222\n",
      "Epoch 74/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5559 - accuracy: 0.7315 - val_loss: 0.5437 - val_accuracy: 0.6667\n",
      "Epoch 75/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6252 - accuracy: 0.6991 - val_loss: 0.6229 - val_accuracy: 0.6713\n",
      "Epoch 76/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6511 - accuracy: 0.7269 - val_loss: 0.7554 - val_accuracy: 0.6852\n",
      "Epoch 77/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6275 - accuracy: 0.7176 - val_loss: 0.5039 - val_accuracy: 0.7824\n",
      "Epoch 78/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6012 - accuracy: 0.7106 - val_loss: 1.2663 - val_accuracy: 0.4907\n",
      "Epoch 79/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6502 - accuracy: 0.7060 - val_loss: 0.7466 - val_accuracy: 0.6250\n",
      "Epoch 80/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6394 - accuracy: 0.7153 - val_loss: 0.5361 - val_accuracy: 0.6944\n",
      "Epoch 81/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5778 - accuracy: 0.7130 - val_loss: 0.7437 - val_accuracy: 0.6574\n",
      "Epoch 82/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6734 - accuracy: 0.6910 - val_loss: 0.5877 - val_accuracy: 0.7176\n",
      "Epoch 83/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6371 - accuracy: 0.7118 - val_loss: 0.5393 - val_accuracy: 0.7130\n",
      "Epoch 84/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6268 - accuracy: 0.6910 - val_loss: 0.5008 - val_accuracy: 0.7407\n",
      "Epoch 85/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6358 - accuracy: 0.7211 - val_loss: 0.5599 - val_accuracy: 0.7130\n",
      "Epoch 86/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6003 - accuracy: 0.6921 - val_loss: 0.5440 - val_accuracy: 0.7130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5410 - accuracy: 0.7604 - val_loss: 0.4793 - val_accuracy: 0.7222\n",
      "Epoch 88/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6000 - accuracy: 0.7025 - val_loss: 0.4718 - val_accuracy: 0.7407\n",
      "Epoch 89/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6029 - accuracy: 0.7072 - val_loss: 0.5357 - val_accuracy: 0.7315\n",
      "Epoch 90/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6517 - accuracy: 0.7176 - val_loss: 1.2760 - val_accuracy: 0.4861\n",
      "Epoch 91/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7272 - accuracy: 0.6748 - val_loss: 0.5729 - val_accuracy: 0.6852\n",
      "Epoch 92/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6128 - accuracy: 0.7176 - val_loss: 0.6942 - val_accuracy: 0.6667\n",
      "Epoch 93/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6761 - accuracy: 0.7002 - val_loss: 0.5785 - val_accuracy: 0.6667\n",
      "Epoch 94/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6201 - accuracy: 0.7141 - val_loss: 0.6953 - val_accuracy: 0.6852\n",
      "Epoch 95/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7273 - accuracy: 0.6898 - val_loss: 0.6259 - val_accuracy: 0.6944\n",
      "Epoch 96/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5922 - accuracy: 0.7118 - val_loss: 0.4877 - val_accuracy: 0.7269\n",
      "Epoch 97/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5837 - accuracy: 0.7130 - val_loss: 0.9266 - val_accuracy: 0.7037\n",
      "Epoch 98/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6108 - accuracy: 0.6968 - val_loss: 0.5659 - val_accuracy: 0.6806\n",
      "Epoch 99/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5758 - accuracy: 0.7338 - val_loss: 0.5403 - val_accuracy: 0.6667\n",
      "Epoch 100/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5813 - accuracy: 0.7257 - val_loss: 0.5582 - val_accuracy: 0.6759\n",
      "Epoch 101/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6263 - accuracy: 0.7199 - val_loss: 0.6067 - val_accuracy: 0.7083\n",
      "Epoch 102/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5988 - accuracy: 0.7199 - val_loss: 0.5430 - val_accuracy: 0.7315\n",
      "Epoch 103/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5852 - accuracy: 0.7199 - val_loss: 0.5126 - val_accuracy: 0.6713\n",
      "Epoch 104/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5914 - accuracy: 0.7118 - val_loss: 0.4998 - val_accuracy: 0.7315\n",
      "Epoch 105/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5595 - accuracy: 0.7488 - val_loss: 0.5105 - val_accuracy: 0.7269\n",
      "Epoch 106/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5766 - accuracy: 0.7257 - val_loss: 0.4893 - val_accuracy: 0.7593\n",
      "Epoch 107/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6038 - accuracy: 0.7315 - val_loss: 0.6024 - val_accuracy: 0.6806\n",
      "Epoch 108/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5716 - accuracy: 0.7269 - val_loss: 0.5142 - val_accuracy: 0.7407\n",
      "Epoch 109/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5932 - accuracy: 0.7280 - val_loss: 0.5333 - val_accuracy: 0.6944\n",
      "Epoch 110/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5919 - accuracy: 0.7164 - val_loss: 0.5290 - val_accuracy: 0.7685\n",
      "Epoch 111/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6029 - accuracy: 0.7164 - val_loss: 0.4954 - val_accuracy: 0.7130\n",
      "Epoch 112/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6075 - accuracy: 0.7060 - val_loss: 0.6096 - val_accuracy: 0.6991\n",
      "Epoch 113/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5628 - accuracy: 0.7361 - val_loss: 0.5118 - val_accuracy: 0.6759\n",
      "Epoch 114/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6052 - accuracy: 0.7106 - val_loss: 0.7992 - val_accuracy: 0.7083\n",
      "Epoch 115/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5785 - accuracy: 0.7257 - val_loss: 0.6009 - val_accuracy: 0.7222\n",
      "Epoch 116/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5848 - accuracy: 0.7211 - val_loss: 0.6731 - val_accuracy: 0.7083\n",
      "Epoch 117/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7879 - accuracy: 0.6377 - val_loss: 0.6449 - val_accuracy: 0.6389\n",
      "Epoch 118/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6937 - accuracy: 0.7014 - val_loss: 0.5555 - val_accuracy: 0.7130\n",
      "Epoch 119/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6291 - accuracy: 0.7153 - val_loss: 0.5282 - val_accuracy: 0.7315\n",
      "Epoch 120/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6297 - accuracy: 0.7060 - val_loss: 0.5894 - val_accuracy: 0.6620\n",
      "Epoch 121/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6044 - accuracy: 0.7083 - val_loss: 0.5705 - val_accuracy: 0.6806\n",
      "Epoch 122/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6523 - accuracy: 0.6933 - val_loss: 0.5335 - val_accuracy: 0.7130\n",
      "Epoch 123/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6478 - accuracy: 0.6829 - val_loss: 0.5095 - val_accuracy: 0.7176\n",
      "Epoch 124/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5802 - accuracy: 0.7234 - val_loss: 0.5641 - val_accuracy: 0.6713\n",
      "Epoch 125/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5946 - accuracy: 0.7245 - val_loss: 0.5359 - val_accuracy: 0.6759\n",
      "Epoch 126/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5745 - accuracy: 0.7361 - val_loss: 0.5074 - val_accuracy: 0.7083\n",
      "Epoch 127/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5857 - accuracy: 0.7118 - val_loss: 0.5166 - val_accuracy: 0.7361\n",
      "Epoch 128/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5993 - accuracy: 0.7095 - val_loss: 0.4967 - val_accuracy: 0.7222\n",
      "Epoch 129/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5996 - accuracy: 0.7106 - val_loss: 0.5442 - val_accuracy: 0.6713\n",
      "Epoch 130/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6001 - accuracy: 0.7176 - val_loss: 0.5550 - val_accuracy: 0.6620\n",
      "Epoch 131/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5597 - accuracy: 0.7454 - val_loss: 0.4948 - val_accuracy: 0.7269\n",
      "Epoch 132/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5995 - accuracy: 0.7222 - val_loss: 0.5336 - val_accuracy: 0.7222\n",
      "Epoch 133/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5576 - accuracy: 0.7326 - val_loss: 0.5078 - val_accuracy: 0.7222\n",
      "Epoch 134/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5721 - accuracy: 0.7245 - val_loss: 0.5455 - val_accuracy: 0.6991\n",
      "Epoch 135/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5659 - accuracy: 0.7257 - val_loss: 0.4965 - val_accuracy: 0.7361\n",
      "Epoch 136/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5729 - accuracy: 0.7083 - val_loss: 0.4815 - val_accuracy: 0.7407\n",
      "Epoch 137/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5412 - accuracy: 0.7419 - val_loss: 0.4792 - val_accuracy: 0.7083\n",
      "Epoch 138/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5695 - accuracy: 0.7153 - val_loss: 0.5150 - val_accuracy: 0.7361\n",
      "Epoch 139/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5865 - accuracy: 0.7199 - val_loss: 0.5642 - val_accuracy: 0.7176\n",
      "Epoch 140/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5525 - accuracy: 0.7407 - val_loss: 0.5020 - val_accuracy: 0.7361\n",
      "Epoch 141/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5628 - accuracy: 0.7292 - val_loss: 0.5414 - val_accuracy: 0.6574\n",
      "Epoch 142/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5526 - accuracy: 0.7431 - val_loss: 0.4902 - val_accuracy: 0.7407\n",
      "Epoch 143/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5803 - accuracy: 0.7188 - val_loss: 0.4802 - val_accuracy: 0.7361\n",
      "Epoch 144/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5758 - accuracy: 0.7361 - val_loss: 0.4831 - val_accuracy: 0.7269\n",
      "Epoch 145/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5528 - accuracy: 0.7234 - val_loss: 0.7140 - val_accuracy: 0.6713\n",
      "Epoch 146/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5755 - accuracy: 0.7269 - val_loss: 0.6024 - val_accuracy: 0.6713\n",
      "Epoch 147/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6169 - accuracy: 0.6944 - val_loss: 0.5376 - val_accuracy: 0.6898\n",
      "Epoch 148/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5558 - accuracy: 0.7280 - val_loss: 0.4619 - val_accuracy: 0.7593\n",
      "Epoch 149/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5834 - accuracy: 0.7164 - val_loss: 0.5168 - val_accuracy: 0.7083\n",
      "Epoch 150/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5468 - accuracy: 0.7234 - val_loss: 0.4945 - val_accuracy: 0.7269\n",
      "Epoch 151/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5631 - accuracy: 0.7211 - val_loss: 0.4783 - val_accuracy: 0.7315\n",
      "Epoch 152/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6345 - accuracy: 0.7188 - val_loss: 0.5629 - val_accuracy: 0.6991\n",
      "Epoch 153/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5248 - accuracy: 0.7350 - val_loss: 0.5055 - val_accuracy: 0.6898\n",
      "Epoch 154/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5841 - accuracy: 0.7257 - val_loss: 0.5708 - val_accuracy: 0.7176\n",
      "Epoch 155/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6422 - accuracy: 0.7002 - val_loss: 0.6149 - val_accuracy: 0.6528\n",
      "Epoch 156/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6180 - accuracy: 0.7211 - val_loss: 0.9220 - val_accuracy: 0.7454\n",
      "Epoch 157/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5903 - accuracy: 0.7188 - val_loss: 0.5149 - val_accuracy: 0.7130\n",
      "Epoch 158/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5569 - accuracy: 0.7303 - val_loss: 0.5404 - val_accuracy: 0.7269\n",
      "Epoch 159/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5959 - accuracy: 0.7141 - val_loss: 0.5030 - val_accuracy: 0.7315\n",
      "Epoch 160/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5638 - accuracy: 0.7211 - val_loss: 0.5022 - val_accuracy: 0.7176\n",
      "Epoch 161/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5656 - accuracy: 0.7292 - val_loss: 0.5202 - val_accuracy: 0.7037\n",
      "Epoch 162/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5366 - accuracy: 0.7361 - val_loss: 0.5308 - val_accuracy: 0.7083\n",
      "Epoch 163/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5193 - accuracy: 0.7454 - val_loss: 0.5380 - val_accuracy: 0.6574\n",
      "Epoch 164/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6792 - accuracy: 0.6968 - val_loss: 0.5372 - val_accuracy: 0.7130\n",
      "Epoch 165/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5807 - accuracy: 0.7326 - val_loss: 0.5336 - val_accuracy: 0.7083\n",
      "Epoch 166/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5590 - accuracy: 0.7245 - val_loss: 0.5085 - val_accuracy: 0.7269\n",
      "Epoch 167/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5527 - accuracy: 0.7512 - val_loss: 0.5437 - val_accuracy: 0.7315\n",
      "Epoch 168/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5632 - accuracy: 0.7234 - val_loss: 0.5341 - val_accuracy: 0.7083\n",
      "Epoch 169/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5727 - accuracy: 0.7222 - val_loss: 0.5077 - val_accuracy: 0.7130\n",
      "Epoch 170/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5350 - accuracy: 0.7292 - val_loss: 0.4892 - val_accuracy: 0.7176\n",
      "Epoch 171/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5430 - accuracy: 0.7396 - val_loss: 0.4909 - val_accuracy: 0.7083\n",
      "Epoch 172/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5156 - accuracy: 0.7500 - val_loss: 0.5298 - val_accuracy: 0.6713\n",
      "Epoch 173/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5670 - accuracy: 0.7222 - val_loss: 0.4945 - val_accuracy: 0.7083\n",
      "Epoch 174/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5402 - accuracy: 0.7384 - val_loss: 0.4841 - val_accuracy: 0.7361\n",
      "Epoch 175/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5766 - accuracy: 0.7303 - val_loss: 0.4791 - val_accuracy: 0.7361\n",
      "Epoch 176/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5721 - accuracy: 0.7326 - val_loss: 0.5067 - val_accuracy: 0.6806\n",
      "Epoch 177/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5549 - accuracy: 0.7373 - val_loss: 0.4975 - val_accuracy: 0.6944\n",
      "Epoch 178/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5275 - accuracy: 0.7315 - val_loss: 0.5335 - val_accuracy: 0.6852\n",
      "Epoch 179/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5525 - accuracy: 0.7303 - val_loss: 0.4822 - val_accuracy: 0.7361\n",
      "Epoch 180/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5438 - accuracy: 0.7234 - val_loss: 0.4953 - val_accuracy: 0.7083\n",
      "Epoch 181/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5523 - accuracy: 0.7419 - val_loss: 0.4691 - val_accuracy: 0.7176\n",
      "Epoch 182/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5575 - accuracy: 0.7303 - val_loss: 0.4754 - val_accuracy: 0.7269\n",
      "Epoch 183/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6528 - accuracy: 0.6852 - val_loss: 0.5592 - val_accuracy: 0.7269\n",
      "Epoch 184/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5724 - accuracy: 0.7222 - val_loss: 0.4984 - val_accuracy: 0.7269\n",
      "Epoch 185/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5461 - accuracy: 0.7315 - val_loss: 0.4880 - val_accuracy: 0.7083\n",
      "Epoch 186/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5694 - accuracy: 0.7083 - val_loss: 0.4967 - val_accuracy: 0.6898\n",
      "Epoch 187/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5681 - accuracy: 0.7234 - val_loss: 0.4748 - val_accuracy: 0.7130\n",
      "Epoch 188/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5361 - accuracy: 0.7338 - val_loss: 0.4702 - val_accuracy: 0.7130\n",
      "Epoch 189/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5447 - accuracy: 0.7269 - val_loss: 0.4641 - val_accuracy: 0.7361\n",
      "Epoch 190/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5545 - accuracy: 0.7419 - val_loss: 0.5118 - val_accuracy: 0.6898\n",
      "Epoch 191/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5917 - accuracy: 0.7049 - val_loss: 0.4845 - val_accuracy: 0.6991\n",
      "Epoch 192/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5388 - accuracy: 0.7245 - val_loss: 0.4764 - val_accuracy: 0.7083\n",
      "Epoch 193/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5695 - accuracy: 0.7141 - val_loss: 0.4755 - val_accuracy: 0.7222\n",
      "Epoch 194/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5442 - accuracy: 0.7373 - val_loss: 0.4813 - val_accuracy: 0.7083\n",
      "Epoch 195/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5344 - accuracy: 0.7396 - val_loss: 0.4855 - val_accuracy: 0.7037\n",
      "Epoch 196/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5365 - accuracy: 0.7315 - val_loss: 0.4696 - val_accuracy: 0.7176\n",
      "Epoch 197/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5273 - accuracy: 0.7234 - val_loss: 0.4771 - val_accuracy: 0.7176\n",
      "Epoch 198/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5552 - accuracy: 0.7269 - val_loss: 0.4774 - val_accuracy: 0.7315\n",
      "Epoch 199/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5351 - accuracy: 0.7454 - val_loss: 0.4829 - val_accuracy: 0.6991\n",
      "Epoch 200/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5431 - accuracy: 0.7454 - val_loss: 0.4992 - val_accuracy: 0.6667\n",
      "Epoch 201/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5069 - accuracy: 0.7454 - val_loss: 0.4642 - val_accuracy: 0.7361\n",
      "Epoch 202/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5083 - accuracy: 0.7431 - val_loss: 0.4607 - val_accuracy: 0.7546\n",
      "Epoch 203/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5469 - accuracy: 0.7188 - val_loss: 0.4709 - val_accuracy: 0.7083\n",
      "Epoch 204/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5273 - accuracy: 0.7245 - val_loss: 0.4699 - val_accuracy: 0.7407\n",
      "Epoch 205/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5285 - accuracy: 0.7384 - val_loss: 0.4548 - val_accuracy: 0.7222\n",
      "Epoch 206/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5374 - accuracy: 0.7211 - val_loss: 0.4781 - val_accuracy: 0.7269\n",
      "Epoch 207/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5337 - accuracy: 0.7350 - val_loss: 0.4699 - val_accuracy: 0.7361\n",
      "Epoch 208/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5135 - accuracy: 0.7569 - val_loss: 0.4772 - val_accuracy: 0.7130\n",
      "Epoch 209/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5135 - accuracy: 0.7384 - val_loss: 0.4621 - val_accuracy: 0.7083\n",
      "Epoch 210/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5185 - accuracy: 0.7315 - val_loss: 0.4655 - val_accuracy: 0.7130\n",
      "Epoch 211/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5169 - accuracy: 0.7488 - val_loss: 0.4607 - val_accuracy: 0.7269\n",
      "Epoch 212/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5338 - accuracy: 0.7373 - val_loss: 0.4568 - val_accuracy: 0.7222\n",
      "Epoch 213/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5214 - accuracy: 0.7442 - val_loss: 0.5028 - val_accuracy: 0.7083\n",
      "Epoch 214/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5408 - accuracy: 0.7199 - val_loss: 0.6875 - val_accuracy: 0.6620\n",
      "Epoch 215/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5310 - accuracy: 0.7407 - val_loss: 0.4806 - val_accuracy: 0.7130\n",
      "Epoch 216/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5315 - accuracy: 0.7500 - val_loss: 0.4714 - val_accuracy: 0.6852\n",
      "Epoch 217/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5467 - accuracy: 0.7465 - val_loss: 0.4704 - val_accuracy: 0.6991\n",
      "Epoch 218/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5197 - accuracy: 0.7431 - val_loss: 0.4624 - val_accuracy: 0.7176\n",
      "Epoch 219/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5357 - accuracy: 0.7245 - val_loss: 0.4688 - val_accuracy: 0.7593\n",
      "Epoch 220/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5276 - accuracy: 0.7361 - val_loss: 0.4731 - val_accuracy: 0.7639\n",
      "Epoch 221/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5570 - accuracy: 0.7188 - val_loss: 0.4768 - val_accuracy: 0.7407\n",
      "Epoch 222/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5125 - accuracy: 0.7350 - val_loss: 0.4712 - val_accuracy: 0.7269\n",
      "Epoch 223/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5103 - accuracy: 0.7523 - val_loss: 0.4660 - val_accuracy: 0.7315\n",
      "Epoch 224/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5201 - accuracy: 0.7384 - val_loss: 0.4758 - val_accuracy: 0.7130\n",
      "Epoch 225/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5815 - accuracy: 0.7245 - val_loss: 0.4779 - val_accuracy: 0.7130\n",
      "Epoch 226/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5264 - accuracy: 0.7465 - val_loss: 0.4641 - val_accuracy: 0.7454\n",
      "Epoch 227/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5055 - accuracy: 0.7431 - val_loss: 0.4705 - val_accuracy: 0.7130\n",
      "Epoch 228/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5164 - accuracy: 0.7616 - val_loss: 0.4589 - val_accuracy: 0.7454\n",
      "Epoch 229/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4892 - accuracy: 0.7535 - val_loss: 0.4802 - val_accuracy: 0.6944\n",
      "Epoch 230/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5407 - accuracy: 0.7292 - val_loss: 0.4756 - val_accuracy: 0.7546\n",
      "Epoch 231/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5064 - accuracy: 0.7442 - val_loss: 0.4671 - val_accuracy: 0.7546\n",
      "Epoch 232/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5058 - accuracy: 0.7616 - val_loss: 0.4640 - val_accuracy: 0.7454\n",
      "Epoch 233/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5262 - accuracy: 0.7465 - val_loss: 0.4627 - val_accuracy: 0.7222\n",
      "Epoch 234/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4891 - accuracy: 0.7419 - val_loss: 0.4623 - val_accuracy: 0.7500\n",
      "Epoch 235/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5142 - accuracy: 0.7407 - val_loss: 0.4647 - val_accuracy: 0.7454\n",
      "Epoch 236/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5251 - accuracy: 0.7292 - val_loss: 0.4640 - val_accuracy: 0.7269\n",
      "Epoch 237/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5234 - accuracy: 0.7512 - val_loss: 0.4686 - val_accuracy: 0.7639\n",
      "Epoch 238/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5252 - accuracy: 0.7431 - val_loss: 0.4664 - val_accuracy: 0.7176\n",
      "Epoch 239/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5057 - accuracy: 0.7465 - val_loss: 0.4720 - val_accuracy: 0.7222\n",
      "Epoch 240/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5339 - accuracy: 0.7407 - val_loss: 0.4756 - val_accuracy: 0.7222\n",
      "Epoch 241/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5500 - accuracy: 0.7106 - val_loss: 0.4776 - val_accuracy: 0.7315\n",
      "Epoch 242/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4973 - accuracy: 0.7558 - val_loss: 0.4685 - val_accuracy: 0.7315\n",
      "Epoch 243/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5057 - accuracy: 0.7442 - val_loss: 0.4665 - val_accuracy: 0.7269\n",
      "Epoch 244/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4973 - accuracy: 0.7604 - val_loss: 0.4631 - val_accuracy: 0.7315\n",
      "Epoch 245/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5290 - accuracy: 0.7326 - val_loss: 0.4607 - val_accuracy: 0.7269\n",
      "Epoch 246/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5001 - accuracy: 0.7465 - val_loss: 0.4617 - val_accuracy: 0.7315\n",
      "Epoch 247/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5241 - accuracy: 0.7222 - val_loss: 0.4670 - val_accuracy: 0.7407\n",
      "Epoch 248/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4889 - accuracy: 0.7604 - val_loss: 0.4698 - val_accuracy: 0.7176\n",
      "Epoch 249/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5071 - accuracy: 0.7650 - val_loss: 0.4709 - val_accuracy: 0.7407\n",
      "Epoch 250/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5445 - accuracy: 0.7442 - val_loss: 0.4647 - val_accuracy: 0.7315\n",
      "Epoch 251/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5046 - accuracy: 0.7523 - val_loss: 0.4673 - val_accuracy: 0.7176\n",
      "Epoch 252/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5027 - accuracy: 0.7407 - val_loss: 0.4655 - val_accuracy: 0.7222\n",
      "Epoch 253/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4979 - accuracy: 0.7546 - val_loss: 0.4662 - val_accuracy: 0.7222\n",
      "Epoch 254/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5018 - accuracy: 0.7558 - val_loss: 0.4618 - val_accuracy: 0.6944\n",
      "Epoch 255/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5069 - accuracy: 0.7373 - val_loss: 0.4617 - val_accuracy: 0.7083\n",
      "Epoch 256/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4998 - accuracy: 0.7431 - val_loss: 0.4669 - val_accuracy: 0.7130\n",
      "Epoch 257/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5116 - accuracy: 0.7442 - val_loss: 0.4675 - val_accuracy: 0.7130\n",
      "Epoch 258/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4938 - accuracy: 0.7512 - val_loss: 0.4613 - val_accuracy: 0.7176\n",
      "Epoch 259/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5033 - accuracy: 0.7361 - val_loss: 0.4631 - val_accuracy: 0.7454\n",
      "Epoch 260/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5095 - accuracy: 0.7442 - val_loss: 0.4614 - val_accuracy: 0.7361\n",
      "Epoch 261/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4960 - accuracy: 0.7558 - val_loss: 0.4632 - val_accuracy: 0.7500\n",
      "Epoch 262/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5606 - accuracy: 0.7396 - val_loss: 0.4638 - val_accuracy: 0.7454\n",
      "Epoch 263/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4875 - accuracy: 0.7407 - val_loss: 0.4639 - val_accuracy: 0.7500\n",
      "Epoch 264/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5003 - accuracy: 0.7535 - val_loss: 0.4649 - val_accuracy: 0.7454\n",
      "Epoch 265/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4933 - accuracy: 0.7535 - val_loss: 0.4610 - val_accuracy: 0.7454\n",
      "Epoch 266/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4911 - accuracy: 0.7477 - val_loss: 0.4614 - val_accuracy: 0.7407\n",
      "Epoch 267/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5189 - accuracy: 0.7361 - val_loss: 0.4639 - val_accuracy: 0.7407\n",
      "Epoch 268/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4890 - accuracy: 0.7627 - val_loss: 0.4626 - val_accuracy: 0.7222\n",
      "Epoch 269/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5023 - accuracy: 0.7373 - val_loss: 0.4584 - val_accuracy: 0.7500\n",
      "Epoch 270/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4930 - accuracy: 0.7593 - val_loss: 0.4576 - val_accuracy: 0.7639\n",
      "Epoch 271/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4934 - accuracy: 0.7465 - val_loss: 0.4621 - val_accuracy: 0.7222\n",
      "Epoch 272/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4800 - accuracy: 0.7465 - val_loss: 0.4620 - val_accuracy: 0.7407\n",
      "Epoch 273/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5091 - accuracy: 0.7465 - val_loss: 0.4587 - val_accuracy: 0.7546\n",
      "Epoch 274/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4939 - accuracy: 0.7512 - val_loss: 0.4585 - val_accuracy: 0.7407\n",
      "Epoch 275/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4743 - accuracy: 0.7627 - val_loss: 0.4569 - val_accuracy: 0.7593\n",
      "Epoch 276/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5038 - accuracy: 0.7442 - val_loss: 0.4587 - val_accuracy: 0.7407\n",
      "Epoch 277/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4967 - accuracy: 0.7535 - val_loss: 0.4580 - val_accuracy: 0.7454\n",
      "Epoch 278/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5039 - accuracy: 0.7419 - val_loss: 0.4575 - val_accuracy: 0.7454\n",
      "Epoch 279/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4970 - accuracy: 0.7477 - val_loss: 0.4585 - val_accuracy: 0.7407\n",
      "Epoch 280/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5061 - accuracy: 0.7442 - val_loss: 0.4590 - val_accuracy: 0.7407\n",
      "Epoch 281/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5126 - accuracy: 0.7465 - val_loss: 0.4553 - val_accuracy: 0.7454\n",
      "Epoch 282/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5099 - accuracy: 0.7350 - val_loss: 0.4580 - val_accuracy: 0.7361\n",
      "Epoch 283/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4662 - accuracy: 0.7662 - val_loss: 0.4603 - val_accuracy: 0.7315\n",
      "Epoch 284/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5066 - accuracy: 0.7523 - val_loss: 0.4558 - val_accuracy: 0.7454\n",
      "Epoch 285/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5127 - accuracy: 0.7373 - val_loss: 0.4577 - val_accuracy: 0.7454\n",
      "Epoch 286/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4897 - accuracy: 0.7500 - val_loss: 0.4550 - val_accuracy: 0.7546\n",
      "Epoch 287/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4964 - accuracy: 0.7662 - val_loss: 0.4563 - val_accuracy: 0.7407\n",
      "Epoch 288/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5043 - accuracy: 0.7477 - val_loss: 0.4572 - val_accuracy: 0.7500\n",
      "Epoch 289/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5004 - accuracy: 0.7558 - val_loss: 0.4596 - val_accuracy: 0.7407\n",
      "Epoch 290/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4855 - accuracy: 0.7639 - val_loss: 0.4561 - val_accuracy: 0.7593\n",
      "Epoch 291/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4929 - accuracy: 0.7616 - val_loss: 0.4565 - val_accuracy: 0.7500\n",
      "Epoch 292/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5009 - accuracy: 0.7662 - val_loss: 0.4564 - val_accuracy: 0.7407\n",
      "Epoch 293/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4748 - accuracy: 0.7650 - val_loss: 0.4561 - val_accuracy: 0.7407\n",
      "Epoch 294/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4999 - accuracy: 0.7488 - val_loss: 0.4556 - val_accuracy: 0.7176\n",
      "Epoch 295/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5134 - accuracy: 0.7326 - val_loss: 0.4577 - val_accuracy: 0.7269\n",
      "Epoch 296/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.4763 - accuracy: 0.7569 - val_loss: 0.4554 - val_accuracy: 0.7454\n",
      "Epoch 297/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5122 - accuracy: 0.7407 - val_loss: 0.4566 - val_accuracy: 0.7269\n",
      "Epoch 298/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5027 - accuracy: 0.7477 - val_loss: 0.4571 - val_accuracy: 0.7361\n",
      "Epoch 299/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5038 - accuracy: 0.7523 - val_loss: 0.4568 - val_accuracy: 0.7454\n",
      "Epoch 300/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5032 - accuracy: 0.7685 - val_loss: 0.4558 - val_accuracy: 0.7454\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "(None, 1536)\n",
      "Epoch 1/300\n",
      "87/87 [==============================] - 2s 11ms/step - loss: 1.6448 - accuracy: 0.4016 - val_loss: 2.3378 - val_accuracy: 0.4769\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.2348 - accuracy: 0.4919 - val_loss: 1.7062 - val_accuracy: 0.3426\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0934 - accuracy: 0.5312 - val_loss: 2.0271 - val_accuracy: 0.2963\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.1375 - accuracy: 0.5023 - val_loss: 1.1691 - val_accuracy: 0.4815\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0375 - accuracy: 0.5845 - val_loss: 1.1335 - val_accuracy: 0.5185\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.9709 - accuracy: 0.5914 - val_loss: 1.0476 - val_accuracy: 0.5648\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9293 - accuracy: 0.6076 - val_loss: 1.5697 - val_accuracy: 0.4398\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8880 - accuracy: 0.6111 - val_loss: 0.9204 - val_accuracy: 0.6111\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8540 - accuracy: 0.6285 - val_loss: 0.8516 - val_accuracy: 0.6343\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8541 - accuracy: 0.6296 - val_loss: 0.6737 - val_accuracy: 0.7083\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7742 - accuracy: 0.6690 - val_loss: 0.8298 - val_accuracy: 0.6250\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8056 - accuracy: 0.6331 - val_loss: 1.0103 - val_accuracy: 0.5185\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7999 - accuracy: 0.6609 - val_loss: 0.8705 - val_accuracy: 0.6481\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7769 - accuracy: 0.6551 - val_loss: 1.2110 - val_accuracy: 0.5509\n",
      "Epoch 15/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7306 - accuracy: 0.6713 - val_loss: 0.7158 - val_accuracy: 0.6667\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7374 - accuracy: 0.6667 - val_loss: 1.0474 - val_accuracy: 0.6481\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7052 - accuracy: 0.6852 - val_loss: 0.8006 - val_accuracy: 0.6250\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7091 - accuracy: 0.6852 - val_loss: 0.7919 - val_accuracy: 0.6759\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7613 - accuracy: 0.6725 - val_loss: 0.8255 - val_accuracy: 0.6574\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7075 - accuracy: 0.6759 - val_loss: 0.6445 - val_accuracy: 0.6991\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7068 - accuracy: 0.6817 - val_loss: 0.8060 - val_accuracy: 0.6296\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7246 - accuracy: 0.6863 - val_loss: 0.7330 - val_accuracy: 0.6620\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6754 - accuracy: 0.6794 - val_loss: 0.6364 - val_accuracy: 0.7222\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6746 - accuracy: 0.6968 - val_loss: 0.7354 - val_accuracy: 0.7037\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6605 - accuracy: 0.7014 - val_loss: 0.6723 - val_accuracy: 0.6991\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6788 - accuracy: 0.7176 - val_loss: 0.9367 - val_accuracy: 0.6157\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7226 - accuracy: 0.6690 - val_loss: 0.7665 - val_accuracy: 0.6250\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7897 - accuracy: 0.6505 - val_loss: 0.6581 - val_accuracy: 0.6713\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7652 - accuracy: 0.6609 - val_loss: 0.6685 - val_accuracy: 0.6898\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7276 - accuracy: 0.6829 - val_loss: 0.7314 - val_accuracy: 0.6435\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6711 - accuracy: 0.7211 - val_loss: 0.7351 - val_accuracy: 0.6713\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7406 - accuracy: 0.6736 - val_loss: 1.1070 - val_accuracy: 0.5972\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7553 - accuracy: 0.6539 - val_loss: 0.6470 - val_accuracy: 0.6898\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6660 - accuracy: 0.6806 - val_loss: 0.6066 - val_accuracy: 0.7315\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7024 - accuracy: 0.6701 - val_loss: 0.5487 - val_accuracy: 0.7176\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6950 - accuracy: 0.6794 - val_loss: 0.7381 - val_accuracy: 0.6574\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6405 - accuracy: 0.6991 - val_loss: 0.6410 - val_accuracy: 0.6528\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6551 - accuracy: 0.6898 - val_loss: 0.5783 - val_accuracy: 0.6898\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6382 - accuracy: 0.6898 - val_loss: 1.1124 - val_accuracy: 0.5926\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6330 - accuracy: 0.6933 - val_loss: 0.9209 - val_accuracy: 0.6157\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7351 - accuracy: 0.6725 - val_loss: 0.8163 - val_accuracy: 0.6759\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6687 - accuracy: 0.6933 - val_loss: 0.5743 - val_accuracy: 0.6759\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6315 - accuracy: 0.7164 - val_loss: 0.7935 - val_accuracy: 0.7176\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6837 - accuracy: 0.6829 - val_loss: 0.6740 - val_accuracy: 0.6898\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6308 - accuracy: 0.6968 - val_loss: 0.6868 - val_accuracy: 0.7130\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6473 - accuracy: 0.7025 - val_loss: 0.5945 - val_accuracy: 0.7176\n",
      "Epoch 47/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7247 - accuracy: 0.6817 - val_loss: 0.6751 - val_accuracy: 0.6528\n",
      "Epoch 48/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6607 - accuracy: 0.6840 - val_loss: 0.5825 - val_accuracy: 0.7083\n",
      "Epoch 49/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6640 - accuracy: 0.6944 - val_loss: 0.7227 - val_accuracy: 0.6898\n",
      "Epoch 50/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6441 - accuracy: 0.6991 - val_loss: 0.5788 - val_accuracy: 0.7130\n",
      "Epoch 51/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5920 - accuracy: 0.7245 - val_loss: 0.5602 - val_accuracy: 0.7222\n",
      "Epoch 52/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6267 - accuracy: 0.7153 - val_loss: 0.5608 - val_accuracy: 0.6852\n",
      "Epoch 53/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6623 - accuracy: 0.6968 - val_loss: 0.6649 - val_accuracy: 0.6944\n",
      "Epoch 54/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6467 - accuracy: 0.6794 - val_loss: 0.5555 - val_accuracy: 0.6991\n",
      "Epoch 55/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7055 - accuracy: 0.6933 - val_loss: 0.8074 - val_accuracy: 0.6574\n",
      "Epoch 56/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6902 - accuracy: 0.6725 - val_loss: 0.5422 - val_accuracy: 0.6898\n",
      "Epoch 57/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6379 - accuracy: 0.7106 - val_loss: 0.5965 - val_accuracy: 0.7037\n",
      "Epoch 58/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6308 - accuracy: 0.7025 - val_loss: 0.5821 - val_accuracy: 0.6898\n",
      "Epoch 59/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6642 - accuracy: 0.6944 - val_loss: 0.6784 - val_accuracy: 0.6852\n",
      "Epoch 60/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6521 - accuracy: 0.7049 - val_loss: 0.7668 - val_accuracy: 0.6806\n",
      "Epoch 61/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6028 - accuracy: 0.7002 - val_loss: 0.6005 - val_accuracy: 0.6852\n",
      "Epoch 62/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6654 - accuracy: 0.7002 - val_loss: 0.6041 - val_accuracy: 0.7315\n",
      "Epoch 63/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5968 - accuracy: 0.7072 - val_loss: 0.5620 - val_accuracy: 0.7083\n",
      "Epoch 64/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6612 - accuracy: 0.6817 - val_loss: 0.8284 - val_accuracy: 0.6759\n",
      "Epoch 65/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6330 - accuracy: 0.6968 - val_loss: 0.5173 - val_accuracy: 0.7222\n",
      "Epoch 66/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6483 - accuracy: 0.6968 - val_loss: 0.8108 - val_accuracy: 0.7037\n",
      "Epoch 67/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6460 - accuracy: 0.6956 - val_loss: 0.5629 - val_accuracy: 0.7176\n",
      "Epoch 68/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6199 - accuracy: 0.7083 - val_loss: 0.6170 - val_accuracy: 0.7454\n",
      "Epoch 69/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6329 - accuracy: 0.7049 - val_loss: 0.6870 - val_accuracy: 0.6944\n",
      "Epoch 70/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7644 - accuracy: 0.6667 - val_loss: 0.7210 - val_accuracy: 0.6528\n",
      "Epoch 71/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6586 - accuracy: 0.7153 - val_loss: 0.5821 - val_accuracy: 0.6620\n",
      "Epoch 72/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6374 - accuracy: 0.7060 - val_loss: 0.5358 - val_accuracy: 0.7315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6615 - accuracy: 0.6852 - val_loss: 0.5478 - val_accuracy: 0.7037\n",
      "Epoch 74/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6088 - accuracy: 0.7164 - val_loss: 0.5254 - val_accuracy: 0.6944\n",
      "Epoch 75/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6283 - accuracy: 0.6956 - val_loss: 0.4927 - val_accuracy: 0.7407\n",
      "Epoch 76/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6070 - accuracy: 0.7095 - val_loss: 0.6146 - val_accuracy: 0.6991\n",
      "Epoch 77/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6135 - accuracy: 0.6979 - val_loss: 0.5043 - val_accuracy: 0.6944\n",
      "Epoch 78/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6244 - accuracy: 0.6979 - val_loss: 0.8959 - val_accuracy: 0.6435\n",
      "Epoch 79/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6506 - accuracy: 0.6863 - val_loss: 0.6285 - val_accuracy: 0.6296\n",
      "Epoch 80/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5896 - accuracy: 0.7153 - val_loss: 0.5889 - val_accuracy: 0.7176\n",
      "Epoch 81/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5967 - accuracy: 0.7049 - val_loss: 0.4963 - val_accuracy: 0.6991\n",
      "Epoch 82/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6061 - accuracy: 0.6991 - val_loss: 0.5494 - val_accuracy: 0.7222\n",
      "Epoch 83/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5921 - accuracy: 0.7153 - val_loss: 0.5103 - val_accuracy: 0.7130\n",
      "Epoch 84/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6173 - accuracy: 0.6898 - val_loss: 0.5372 - val_accuracy: 0.7130\n",
      "Epoch 85/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6413 - accuracy: 0.7060 - val_loss: 0.6968 - val_accuracy: 0.6620\n",
      "Epoch 86/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6193 - accuracy: 0.7106 - val_loss: 3.1391 - val_accuracy: 0.5185\n",
      "Epoch 87/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6117 - accuracy: 0.6771 - val_loss: 0.4961 - val_accuracy: 0.7083\n",
      "Epoch 88/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6163 - accuracy: 0.6968 - val_loss: 0.5539 - val_accuracy: 0.7130\n",
      "Epoch 89/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6233 - accuracy: 0.7014 - val_loss: 0.8720 - val_accuracy: 0.5972\n",
      "Epoch 90/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6179 - accuracy: 0.7095 - val_loss: 0.5271 - val_accuracy: 0.7083\n",
      "Epoch 91/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6460 - accuracy: 0.6968 - val_loss: 0.5249 - val_accuracy: 0.7685\n",
      "Epoch 92/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5845 - accuracy: 0.7095 - val_loss: 0.6133 - val_accuracy: 0.6944\n",
      "Epoch 93/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5650 - accuracy: 0.7130 - val_loss: 0.8691 - val_accuracy: 0.6759\n",
      "Epoch 94/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6125 - accuracy: 0.7095 - val_loss: 0.5570 - val_accuracy: 0.6667\n",
      "Epoch 95/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6175 - accuracy: 0.7002 - val_loss: 0.5877 - val_accuracy: 0.7222\n",
      "Epoch 96/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5909 - accuracy: 0.6887 - val_loss: 0.5052 - val_accuracy: 0.7176\n",
      "Epoch 97/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5806 - accuracy: 0.7060 - val_loss: 0.5320 - val_accuracy: 0.7176\n",
      "Epoch 98/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5971 - accuracy: 0.7014 - val_loss: 0.6529 - val_accuracy: 0.6806\n",
      "Epoch 99/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5845 - accuracy: 0.6817 - val_loss: 0.7686 - val_accuracy: 0.6713\n",
      "Epoch 100/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5907 - accuracy: 0.7130 - val_loss: 0.5056 - val_accuracy: 0.7269\n",
      "Epoch 101/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5835 - accuracy: 0.7014 - val_loss: 0.5296 - val_accuracy: 0.6898\n",
      "Epoch 102/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6183 - accuracy: 0.6968 - val_loss: 0.8831 - val_accuracy: 0.6296\n",
      "Epoch 103/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5845 - accuracy: 0.7164 - val_loss: 0.5229 - val_accuracy: 0.7130\n",
      "Epoch 104/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5772 - accuracy: 0.7037 - val_loss: 0.4974 - val_accuracy: 0.7315\n",
      "Epoch 105/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6135 - accuracy: 0.6956 - val_loss: 0.5578 - val_accuracy: 0.6898\n",
      "Epoch 106/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6076 - accuracy: 0.6887 - val_loss: 1.0323 - val_accuracy: 0.5556\n",
      "Epoch 107/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5997 - accuracy: 0.7037 - val_loss: 0.6735 - val_accuracy: 0.7130\n",
      "Epoch 108/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5950 - accuracy: 0.6979 - val_loss: 0.5099 - val_accuracy: 0.6991\n",
      "Epoch 109/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5804 - accuracy: 0.7106 - val_loss: 0.5583 - val_accuracy: 0.6898\n",
      "Epoch 110/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5625 - accuracy: 0.7072 - val_loss: 0.6686 - val_accuracy: 0.6667\n",
      "Epoch 111/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5903 - accuracy: 0.7141 - val_loss: 0.5443 - val_accuracy: 0.7176\n",
      "Epoch 112/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5862 - accuracy: 0.7234 - val_loss: 0.6647 - val_accuracy: 0.7315\n",
      "Epoch 113/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5705 - accuracy: 0.6979 - val_loss: 0.5627 - val_accuracy: 0.7269\n",
      "Epoch 114/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5726 - accuracy: 0.7222 - val_loss: 0.5293 - val_accuracy: 0.7176\n",
      "Epoch 115/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5945 - accuracy: 0.6991 - val_loss: 0.4768 - val_accuracy: 0.7269\n",
      "Epoch 116/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5810 - accuracy: 0.6979 - val_loss: 0.5152 - val_accuracy: 0.7222\n",
      "Epoch 117/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5450 - accuracy: 0.7245 - val_loss: 0.5625 - val_accuracy: 0.7176\n",
      "Epoch 118/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.5509 - accuracy: 0.7199 - val_loss: 0.5249 - val_accuracy: 0.7130\n",
      "Epoch 119/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6390 - accuracy: 0.6991 - val_loss: 1.4850 - val_accuracy: 0.5231\n",
      "Epoch 120/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7492 - accuracy: 0.6597 - val_loss: 0.6099 - val_accuracy: 0.7176\n",
      "Epoch 121/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8007 - accuracy: 0.6458 - val_loss: 0.7160 - val_accuracy: 0.7407\n",
      "Epoch 122/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7584 - accuracy: 0.6806 - val_loss: 0.6242 - val_accuracy: 0.7037\n",
      "Epoch 123/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7264 - accuracy: 0.6701 - val_loss: 0.6877 - val_accuracy: 0.6759\n",
      "Epoch 124/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7650 - accuracy: 0.6690 - val_loss: 1.7465 - val_accuracy: 0.4861\n",
      "Epoch 125/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6713 - accuracy: 0.6979 - val_loss: 1.5201 - val_accuracy: 0.4907\n",
      "Epoch 126/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7105 - accuracy: 0.6597 - val_loss: 0.6470 - val_accuracy: 0.6944\n",
      "Epoch 127/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7132 - accuracy: 0.6655 - val_loss: 0.5599 - val_accuracy: 0.7407\n",
      "Epoch 128/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6571 - accuracy: 0.6944 - val_loss: 0.5471 - val_accuracy: 0.6759\n",
      "Epoch 129/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6756 - accuracy: 0.7014 - val_loss: 0.6080 - val_accuracy: 0.7407\n",
      "Epoch 130/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6489 - accuracy: 0.7014 - val_loss: 0.6914 - val_accuracy: 0.7176\n",
      "Epoch 131/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7319 - accuracy: 0.6690 - val_loss: 1.2002 - val_accuracy: 0.4167\n",
      "Epoch 132/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6784 - accuracy: 0.6806 - val_loss: 0.8017 - val_accuracy: 0.6528\n",
      "Epoch 133/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6992 - accuracy: 0.6887 - val_loss: 0.7971 - val_accuracy: 0.6944\n",
      "Epoch 134/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6546 - accuracy: 0.6887 - val_loss: 0.7900 - val_accuracy: 0.5648\n",
      "Epoch 135/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6628 - accuracy: 0.6829 - val_loss: 0.6144 - val_accuracy: 0.7176\n",
      "Epoch 136/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6505 - accuracy: 0.6782 - val_loss: 0.5721 - val_accuracy: 0.6667\n",
      "Epoch 137/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6250 - accuracy: 0.7211 - val_loss: 0.5834 - val_accuracy: 0.6852\n",
      "Epoch 138/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6279 - accuracy: 0.7095 - val_loss: 0.6615 - val_accuracy: 0.6481\n",
      "Epoch 139/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7118 - accuracy: 0.6979 - val_loss: 0.6067 - val_accuracy: 0.7083\n",
      "Epoch 140/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6542 - accuracy: 0.7025 - val_loss: 0.5502 - val_accuracy: 0.7176\n",
      "Epoch 141/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6374 - accuracy: 0.6852 - val_loss: 0.5244 - val_accuracy: 0.7176\n",
      "Epoch 142/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6253 - accuracy: 0.7060 - val_loss: 0.5601 - val_accuracy: 0.7037\n",
      "Epoch 143/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6525 - accuracy: 0.6944 - val_loss: 0.6790 - val_accuracy: 0.6620\n",
      "Epoch 144/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6401 - accuracy: 0.6979 - val_loss: 0.5494 - val_accuracy: 0.6944\n",
      "Epoch 145/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6367 - accuracy: 0.7176 - val_loss: 0.9965 - val_accuracy: 0.6019\n",
      "Epoch 146/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6093 - accuracy: 0.7106 - val_loss: 0.5506 - val_accuracy: 0.6852\n",
      "Epoch 147/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6578 - accuracy: 0.6840 - val_loss: 0.5395 - val_accuracy: 0.7037\n",
      "Epoch 148/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6518 - accuracy: 0.6852 - val_loss: 0.5577 - val_accuracy: 0.7315\n",
      "Epoch 149/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6214 - accuracy: 0.6863 - val_loss: 0.5444 - val_accuracy: 0.6806\n",
      "Epoch 150/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6501 - accuracy: 0.6921 - val_loss: 0.5230 - val_accuracy: 0.7361\n",
      "Epoch 151/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6142 - accuracy: 0.7188 - val_loss: 0.5048 - val_accuracy: 0.7176\n",
      "Epoch 152/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6320 - accuracy: 0.7118 - val_loss: 0.5436 - val_accuracy: 0.6806\n",
      "Epoch 153/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6268 - accuracy: 0.7153 - val_loss: 0.5047 - val_accuracy: 0.7407\n",
      "Epoch 154/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6915 - accuracy: 0.6875 - val_loss: 0.7736 - val_accuracy: 0.5972\n",
      "Epoch 155/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6204 - accuracy: 0.6863 - val_loss: 0.5265 - val_accuracy: 0.7083\n",
      "Epoch 156/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6492 - accuracy: 0.7002 - val_loss: 0.5671 - val_accuracy: 0.6759\n",
      "Epoch 157/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6270 - accuracy: 0.7141 - val_loss: 0.5269 - val_accuracy: 0.7176\n",
      "Epoch 158/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6464 - accuracy: 0.6887 - val_loss: 0.5454 - val_accuracy: 0.6898\n",
      "Epoch 159/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6253 - accuracy: 0.7164 - val_loss: 1.1837 - val_accuracy: 0.5648\n",
      "Epoch 160/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6287 - accuracy: 0.7072 - val_loss: 0.5078 - val_accuracy: 0.7083\n",
      "Epoch 161/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6362 - accuracy: 0.7106 - val_loss: 1.8706 - val_accuracy: 0.5093\n",
      "Epoch 162/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9867 - accuracy: 0.5544 - val_loss: 0.7904 - val_accuracy: 0.6157\n",
      "Epoch 163/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9619 - accuracy: 0.5544 - val_loss: 0.8454 - val_accuracy: 0.5694\n",
      "Epoch 164/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8887 - accuracy: 0.5926 - val_loss: 0.8330 - val_accuracy: 0.6528\n",
      "Epoch 165/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8853 - accuracy: 0.5868 - val_loss: 0.8079 - val_accuracy: 0.6435\n",
      "Epoch 166/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8785 - accuracy: 0.5903 - val_loss: 0.7484 - val_accuracy: 0.6019\n",
      "Epoch 167/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8448 - accuracy: 0.6088 - val_loss: 0.8156 - val_accuracy: 0.6019\n",
      "Epoch 168/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9043 - accuracy: 0.5995 - val_loss: 0.7342 - val_accuracy: 0.6759\n",
      "Epoch 169/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8136 - accuracy: 0.6146 - val_loss: 0.8393 - val_accuracy: 0.5880\n",
      "Epoch 170/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8551 - accuracy: 0.5949 - val_loss: 0.8146 - val_accuracy: 0.6250\n",
      "Epoch 171/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8027 - accuracy: 0.6354 - val_loss: 0.8417 - val_accuracy: 0.6296\n",
      "Epoch 172/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8518 - accuracy: 0.6215 - val_loss: 0.7775 - val_accuracy: 0.6389\n",
      "Epoch 173/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8195 - accuracy: 0.6204 - val_loss: 0.7306 - val_accuracy: 0.6019\n",
      "Epoch 174/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8180 - accuracy: 0.6238 - val_loss: 0.7841 - val_accuracy: 0.5787\n",
      "Epoch 175/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8514 - accuracy: 0.6215 - val_loss: 0.7492 - val_accuracy: 0.6343\n",
      "Epoch 176/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8337 - accuracy: 0.6250 - val_loss: 0.6927 - val_accuracy: 0.6481\n",
      "Epoch 177/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7888 - accuracy: 0.6458 - val_loss: 0.7177 - val_accuracy: 0.6713\n",
      "Epoch 178/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8366 - accuracy: 0.6377 - val_loss: 0.6681 - val_accuracy: 0.6667\n",
      "Epoch 179/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8451 - accuracy: 0.6285 - val_loss: 0.7604 - val_accuracy: 0.6204\n",
      "Epoch 180/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7600 - accuracy: 0.6609 - val_loss: 0.6587 - val_accuracy: 0.6759\n",
      "Epoch 181/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7884 - accuracy: 0.6389 - val_loss: 0.7296 - val_accuracy: 0.6759\n",
      "Epoch 182/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8042 - accuracy: 0.6609 - val_loss: 0.6469 - val_accuracy: 0.6667\n",
      "Epoch 183/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8025 - accuracy: 0.6481 - val_loss: 0.6279 - val_accuracy: 0.6991\n",
      "Epoch 184/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8097 - accuracy: 0.6505 - val_loss: 0.6648 - val_accuracy: 0.6852\n",
      "Epoch 185/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7689 - accuracy: 0.6632 - val_loss: 0.7359 - val_accuracy: 0.6296\n",
      "Epoch 186/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7589 - accuracy: 0.6655 - val_loss: 0.6379 - val_accuracy: 0.6852\n",
      "Epoch 187/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7957 - accuracy: 0.6644 - val_loss: 0.6890 - val_accuracy: 0.6667\n",
      "Epoch 188/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8069 - accuracy: 0.6389 - val_loss: 0.6336 - val_accuracy: 0.6898\n",
      "Epoch 189/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7667 - accuracy: 0.6528 - val_loss: 0.6703 - val_accuracy: 0.6250\n",
      "Epoch 190/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7821 - accuracy: 0.6516 - val_loss: 0.7246 - val_accuracy: 0.6389\n",
      "Epoch 191/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8345 - accuracy: 0.6354 - val_loss: 0.6995 - val_accuracy: 0.6852\n",
      "Epoch 192/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7758 - accuracy: 0.6609 - val_loss: 0.6462 - val_accuracy: 0.6713\n",
      "Epoch 193/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7844 - accuracy: 0.6354 - val_loss: 0.6666 - val_accuracy: 0.6481\n",
      "Epoch 194/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7932 - accuracy: 0.6539 - val_loss: 0.6289 - val_accuracy: 0.6667\n",
      "Epoch 195/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7670 - accuracy: 0.6806 - val_loss: 0.6620 - val_accuracy: 0.6944\n",
      "Epoch 196/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7627 - accuracy: 0.6644 - val_loss: 0.6532 - val_accuracy: 0.6713\n",
      "Epoch 197/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7464 - accuracy: 0.6586 - val_loss: 0.6473 - val_accuracy: 0.7037\n",
      "Epoch 198/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7699 - accuracy: 0.6620 - val_loss: 0.6022 - val_accuracy: 0.7083\n",
      "Epoch 199/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7564 - accuracy: 0.6655 - val_loss: 8.5937 - val_accuracy: 0.3287\n",
      "Epoch 200/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7534 - accuracy: 0.6713 - val_loss: 0.6116 - val_accuracy: 0.6852\n",
      "Epoch 201/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7653 - accuracy: 0.6655 - val_loss: 0.6006 - val_accuracy: 0.7083\n",
      "Epoch 202/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7417 - accuracy: 0.6771 - val_loss: 0.6034 - val_accuracy: 0.6852\n",
      "Epoch 203/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7533 - accuracy: 0.6806 - val_loss: 0.7271 - val_accuracy: 0.6296\n",
      "Epoch 204/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7450 - accuracy: 0.6748 - val_loss: 0.6070 - val_accuracy: 0.7407\n",
      "Epoch 205/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8240 - accuracy: 0.6586 - val_loss: 0.6437 - val_accuracy: 0.6944\n",
      "Epoch 206/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7739 - accuracy: 0.6655 - val_loss: 0.5930 - val_accuracy: 0.7222\n",
      "Epoch 207/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7959 - accuracy: 0.6470 - val_loss: 0.6184 - val_accuracy: 0.7269\n",
      "Epoch 208/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7845 - accuracy: 0.6412 - val_loss: 0.6416 - val_accuracy: 0.6806\n",
      "Epoch 209/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7591 - accuracy: 0.6539 - val_loss: 0.6609 - val_accuracy: 0.6759\n",
      "Epoch 210/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7424 - accuracy: 0.6782 - val_loss: 0.6323 - val_accuracy: 0.6991\n",
      "Epoch 211/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7349 - accuracy: 0.6771 - val_loss: 0.6315 - val_accuracy: 0.6898\n",
      "Epoch 212/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7258 - accuracy: 0.6921 - val_loss: 0.6718 - val_accuracy: 0.6991\n",
      "Epoch 213/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7639 - accuracy: 0.6644 - val_loss: 0.6491 - val_accuracy: 0.6898\n",
      "Epoch 214/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7549 - accuracy: 0.6725 - val_loss: 0.5891 - val_accuracy: 0.7083\n",
      "Epoch 215/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8376 - accuracy: 0.6238 - val_loss: 0.7320 - val_accuracy: 0.6667\n",
      "Epoch 216/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8232 - accuracy: 0.6470 - val_loss: 0.6663 - val_accuracy: 0.6991\n",
      "Epoch 217/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7744 - accuracy: 0.6574 - val_loss: 0.6488 - val_accuracy: 0.6898\n",
      "Epoch 218/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7683 - accuracy: 0.6748 - val_loss: 0.6176 - val_accuracy: 0.6852\n",
      "Epoch 219/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7624 - accuracy: 0.6516 - val_loss: 0.6344 - val_accuracy: 0.6759\n",
      "Epoch 220/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7515 - accuracy: 0.6620 - val_loss: 0.6089 - val_accuracy: 0.6898\n",
      "Epoch 221/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7834 - accuracy: 0.6505 - val_loss: 0.6002 - val_accuracy: 0.6991\n",
      "Epoch 222/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7410 - accuracy: 0.6852 - val_loss: 0.6133 - val_accuracy: 0.6991\n",
      "Epoch 223/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7393 - accuracy: 0.6528 - val_loss: 0.5954 - val_accuracy: 0.7037\n",
      "Epoch 224/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7429 - accuracy: 0.6736 - val_loss: 0.5895 - val_accuracy: 0.7083\n",
      "Epoch 225/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7349 - accuracy: 0.6806 - val_loss: 0.6297 - val_accuracy: 0.6991\n",
      "Epoch 226/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7612 - accuracy: 0.6759 - val_loss: 0.5996 - val_accuracy: 0.6806\n",
      "Epoch 227/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6993 - accuracy: 0.6898 - val_loss: 0.6290 - val_accuracy: 0.6991\n",
      "Epoch 228/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7520 - accuracy: 0.6632 - val_loss: 0.5908 - val_accuracy: 0.6944\n",
      "Epoch 229/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7396 - accuracy: 0.6817 - val_loss: 0.6147 - val_accuracy: 0.7083\n",
      "Epoch 230/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7261 - accuracy: 0.6863 - val_loss: 0.5739 - val_accuracy: 0.7222\n",
      "Epoch 231/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7214 - accuracy: 0.6678 - val_loss: 0.5952 - val_accuracy: 0.6898\n",
      "Epoch 232/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7133 - accuracy: 0.6748 - val_loss: 0.5809 - val_accuracy: 0.6991\n",
      "Epoch 233/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7278 - accuracy: 0.6979 - val_loss: 0.5893 - val_accuracy: 0.7083\n",
      "Epoch 234/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7376 - accuracy: 0.6678 - val_loss: 0.5901 - val_accuracy: 0.7037\n",
      "Epoch 235/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6797 - accuracy: 0.7060 - val_loss: 0.5920 - val_accuracy: 0.6991\n",
      "Epoch 236/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7220 - accuracy: 0.6933 - val_loss: 0.6119 - val_accuracy: 0.6991\n",
      "Epoch 237/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7425 - accuracy: 0.6678 - val_loss: 0.5749 - val_accuracy: 0.7037\n",
      "Epoch 238/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7081 - accuracy: 0.6759 - val_loss: 0.5835 - val_accuracy: 0.7176\n",
      "Epoch 239/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7643 - accuracy: 0.6574 - val_loss: 0.5910 - val_accuracy: 0.7176\n",
      "Epoch 240/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7278 - accuracy: 0.6586 - val_loss: 0.5728 - val_accuracy: 0.7315\n",
      "Epoch 241/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7600 - accuracy: 0.6609 - val_loss: 0.5821 - val_accuracy: 0.7361\n",
      "Epoch 242/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7277 - accuracy: 0.6840 - val_loss: 0.5725 - val_accuracy: 0.7083\n",
      "Epoch 243/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7168 - accuracy: 0.6852 - val_loss: 0.5729 - val_accuracy: 0.7407\n",
      "Epoch 244/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7397 - accuracy: 0.6597 - val_loss: 0.5691 - val_accuracy: 0.6991\n",
      "Epoch 245/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7319 - accuracy: 0.6806 - val_loss: 0.5705 - val_accuracy: 0.7083\n",
      "Epoch 246/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7529 - accuracy: 0.6748 - val_loss: 0.5781 - val_accuracy: 0.7315\n",
      "Epoch 247/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7617 - accuracy: 0.6586 - val_loss: 0.5838 - val_accuracy: 0.6944\n",
      "Epoch 248/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7377 - accuracy: 0.6586 - val_loss: 0.5897 - val_accuracy: 0.7176\n",
      "Epoch 249/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7555 - accuracy: 0.6632 - val_loss: 0.5783 - val_accuracy: 0.7500\n",
      "Epoch 250/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7525 - accuracy: 0.6875 - val_loss: 0.5715 - val_accuracy: 0.7083\n",
      "Epoch 251/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7519 - accuracy: 0.6713 - val_loss: 0.5629 - val_accuracy: 0.7130\n",
      "Epoch 252/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7305 - accuracy: 0.6748 - val_loss: 0.5776 - val_accuracy: 0.7361\n",
      "Epoch 253/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7193 - accuracy: 0.6678 - val_loss: 0.5713 - val_accuracy: 0.7222\n",
      "Epoch 254/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7124 - accuracy: 0.6840 - val_loss: 0.5827 - val_accuracy: 0.7083\n",
      "Epoch 255/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7309 - accuracy: 0.6701 - val_loss: 0.5782 - val_accuracy: 0.7315\n",
      "Epoch 256/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7010 - accuracy: 0.6956 - val_loss: 0.5843 - val_accuracy: 0.7130\n",
      "Epoch 257/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7099 - accuracy: 0.6898 - val_loss: 0.5699 - val_accuracy: 0.7269\n",
      "Epoch 258/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7353 - accuracy: 0.6887 - val_loss: 0.5685 - val_accuracy: 0.7407\n",
      "Epoch 259/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7393 - accuracy: 0.6817 - val_loss: 0.5888 - val_accuracy: 0.7269\n",
      "Epoch 260/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7185 - accuracy: 0.6748 - val_loss: 0.5698 - val_accuracy: 0.7315\n",
      "Epoch 261/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6976 - accuracy: 0.7049 - val_loss: 0.5710 - val_accuracy: 0.7361\n",
      "Epoch 262/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7022 - accuracy: 0.6944 - val_loss: 0.5678 - val_accuracy: 0.7361\n",
      "Epoch 263/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7107 - accuracy: 0.7060 - val_loss: 0.5762 - val_accuracy: 0.7269\n",
      "Epoch 264/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7336 - accuracy: 0.6806 - val_loss: 0.5670 - val_accuracy: 0.7407\n",
      "Epoch 265/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6952 - accuracy: 0.7164 - val_loss: 0.5727 - val_accuracy: 0.7269\n",
      "Epoch 266/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7146 - accuracy: 0.6991 - val_loss: 0.5892 - val_accuracy: 0.7269\n",
      "Epoch 267/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7406 - accuracy: 0.6817 - val_loss: 0.5890 - val_accuracy: 0.6944\n",
      "Epoch 268/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6899 - accuracy: 0.6875 - val_loss: 0.5750 - val_accuracy: 0.7269\n",
      "Epoch 269/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7398 - accuracy: 0.6910 - val_loss: 0.5689 - val_accuracy: 0.7361\n",
      "Epoch 270/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7441 - accuracy: 0.6806 - val_loss: 0.5764 - val_accuracy: 0.7083\n",
      "Epoch 271/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7384 - accuracy: 0.6817 - val_loss: 0.5725 - val_accuracy: 0.7222\n",
      "Epoch 272/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7301 - accuracy: 0.6887 - val_loss: 0.5742 - val_accuracy: 0.7222\n",
      "Epoch 273/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7315 - accuracy: 0.6748 - val_loss: 0.5657 - val_accuracy: 0.7222\n",
      "Epoch 274/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7209 - accuracy: 0.6968 - val_loss: 0.5678 - val_accuracy: 0.7407\n",
      "Epoch 275/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7084 - accuracy: 0.7002 - val_loss: 0.5692 - val_accuracy: 0.7130\n",
      "Epoch 276/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6860 - accuracy: 0.6840 - val_loss: 0.5718 - val_accuracy: 0.7315\n",
      "Epoch 277/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7001 - accuracy: 0.6852 - val_loss: 0.5714 - val_accuracy: 0.7315\n",
      "Epoch 278/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7336 - accuracy: 0.6840 - val_loss: 0.5697 - val_accuracy: 0.7222\n",
      "Epoch 279/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7049 - accuracy: 0.6875 - val_loss: 0.5648 - val_accuracy: 0.7546\n",
      "Epoch 280/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7313 - accuracy: 0.6725 - val_loss: 0.5659 - val_accuracy: 0.7130\n",
      "Epoch 281/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7419 - accuracy: 0.6944 - val_loss: 0.5789 - val_accuracy: 0.7222\n",
      "Epoch 282/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6904 - accuracy: 0.6944 - val_loss: 0.5713 - val_accuracy: 0.7500\n",
      "Epoch 283/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6902 - accuracy: 0.6782 - val_loss: 0.5690 - val_accuracy: 0.7176\n",
      "Epoch 284/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7428 - accuracy: 0.6713 - val_loss: 0.5738 - val_accuracy: 0.7500\n",
      "Epoch 285/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6724 - accuracy: 0.7118 - val_loss: 0.5697 - val_accuracy: 0.7315\n",
      "Epoch 286/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7137 - accuracy: 0.6944 - val_loss: 0.5652 - val_accuracy: 0.7083\n",
      "Epoch 287/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7370 - accuracy: 0.6771 - val_loss: 0.5611 - val_accuracy: 0.7315\n",
      "Epoch 288/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7003 - accuracy: 0.7095 - val_loss: 0.5637 - val_accuracy: 0.7361\n",
      "Epoch 289/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7292 - accuracy: 0.6852 - val_loss: 0.5551 - val_accuracy: 0.7083\n",
      "Epoch 290/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7082 - accuracy: 0.6875 - val_loss: 0.5722 - val_accuracy: 0.7361\n",
      "Epoch 291/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7274 - accuracy: 0.6852 - val_loss: 0.5789 - val_accuracy: 0.7407\n",
      "Epoch 292/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7049 - accuracy: 0.6840 - val_loss: 0.5628 - val_accuracy: 0.7361\n",
      "Epoch 293/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6923 - accuracy: 0.7037 - val_loss: 0.5659 - val_accuracy: 0.7269\n",
      "Epoch 294/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7043 - accuracy: 0.6748 - val_loss: 0.5699 - val_accuracy: 0.7269\n",
      "Epoch 295/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7117 - accuracy: 0.6933 - val_loss: 0.5622 - val_accuracy: 0.7130\n",
      "Epoch 296/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7137 - accuracy: 0.7083 - val_loss: 0.5717 - val_accuracy: 0.7361\n",
      "Epoch 297/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6932 - accuracy: 0.6840 - val_loss: 0.5765 - val_accuracy: 0.7222\n",
      "Epoch 298/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7282 - accuracy: 0.6725 - val_loss: 0.5707 - val_accuracy: 0.7037\n",
      "Epoch 299/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6675 - accuracy: 0.7014 - val_loss: 0.5679 - val_accuracy: 0.7269\n",
      "Epoch 300/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7295 - accuracy: 0.6644 - val_loss: 0.5746 - val_accuracy: 0.7037\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "(None, 1536)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "87/87 [==============================] - 2s 11ms/step - loss: 1.7069 - accuracy: 0.3831 - val_loss: 1.2699 - val_accuracy: 0.4861\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.2090 - accuracy: 0.5012 - val_loss: 2.5346 - val_accuracy: 0.3241\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.2288 - accuracy: 0.4931 - val_loss: 0.9671 - val_accuracy: 0.6111\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0431 - accuracy: 0.5729 - val_loss: 1.4744 - val_accuracy: 0.5370\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1136 - accuracy: 0.5590 - val_loss: 1.3819 - val_accuracy: 0.4676\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1099 - accuracy: 0.5000 - val_loss: 2.4247 - val_accuracy: 0.3935\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0670 - accuracy: 0.5417 - val_loss: 1.2617 - val_accuracy: 0.4583\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9868 - accuracy: 0.5764 - val_loss: 0.9234 - val_accuracy: 0.5787\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9444 - accuracy: 0.5822 - val_loss: 0.9702 - val_accuracy: 0.5972\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8931 - accuracy: 0.6238 - val_loss: 1.1638 - val_accuracy: 0.4491\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9373 - accuracy: 0.6053 - val_loss: 1.3323 - val_accuracy: 0.4213\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8682 - accuracy: 0.6528 - val_loss: 0.7758 - val_accuracy: 0.6852\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8811 - accuracy: 0.6204 - val_loss: 0.6816 - val_accuracy: 0.6944\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8149 - accuracy: 0.6516 - val_loss: 1.8955 - val_accuracy: 0.3287\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8367 - accuracy: 0.6655 - val_loss: 1.1093 - val_accuracy: 0.5509\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.8653 - accuracy: 0.6551 - val_loss: 0.7593 - val_accuracy: 0.6991\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.7870 - accuracy: 0.6736 - val_loss: 0.7593 - val_accuracy: 0.6944\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9856 - accuracy: 0.5752 - val_loss: 2.5119 - val_accuracy: 0.2824\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1622 - accuracy: 0.4282 - val_loss: 1.0407 - val_accuracy: 0.4074\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1112 - accuracy: 0.4225 - val_loss: 1.0148 - val_accuracy: 0.5278\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0986 - accuracy: 0.4572 - val_loss: 1.2153 - val_accuracy: 0.3935\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1384 - accuracy: 0.4572 - val_loss: 1.2886 - val_accuracy: 0.4491\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1156 - accuracy: 0.4144 - val_loss: 1.3911 - val_accuracy: 0.4583\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0940 - accuracy: 0.4618 - val_loss: 1.0460 - val_accuracy: 0.4676\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0743 - accuracy: 0.4780 - val_loss: 0.9814 - val_accuracy: 0.5139\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1078 - accuracy: 0.4780 - val_loss: 0.9657 - val_accuracy: 0.5833\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0760 - accuracy: 0.5231 - val_loss: 1.1279 - val_accuracy: 0.3889\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0774 - accuracy: 0.5116 - val_loss: 1.1511 - val_accuracy: 0.3935\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0624 - accuracy: 0.5069 - val_loss: 0.8961 - val_accuracy: 0.6296\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0935 - accuracy: 0.5069 - val_loss: 0.9421 - val_accuracy: 0.6111\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0335 - accuracy: 0.5220 - val_loss: 0.8991 - val_accuracy: 0.6296\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0487 - accuracy: 0.5220 - val_loss: 1.1246 - val_accuracy: 0.4352\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1020 - accuracy: 0.4815 - val_loss: 0.8995 - val_accuracy: 0.6389\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0734 - accuracy: 0.5081 - val_loss: 0.8739 - val_accuracy: 0.6528\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0643 - accuracy: 0.5208 - val_loss: 0.8893 - val_accuracy: 0.6435\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0381 - accuracy: 0.5475 - val_loss: 1.2260 - val_accuracy: 0.3519\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0601 - accuracy: 0.5278 - val_loss: 1.1610 - val_accuracy: 0.4398\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1001 - accuracy: 0.4444 - val_loss: 0.9691 - val_accuracy: 0.5694\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0271 - accuracy: 0.5475 - val_loss: 1.2592 - val_accuracy: 0.3796\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0396 - accuracy: 0.5671 - val_loss: 1.0862 - val_accuracy: 0.4491\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0107 - accuracy: 0.5428 - val_loss: 1.0990 - val_accuracy: 0.6111\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0984 - accuracy: 0.5324 - val_loss: 2.1026 - val_accuracy: 0.2407\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.0682 - accuracy: 0.5231 - val_loss: 0.8621 - val_accuracy: 0.7083\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0366 - accuracy: 0.5440 - val_loss: 0.8096 - val_accuracy: 0.7083\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9903 - accuracy: 0.5498 - val_loss: 0.9646 - val_accuracy: 0.5648\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0510 - accuracy: 0.5266 - val_loss: 0.8828 - val_accuracy: 0.6343\n",
      "Epoch 47/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0560 - accuracy: 0.5475 - val_loss: 1.5943 - val_accuracy: 0.2731\n",
      "Epoch 48/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1194 - accuracy: 0.5093 - val_loss: 1.0313 - val_accuracy: 0.5046\n",
      "Epoch 49/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0451 - accuracy: 0.5336 - val_loss: 1.1281 - val_accuracy: 0.4259\n",
      "Epoch 50/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0244 - accuracy: 0.5764 - val_loss: 0.8609 - val_accuracy: 0.6204\n",
      "Epoch 51/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0113 - accuracy: 0.5660 - val_loss: 0.8833 - val_accuracy: 0.6250\n",
      "Epoch 52/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9995 - accuracy: 0.5683 - val_loss: 0.8616 - val_accuracy: 0.6250\n",
      "Epoch 53/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0539 - accuracy: 0.5544 - val_loss: 1.7271 - val_accuracy: 0.2824\n",
      "Epoch 54/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0445 - accuracy: 0.5359 - val_loss: 1.0930 - val_accuracy: 0.5602\n",
      "Epoch 55/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0151 - accuracy: 0.5752 - val_loss: 0.8257 - val_accuracy: 0.6759\n",
      "Epoch 56/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0013 - accuracy: 0.5799 - val_loss: 2.0422 - val_accuracy: 0.2639\n",
      "Epoch 57/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0105 - accuracy: 0.5648 - val_loss: 0.8701 - val_accuracy: 0.5926\n",
      "Epoch 58/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0006 - accuracy: 0.5694 - val_loss: 0.8605 - val_accuracy: 0.6574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.0127 - accuracy: 0.5637 - val_loss: 0.7999 - val_accuracy: 0.7361\n",
      "Epoch 60/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9894 - accuracy: 0.5880 - val_loss: 0.8432 - val_accuracy: 0.7222\n",
      "Epoch 61/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9836 - accuracy: 0.5972 - val_loss: 0.9949 - val_accuracy: 0.5648\n",
      "Epoch 62/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9934 - accuracy: 0.5799 - val_loss: 1.3826 - val_accuracy: 0.3009\n",
      "Epoch 63/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9963 - accuracy: 0.5903 - val_loss: 0.8101 - val_accuracy: 0.6667\n",
      "Epoch 64/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9874 - accuracy: 0.5787 - val_loss: 0.8829 - val_accuracy: 0.6620\n",
      "Epoch 65/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9835 - accuracy: 0.5775 - val_loss: 0.8581 - val_accuracy: 0.5787\n",
      "Epoch 66/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0355 - accuracy: 0.5463 - val_loss: 0.9380 - val_accuracy: 0.6111\n",
      "Epoch 67/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9803 - accuracy: 0.5938 - val_loss: 0.7709 - val_accuracy: 0.7361\n",
      "Epoch 68/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9777 - accuracy: 0.5938 - val_loss: 0.7807 - val_accuracy: 0.7222\n",
      "Epoch 69/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0012 - accuracy: 0.5822 - val_loss: 1.0678 - val_accuracy: 0.6204\n",
      "Epoch 70/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0435 - accuracy: 0.5509 - val_loss: 1.4132 - val_accuracy: 0.2361\n",
      "Epoch 71/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9763 - accuracy: 0.6076 - val_loss: 0.9638 - val_accuracy: 0.5231\n",
      "Epoch 72/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1146 - accuracy: 0.4942 - val_loss: 1.1185 - val_accuracy: 0.4907\n",
      "Epoch 73/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0347 - accuracy: 0.5266 - val_loss: 0.8699 - val_accuracy: 0.6713\n",
      "Epoch 74/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9750 - accuracy: 0.5799 - val_loss: 0.8833 - val_accuracy: 0.6528\n",
      "Epoch 75/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0641 - accuracy: 0.5498 - val_loss: 0.9325 - val_accuracy: 0.6343\n",
      "Epoch 76/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0094 - accuracy: 0.5637 - val_loss: 1.1001 - val_accuracy: 0.5231\n",
      "Epoch 77/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9904 - accuracy: 0.5833 - val_loss: 0.8660 - val_accuracy: 0.6898\n",
      "Epoch 78/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9810 - accuracy: 0.5926 - val_loss: 0.7798 - val_accuracy: 0.7222\n",
      "Epoch 79/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0102 - accuracy: 0.5741 - val_loss: 0.8074 - val_accuracy: 0.6806\n",
      "Epoch 80/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0298 - accuracy: 0.5741 - val_loss: 0.7750 - val_accuracy: 0.7361\n",
      "Epoch 81/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9873 - accuracy: 0.6100 - val_loss: 0.8213 - val_accuracy: 0.6991\n",
      "Epoch 82/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9934 - accuracy: 0.5903 - val_loss: 0.9261 - val_accuracy: 0.5370\n",
      "Epoch 83/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9802 - accuracy: 0.6019 - val_loss: 0.7854 - val_accuracy: 0.7037\n",
      "Epoch 84/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9269 - accuracy: 0.6308 - val_loss: 0.8847 - val_accuracy: 0.6759\n",
      "Epoch 85/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9492 - accuracy: 0.6238 - val_loss: 0.7796 - val_accuracy: 0.7083\n",
      "Epoch 86/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9871 - accuracy: 0.5752 - val_loss: 1.1370 - val_accuracy: 0.4907\n",
      "Epoch 87/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9838 - accuracy: 0.6134 - val_loss: 0.9791 - val_accuracy: 0.5972\n",
      "Epoch 88/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9769 - accuracy: 0.6076 - val_loss: 0.7852 - val_accuracy: 0.6898\n",
      "Epoch 89/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0023 - accuracy: 0.5926 - val_loss: 0.8387 - val_accuracy: 0.6898\n",
      "Epoch 90/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.9513 - accuracy: 0.6181 - val_loss: 0.7250 - val_accuracy: 0.7407\n",
      "Epoch 91/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9929 - accuracy: 0.5938 - val_loss: 2.0966 - val_accuracy: 0.3426\n",
      "Epoch 92/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0093 - accuracy: 0.5775 - val_loss: 4.2022 - val_accuracy: 0.4352\n",
      "Epoch 93/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0156 - accuracy: 0.5775 - val_loss: 0.7844 - val_accuracy: 0.7130\n",
      "Epoch 94/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0022 - accuracy: 0.5880 - val_loss: 0.8113 - val_accuracy: 0.6898\n",
      "Epoch 95/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0631 - accuracy: 0.5532 - val_loss: 0.8766 - val_accuracy: 0.6898\n",
      "Epoch 96/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9809 - accuracy: 0.6007 - val_loss: 0.8033 - val_accuracy: 0.7037\n",
      "Epoch 97/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9904 - accuracy: 0.5845 - val_loss: 1.0801 - val_accuracy: 0.5463\n",
      "Epoch 98/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9623 - accuracy: 0.5961 - val_loss: 0.9558 - val_accuracy: 0.5741\n",
      "Epoch 99/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9511 - accuracy: 0.6042 - val_loss: 0.8158 - val_accuracy: 0.7037\n",
      "Epoch 100/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9619 - accuracy: 0.6065 - val_loss: 0.7646 - val_accuracy: 0.7315\n",
      "Epoch 101/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9807 - accuracy: 0.5833 - val_loss: 1.1253 - val_accuracy: 0.4676\n",
      "Epoch 102/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9620 - accuracy: 0.6204 - val_loss: 0.7590 - val_accuracy: 0.7361\n",
      "Epoch 103/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9825 - accuracy: 0.5995 - val_loss: 0.7514 - val_accuracy: 0.7361\n",
      "Epoch 104/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9749 - accuracy: 0.5914 - val_loss: 0.7707 - val_accuracy: 0.7269\n",
      "Epoch 105/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9726 - accuracy: 0.6019 - val_loss: 0.8733 - val_accuracy: 0.6528\n",
      "Epoch 106/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9702 - accuracy: 0.5833 - val_loss: 0.8525 - val_accuracy: 0.6852\n",
      "Epoch 107/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9173 - accuracy: 0.6354 - val_loss: 0.7749 - val_accuracy: 0.7083\n",
      "Epoch 108/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0971 - accuracy: 0.5521 - val_loss: 0.8150 - val_accuracy: 0.6991\n",
      "Epoch 109/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9594 - accuracy: 0.5891 - val_loss: 0.7737 - val_accuracy: 0.7315\n",
      "Epoch 110/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9851 - accuracy: 0.6007 - val_loss: 3.3132 - val_accuracy: 0.2593\n",
      "Epoch 111/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1770 - accuracy: 0.4479 - val_loss: 1.1461 - val_accuracy: 0.4769\n",
      "Epoch 112/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9761 - accuracy: 0.6065 - val_loss: 0.7941 - val_accuracy: 0.7037\n",
      "Epoch 113/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9278 - accuracy: 0.6319 - val_loss: 0.7993 - val_accuracy: 0.6481\n",
      "Epoch 114/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.9658 - accuracy: 0.6285 - val_loss: 0.7366 - val_accuracy: 0.7500\n",
      "Epoch 115/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9587 - accuracy: 0.6007 - val_loss: 1.0003 - val_accuracy: 0.5602\n",
      "Epoch 116/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9651 - accuracy: 0.6100 - val_loss: 0.7692 - val_accuracy: 0.7500\n",
      "Epoch 117/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9570 - accuracy: 0.5949 - val_loss: 0.9087 - val_accuracy: 0.6343\n",
      "Epoch 118/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9843 - accuracy: 0.5961 - val_loss: 0.7662 - val_accuracy: 0.7269\n",
      "Epoch 119/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9327 - accuracy: 0.6262 - val_loss: 0.7638 - val_accuracy: 0.6944\n",
      "Epoch 120/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9731 - accuracy: 0.6146 - val_loss: 0.7566 - val_accuracy: 0.7083\n",
      "Epoch 121/300\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.9472 - accuracy: 0.6215 - val_loss: 0.7031 - val_accuracy: 0.7546\n",
      "Epoch 122/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9496 - accuracy: 0.6204 - val_loss: 0.7271 - val_accuracy: 0.7130\n",
      "Epoch 123/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9802 - accuracy: 0.5856 - val_loss: 0.7168 - val_accuracy: 0.7593\n",
      "Epoch 124/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9773 - accuracy: 0.5868 - val_loss: 0.7510 - val_accuracy: 0.7315\n",
      "Epoch 125/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9468 - accuracy: 0.6019 - val_loss: 0.7479 - val_accuracy: 0.7176\n",
      "Epoch 126/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9848 - accuracy: 0.6030 - val_loss: 0.7826 - val_accuracy: 0.7315\n",
      "Epoch 127/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0261 - accuracy: 0.5729 - val_loss: 1.7926 - val_accuracy: 0.1898\n",
      "Epoch 128/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0411 - accuracy: 0.5637 - val_loss: 0.7924 - val_accuracy: 0.7315\n",
      "Epoch 129/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9316 - accuracy: 0.6319 - val_loss: 0.7648 - val_accuracy: 0.6991\n",
      "Epoch 130/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9820 - accuracy: 0.5984 - val_loss: 0.8054 - val_accuracy: 0.7037\n",
      "Epoch 131/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9603 - accuracy: 0.6250 - val_loss: 0.7611 - val_accuracy: 0.7269\n",
      "Epoch 132/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9329 - accuracy: 0.6273 - val_loss: 0.7958 - val_accuracy: 0.7083\n",
      "Epoch 133/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9633 - accuracy: 0.6123 - val_loss: 0.8627 - val_accuracy: 0.6620\n",
      "Epoch 134/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9555 - accuracy: 0.6111 - val_loss: 0.7393 - val_accuracy: 0.7361\n",
      "Epoch 135/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9546 - accuracy: 0.6238 - val_loss: 0.7853 - val_accuracy: 0.7083\n",
      "Epoch 136/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9635 - accuracy: 0.5972 - val_loss: 0.7295 - val_accuracy: 0.7407\n",
      "Epoch 137/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9278 - accuracy: 0.6343 - val_loss: 0.7939 - val_accuracy: 0.7315\n",
      "Epoch 138/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9565 - accuracy: 0.5995 - val_loss: 0.7460 - val_accuracy: 0.7500\n",
      "Epoch 139/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9492 - accuracy: 0.6285 - val_loss: 0.9116 - val_accuracy: 0.6111\n",
      "Epoch 140/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9378 - accuracy: 0.6250 - val_loss: 0.7800 - val_accuracy: 0.6991\n",
      "Epoch 141/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9689 - accuracy: 0.6111 - val_loss: 0.7368 - val_accuracy: 0.7500\n",
      "Epoch 142/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9448 - accuracy: 0.6204 - val_loss: 0.7362 - val_accuracy: 0.7407\n",
      "Epoch 143/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9675 - accuracy: 0.6007 - val_loss: 0.8476 - val_accuracy: 0.5972\n",
      "Epoch 144/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0066 - accuracy: 0.5694 - val_loss: 0.7889 - val_accuracy: 0.6806\n",
      "Epoch 145/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9884 - accuracy: 0.6157 - val_loss: 0.7593 - val_accuracy: 0.7454\n",
      "Epoch 146/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9251 - accuracy: 0.6181 - val_loss: 1.1261 - val_accuracy: 0.4861\n",
      "Epoch 147/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9834 - accuracy: 0.6053 - val_loss: 0.8371 - val_accuracy: 0.6759\n",
      "Epoch 148/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9405 - accuracy: 0.6157 - val_loss: 1.2125 - val_accuracy: 0.3981\n",
      "Epoch 149/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9453 - accuracy: 0.6215 - val_loss: 0.7831 - val_accuracy: 0.6991\n",
      "Epoch 150/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9643 - accuracy: 0.6134 - val_loss: 0.7282 - val_accuracy: 0.7500\n",
      "Epoch 151/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9349 - accuracy: 0.6042 - val_loss: 0.9277 - val_accuracy: 0.6343\n",
      "Epoch 152/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0460 - accuracy: 0.5660 - val_loss: 1.0601 - val_accuracy: 0.5324\n",
      "Epoch 153/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0029 - accuracy: 0.5613 - val_loss: 0.7581 - val_accuracy: 0.7454\n",
      "Epoch 154/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9409 - accuracy: 0.6204 - val_loss: 0.7269 - val_accuracy: 0.7269\n",
      "Epoch 155/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9340 - accuracy: 0.6354 - val_loss: 0.7355 - val_accuracy: 0.7269\n",
      "Epoch 156/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9046 - accuracy: 0.6366 - val_loss: 0.7555 - val_accuracy: 0.6991\n",
      "Epoch 157/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9568 - accuracy: 0.6227 - val_loss: 0.7872 - val_accuracy: 0.7269\n",
      "Epoch 158/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9341 - accuracy: 0.6123 - val_loss: 0.7342 - val_accuracy: 0.7500\n",
      "Epoch 159/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9589 - accuracy: 0.5903 - val_loss: 1.0325 - val_accuracy: 0.5694\n",
      "Epoch 160/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9899 - accuracy: 0.5706 - val_loss: 0.8858 - val_accuracy: 0.5880\n",
      "Epoch 161/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9674 - accuracy: 0.6100 - val_loss: 0.9533 - val_accuracy: 0.5694\n",
      "Epoch 162/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9368 - accuracy: 0.6308 - val_loss: 0.7397 - val_accuracy: 0.7315\n",
      "Epoch 163/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9147 - accuracy: 0.6331 - val_loss: 0.7731 - val_accuracy: 0.7315\n",
      "Epoch 164/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9297 - accuracy: 0.6181 - val_loss: 0.7242 - val_accuracy: 0.7269\n",
      "Epoch 165/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9919 - accuracy: 0.5984 - val_loss: 0.8744 - val_accuracy: 0.5926\n",
      "Epoch 166/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0116 - accuracy: 0.5718 - val_loss: 0.7728 - val_accuracy: 0.6991\n",
      "Epoch 167/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9726 - accuracy: 0.6030 - val_loss: 0.7335 - val_accuracy: 0.7407\n",
      "Epoch 168/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9753 - accuracy: 0.6076 - val_loss: 0.8541 - val_accuracy: 0.6944\n",
      "Epoch 169/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9111 - accuracy: 0.6296 - val_loss: 0.7292 - val_accuracy: 0.7222\n",
      "Epoch 170/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9250 - accuracy: 0.6562 - val_loss: 0.7363 - val_accuracy: 0.7361\n",
      "Epoch 171/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9466 - accuracy: 0.6331 - val_loss: 0.7376 - val_accuracy: 0.7407\n",
      "Epoch 172/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9380 - accuracy: 0.6146 - val_loss: 0.7686 - val_accuracy: 0.7083\n",
      "Epoch 173/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9568 - accuracy: 0.5984 - val_loss: 0.7442 - val_accuracy: 0.7454\n",
      "Epoch 174/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9329 - accuracy: 0.6181 - val_loss: 0.7356 - val_accuracy: 0.7315\n",
      "Epoch 175/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9197 - accuracy: 0.6308 - val_loss: 0.9444 - val_accuracy: 0.6111\n",
      "Epoch 176/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9055 - accuracy: 0.6435 - val_loss: 0.9212 - val_accuracy: 0.6204\n",
      "Epoch 177/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9119 - accuracy: 0.6343 - val_loss: 0.7113 - val_accuracy: 0.7454\n",
      "Epoch 178/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9351 - accuracy: 0.6319 - val_loss: 0.7994 - val_accuracy: 0.6481\n",
      "Epoch 179/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9606 - accuracy: 0.6088 - val_loss: 0.7261 - val_accuracy: 0.7269\n",
      "Epoch 180/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9273 - accuracy: 0.6377 - val_loss: 0.7360 - val_accuracy: 0.7222\n",
      "Epoch 181/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9196 - accuracy: 0.6273 - val_loss: 0.7425 - val_accuracy: 0.7407\n",
      "Epoch 182/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9273 - accuracy: 0.6574 - val_loss: 0.7337 - val_accuracy: 0.7315\n",
      "Epoch 183/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9136 - accuracy: 0.6308 - val_loss: 0.7080 - val_accuracy: 0.7269\n",
      "Epoch 184/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9760 - accuracy: 0.6019 - val_loss: 0.8113 - val_accuracy: 0.6806\n",
      "Epoch 185/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9734 - accuracy: 0.5984 - val_loss: 0.8569 - val_accuracy: 0.6343\n",
      "Epoch 186/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9082 - accuracy: 0.6400 - val_loss: 0.7317 - val_accuracy: 0.7130\n",
      "Epoch 187/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9440 - accuracy: 0.5903 - val_loss: 0.7513 - val_accuracy: 0.7222\n",
      "Epoch 188/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9173 - accuracy: 0.6389 - val_loss: 0.7203 - val_accuracy: 0.7361\n",
      "Epoch 189/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9624 - accuracy: 0.6146 - val_loss: 0.7488 - val_accuracy: 0.7269\n",
      "Epoch 190/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9151 - accuracy: 0.6308 - val_loss: 0.7240 - val_accuracy: 0.7407\n",
      "Epoch 191/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9082 - accuracy: 0.6308 - val_loss: 0.7176 - val_accuracy: 0.7361\n",
      "Epoch 192/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9292 - accuracy: 0.6134 - val_loss: 0.7264 - val_accuracy: 0.7361\n",
      "Epoch 193/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9364 - accuracy: 0.6250 - val_loss: 0.7122 - val_accuracy: 0.7546\n",
      "Epoch 194/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9911 - accuracy: 0.6053 - val_loss: 0.8303 - val_accuracy: 0.7130\n",
      "Epoch 195/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9250 - accuracy: 0.6227 - val_loss: 0.7305 - val_accuracy: 0.7454\n",
      "Epoch 196/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9225 - accuracy: 0.6343 - val_loss: 0.8581 - val_accuracy: 0.6250\n",
      "Epoch 197/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9467 - accuracy: 0.6250 - val_loss: 0.7150 - val_accuracy: 0.7500\n",
      "Epoch 198/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9483 - accuracy: 0.6134 - val_loss: 0.8559 - val_accuracy: 0.7037\n",
      "Epoch 199/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9321 - accuracy: 0.6447 - val_loss: 0.7808 - val_accuracy: 0.7269\n",
      "Epoch 200/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8977 - accuracy: 0.6331 - val_loss: 0.7656 - val_accuracy: 0.7176\n",
      "Epoch 201/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1611 - accuracy: 0.4965 - val_loss: 0.8548 - val_accuracy: 0.7083\n",
      "Epoch 202/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0247 - accuracy: 0.5880 - val_loss: 0.8274 - val_accuracy: 0.7269\n",
      "Epoch 203/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9735 - accuracy: 0.6111 - val_loss: 0.7883 - val_accuracy: 0.7407\n",
      "Epoch 204/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9344 - accuracy: 0.6389 - val_loss: 0.7731 - val_accuracy: 0.7407\n",
      "Epoch 205/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9218 - accuracy: 0.6354 - val_loss: 0.7819 - val_accuracy: 0.7222\n",
      "Epoch 206/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9124 - accuracy: 0.6435 - val_loss: 0.7419 - val_accuracy: 0.7454\n",
      "Epoch 207/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9223 - accuracy: 0.6458 - val_loss: 0.8564 - val_accuracy: 0.7037\n",
      "Epoch 208/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9209 - accuracy: 0.6343 - val_loss: 0.7374 - val_accuracy: 0.7454\n",
      "Epoch 209/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9462 - accuracy: 0.6285 - val_loss: 0.7255 - val_accuracy: 0.7500\n",
      "Epoch 210/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9049 - accuracy: 0.6505 - val_loss: 0.7082 - val_accuracy: 0.7500\n",
      "Epoch 211/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8983 - accuracy: 0.6400 - val_loss: 0.7035 - val_accuracy: 0.7546\n",
      "Epoch 212/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9349 - accuracy: 0.6250 - val_loss: 0.7390 - val_accuracy: 0.7315\n",
      "Epoch 213/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9241 - accuracy: 0.6285 - val_loss: 0.7105 - val_accuracy: 0.7546\n",
      "Epoch 214/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8859 - accuracy: 0.6481 - val_loss: 0.7087 - val_accuracy: 0.7407\n",
      "Epoch 215/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9201 - accuracy: 0.6308 - val_loss: 0.7189 - val_accuracy: 0.7500\n",
      "Epoch 216/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9354 - accuracy: 0.6285 - val_loss: 0.6953 - val_accuracy: 0.7500\n",
      "Epoch 217/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9211 - accuracy: 0.6377 - val_loss: 0.7306 - val_accuracy: 0.7407\n",
      "Epoch 218/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8791 - accuracy: 0.6505 - val_loss: 0.7085 - val_accuracy: 0.7454\n",
      "Epoch 219/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8999 - accuracy: 0.6458 - val_loss: 0.7034 - val_accuracy: 0.7454\n",
      "Epoch 220/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9067 - accuracy: 0.6458 - val_loss: 0.6988 - val_accuracy: 0.7315\n",
      "Epoch 221/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8942 - accuracy: 0.6481 - val_loss: 0.7013 - val_accuracy: 0.7407\n",
      "Epoch 222/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9108 - accuracy: 0.6458 - val_loss: 0.7187 - val_accuracy: 0.7269\n",
      "Epoch 223/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8520 - accuracy: 0.6655 - val_loss: 0.7028 - val_accuracy: 0.7407\n",
      "Epoch 224/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9046 - accuracy: 0.6447 - val_loss: 0.8566 - val_accuracy: 0.6759\n",
      "Epoch 225/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8793 - accuracy: 0.6539 - val_loss: 0.7209 - val_accuracy: 0.7500\n",
      "Epoch 226/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9278 - accuracy: 0.6192 - val_loss: 0.7542 - val_accuracy: 0.6991\n",
      "Epoch 227/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9083 - accuracy: 0.6470 - val_loss: 0.7133 - val_accuracy: 0.7407\n",
      "Epoch 228/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8960 - accuracy: 0.6377 - val_loss: 0.7029 - val_accuracy: 0.7361\n",
      "Epoch 229/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9139 - accuracy: 0.6331 - val_loss: 0.7129 - val_accuracy: 0.7269\n",
      "Epoch 230/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9046 - accuracy: 0.6424 - val_loss: 0.7111 - val_accuracy: 0.7361\n",
      "Epoch 231/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9174 - accuracy: 0.6296 - val_loss: 0.7291 - val_accuracy: 0.7407\n",
      "Epoch 232/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9263 - accuracy: 0.6447 - val_loss: 0.7876 - val_accuracy: 0.7037\n",
      "Epoch 233/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9332 - accuracy: 0.6366 - val_loss: 0.7131 - val_accuracy: 0.7361\n",
      "Epoch 234/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9239 - accuracy: 0.6273 - val_loss: 0.7130 - val_accuracy: 0.7407\n",
      "Epoch 235/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9015 - accuracy: 0.6458 - val_loss: 0.7356 - val_accuracy: 0.7361\n",
      "Epoch 236/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9049 - accuracy: 0.6412 - val_loss: 0.7239 - val_accuracy: 0.7500\n",
      "Epoch 237/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8936 - accuracy: 0.6343 - val_loss: 0.7018 - val_accuracy: 0.7407\n",
      "Epoch 238/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8958 - accuracy: 0.6308 - val_loss: 0.7048 - val_accuracy: 0.7407\n",
      "Epoch 239/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9194 - accuracy: 0.6609 - val_loss: 0.7057 - val_accuracy: 0.7454\n",
      "Epoch 240/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9027 - accuracy: 0.6400 - val_loss: 0.7136 - val_accuracy: 0.7176\n",
      "Epoch 241/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9001 - accuracy: 0.6435 - val_loss: 0.7142 - val_accuracy: 0.7361\n",
      "Epoch 242/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8916 - accuracy: 0.6447 - val_loss: 0.7097 - val_accuracy: 0.7361\n",
      "Epoch 243/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9279 - accuracy: 0.6447 - val_loss: 0.7113 - val_accuracy: 0.7407\n",
      "Epoch 244/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8910 - accuracy: 0.6505 - val_loss: 0.7064 - val_accuracy: 0.7176\n",
      "Epoch 245/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9459 - accuracy: 0.6238 - val_loss: 0.7048 - val_accuracy: 0.7500\n",
      "Epoch 246/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9396 - accuracy: 0.6215 - val_loss: 0.7544 - val_accuracy: 0.7407\n",
      "Epoch 247/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8897 - accuracy: 0.6481 - val_loss: 0.7095 - val_accuracy: 0.7500\n",
      "Epoch 248/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9008 - accuracy: 0.6481 - val_loss: 0.7042 - val_accuracy: 0.7454\n",
      "Epoch 249/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8984 - accuracy: 0.6308 - val_loss: 0.7290 - val_accuracy: 0.7407\n",
      "Epoch 250/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8931 - accuracy: 0.6609 - val_loss: 0.7093 - val_accuracy: 0.7269\n",
      "Epoch 251/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8925 - accuracy: 0.6620 - val_loss: 0.7085 - val_accuracy: 0.7407\n",
      "Epoch 252/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8981 - accuracy: 0.6458 - val_loss: 0.7083 - val_accuracy: 0.7361\n",
      "Epoch 253/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8846 - accuracy: 0.6574 - val_loss: 0.6959 - val_accuracy: 0.7407\n",
      "Epoch 254/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9072 - accuracy: 0.6505 - val_loss: 0.6979 - val_accuracy: 0.7454\n",
      "Epoch 255/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9231 - accuracy: 0.6250 - val_loss: 0.7004 - val_accuracy: 0.7361\n",
      "Epoch 256/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8871 - accuracy: 0.6667 - val_loss: 0.7053 - val_accuracy: 0.7454\n",
      "Epoch 257/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8738 - accuracy: 0.6562 - val_loss: 0.7219 - val_accuracy: 0.7361\n",
      "Epoch 258/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8718 - accuracy: 0.6678 - val_loss: 0.6991 - val_accuracy: 0.7315\n",
      "Epoch 259/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9190 - accuracy: 0.6354 - val_loss: 0.6995 - val_accuracy: 0.7454\n",
      "Epoch 260/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9064 - accuracy: 0.6296 - val_loss: 0.7012 - val_accuracy: 0.7315\n",
      "Epoch 261/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8892 - accuracy: 0.6447 - val_loss: 0.6981 - val_accuracy: 0.7407\n",
      "Epoch 262/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9142 - accuracy: 0.6296 - val_loss: 0.7070 - val_accuracy: 0.7454\n",
      "Epoch 263/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9084 - accuracy: 0.6366 - val_loss: 0.7111 - val_accuracy: 0.7361\n",
      "Epoch 264/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8663 - accuracy: 0.6690 - val_loss: 0.7054 - val_accuracy: 0.7315\n",
      "Epoch 265/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8603 - accuracy: 0.6678 - val_loss: 0.7130 - val_accuracy: 0.7315\n",
      "Epoch 266/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8913 - accuracy: 0.6458 - val_loss: 0.7038 - val_accuracy: 0.7361\n",
      "Epoch 267/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8928 - accuracy: 0.6528 - val_loss: 0.7016 - val_accuracy: 0.7361\n",
      "Epoch 268/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9167 - accuracy: 0.6319 - val_loss: 0.7075 - val_accuracy: 0.7361\n",
      "Epoch 269/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9120 - accuracy: 0.6343 - val_loss: 0.7043 - val_accuracy: 0.7407\n",
      "Epoch 270/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8772 - accuracy: 0.6400 - val_loss: 0.7071 - val_accuracy: 0.7361\n",
      "Epoch 271/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8680 - accuracy: 0.6435 - val_loss: 0.7031 - val_accuracy: 0.7454\n",
      "Epoch 272/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8626 - accuracy: 0.6586 - val_loss: 0.7026 - val_accuracy: 0.7407\n",
      "Epoch 273/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9031 - accuracy: 0.6574 - val_loss: 0.7024 - val_accuracy: 0.7407\n",
      "Epoch 274/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9054 - accuracy: 0.6400 - val_loss: 0.7070 - val_accuracy: 0.7315\n",
      "Epoch 275/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9059 - accuracy: 0.6319 - val_loss: 0.7031 - val_accuracy: 0.7315\n",
      "Epoch 276/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9072 - accuracy: 0.6516 - val_loss: 0.7258 - val_accuracy: 0.7361\n",
      "Epoch 277/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8979 - accuracy: 0.6447 - val_loss: 0.7210 - val_accuracy: 0.7361\n",
      "Epoch 278/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8952 - accuracy: 0.6493 - val_loss: 0.7062 - val_accuracy: 0.7315\n",
      "Epoch 279/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8649 - accuracy: 0.6609 - val_loss: 0.7048 - val_accuracy: 0.7361\n",
      "Epoch 280/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8875 - accuracy: 0.6609 - val_loss: 0.7017 - val_accuracy: 0.7361\n",
      "Epoch 281/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8769 - accuracy: 0.6435 - val_loss: 0.7021 - val_accuracy: 0.7315\n",
      "Epoch 282/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8311 - accuracy: 0.6725 - val_loss: 0.7106 - val_accuracy: 0.7361\n",
      "Epoch 283/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9116 - accuracy: 0.6412 - val_loss: 0.7013 - val_accuracy: 0.7315\n",
      "Epoch 284/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8879 - accuracy: 0.6481 - val_loss: 0.7069 - val_accuracy: 0.7361\n",
      "Epoch 285/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9157 - accuracy: 0.6389 - val_loss: 0.7100 - val_accuracy: 0.7361\n",
      "Epoch 286/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9136 - accuracy: 0.6296 - val_loss: 0.7038 - val_accuracy: 0.7407\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9067 - accuracy: 0.6343 - val_loss: 0.7048 - val_accuracy: 0.7269\n",
      "Epoch 288/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8849 - accuracy: 0.6516 - val_loss: 0.7059 - val_accuracy: 0.7269\n",
      "Epoch 289/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8991 - accuracy: 0.6516 - val_loss: 0.7085 - val_accuracy: 0.7315\n",
      "Epoch 290/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8725 - accuracy: 0.6516 - val_loss: 0.7161 - val_accuracy: 0.7361\n",
      "Epoch 291/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9035 - accuracy: 0.6493 - val_loss: 0.7021 - val_accuracy: 0.7315\n",
      "Epoch 292/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8905 - accuracy: 0.6562 - val_loss: 0.7041 - val_accuracy: 0.7361\n",
      "Epoch 293/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8596 - accuracy: 0.6644 - val_loss: 0.7040 - val_accuracy: 0.7315\n",
      "Epoch 294/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.9058 - accuracy: 0.6285 - val_loss: 0.7062 - val_accuracy: 0.7361\n",
      "Epoch 295/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8806 - accuracy: 0.6435 - val_loss: 0.7142 - val_accuracy: 0.7361\n",
      "Epoch 296/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8850 - accuracy: 0.6481 - val_loss: 0.7072 - val_accuracy: 0.7361\n",
      "Epoch 297/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8773 - accuracy: 0.6458 - val_loss: 0.7067 - val_accuracy: 0.7315\n",
      "Epoch 298/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8874 - accuracy: 0.6470 - val_loss: 0.7209 - val_accuracy: 0.7315\n",
      "Epoch 299/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8907 - accuracy: 0.6539 - val_loss: 0.7007 - val_accuracy: 0.7454\n",
      "Epoch 300/300\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.8669 - accuracy: 0.6505 - val_loss: 0.7044 - val_accuracy: 0.7361\n",
      "7/7 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 10\n",
    "\n",
    "average_results_acc = []\n",
    "conf_mat = []\n",
    "\n",
    "\n",
    "for train_index, test_index in StratifiedKFold(5, shuffle=True, random_state=42).split(rtemporal_x, rtemporal_y):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    x_train1, x_test1 = central_x[train_index], central_x[test_index]\n",
    "    y_train1, y_test1 = central_y[train_index], central_y[test_index]\n",
    "    \n",
    "    x_train2, x_test2 = rtemporal_x[train_index], rtemporal_x[test_index]\n",
    "    y_train2, y_test2 = rtemporal_y[train_index], rtemporal_y[test_index]\n",
    "    \n",
    "    x_train3, x_test3 = parietal_x[train_index], parietal_x[test_index]\n",
    "    y_train3, y_test3 = parietal_y[train_index], parietal_y[test_index]\n",
    "    \n",
    "    x_train4, x_test4 = occipital_x[train_index], occipital_x[test_index]\n",
    "    y_train4, y_test4 = occipital_y[train_index], occipital_y[test_index]\n",
    "\n",
    "    train_labels = to_categorical(y_train1, 4)\n",
    "    test_labels = to_categorical(y_test1, 4)\n",
    "    \n",
    "    num_train_steps = (train_labels.shape[0] // batch_size) * epochs\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.005, \n",
    "                                                            decay_steps=num_train_steps)\n",
    "    \n",
    "    model = HierarchicalTransformer()\n",
    "    mc = ModelCheckpoint('best.h5', save_best_only=True, monitor='val_accuracy')\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule), #learning_rate=1e-3, weight_decay=0.005),  # lr_schedule\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=[x_train1, x_train2, x_train3, x_train4],\n",
    "        y=train_labels,\n",
    "        validation_data=([x_test1, x_test2, x_test3, x_test4], test_labels),\n",
    "#         class_weight=class_weights,\n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        callbacks=[mc]  #, callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    model.load_weights('best.h5')\n",
    "    \n",
    "    prediction = model.predict([x_test1, x_test2, x_test3, x_test4])\n",
    "\n",
    "    prediction_bool = np.argmax(prediction, axis=1)\n",
    "    true_bool = np.argmax(test_labels, axis=1)\n",
    "    \n",
    "    cm=metrics.confusion_matrix(true_bool, prediction_bool)\n",
    "    conf_mat.append(cm)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    accuracy = metrics.accuracy_score(true_bool, prediction_bool)\n",
    "    average_results_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7592592592592593, 0.7268518518518519, 0.7824074074074074, 0.7685185185185185, 0.7592592592592593]\n",
      "0.7592592592592593\n",
      "0.01828557190567732\n"
     ]
    }
   ],
   "source": [
    "print(average_results_acc)\n",
    "print(np.mean(np.array(average_results_acc)))\n",
    "print(np.std(np.array(average_results_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(np.mean(conf_mat, axis=0), ['нейтральная', 'грусть', 'страх', 'радость'], \n",
    "                     ['нейтральная', 'грусть', 'страх', 'радость'])\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "ax = sns.heatmap(df_cm, cmap=\"YlGnBu\", annot=True, annot_kws={\"size\": 16}) # font size\n",
    "ax.set(xlabel='Предсказанный класс', ylabel='Истинный класс')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 2/216 [00:00<00:12, 16.74it/s]\u001b[A\n",
      "  2%|▏         | 4/216 [00:00<00:12, 16.53it/s]\u001b[A\n",
      "  3%|▎         | 6/216 [00:00<00:13, 15.66it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:12, 17.14it/s]\u001b[A\n",
      "  6%|▌         | 12/216 [00:00<00:10, 19.24it/s]\u001b[A\n",
      "  6%|▋         | 14/216 [00:00<00:10, 19.10it/s]\u001b[A\n",
      "  8%|▊         | 17/216 [00:00<00:09, 20.64it/s]\u001b[A\n",
      "  9%|▉         | 20/216 [00:01<00:10, 18.53it/s]\u001b[A\n",
      " 10%|█         | 22/216 [00:01<00:10, 18.33it/s]\u001b[A\n",
      " 11%|█         | 24/216 [00:01<00:10, 18.40it/s]\u001b[A\n",
      " 12%|█▏        | 26/216 [00:01<00:10, 18.48it/s]\u001b[A\n",
      " 13%|█▎        | 28/216 [00:01<00:10, 17.93it/s]\u001b[A\n",
      " 14%|█▍        | 30/216 [00:01<00:11, 16.34it/s]\u001b[A\n",
      " 15%|█▌        | 33/216 [00:01<00:10, 17.24it/s]\u001b[A\n",
      " 16%|█▌        | 35/216 [00:01<00:11, 15.92it/s]\u001b[A\n",
      " 18%|█▊        | 38/216 [00:02<00:10, 17.78it/s]\u001b[A\n",
      " 19%|█▊        | 40/216 [00:02<00:09, 17.65it/s]\u001b[A\n",
      " 20%|█▉        | 43/216 [00:02<00:08, 19.66it/s]\u001b[A\n",
      " 21%|██▏       | 46/216 [00:02<00:08, 20.80it/s]\u001b[A\n",
      " 23%|██▎       | 49/216 [00:02<00:07, 21.83it/s]\u001b[A\n",
      " 24%|██▍       | 52/216 [00:02<00:07, 20.73it/s]\u001b[A\n",
      " 25%|██▌       | 55/216 [00:02<00:07, 20.26it/s]\u001b[A\n",
      " 27%|██▋       | 58/216 [00:03<00:07, 19.96it/s]\u001b[A\n",
      " 28%|██▊       | 61/216 [00:03<00:07, 20.83it/s]\u001b[A\n",
      " 30%|██▉       | 64/216 [00:03<00:07, 20.20it/s]\u001b[A\n",
      " 31%|███       | 67/216 [00:03<00:07, 20.38it/s]\u001b[A\n",
      " 32%|███▏      | 70/216 [00:03<00:07, 19.71it/s]\u001b[A\n",
      " 34%|███▍      | 73/216 [00:03<00:06, 20.87it/s]\u001b[A\n",
      " 35%|███▌      | 76/216 [00:03<00:06, 21.59it/s]\u001b[A\n",
      " 37%|███▋      | 79/216 [00:04<00:06, 20.47it/s]\u001b[A\n",
      " 38%|███▊      | 82/216 [00:04<00:06, 21.44it/s]\u001b[A\n",
      " 39%|███▉      | 85/216 [00:04<00:06, 21.19it/s]\u001b[A\n",
      " 41%|████      | 88/216 [00:04<00:06, 20.47it/s]\u001b[A\n",
      " 42%|████▏     | 91/216 [00:04<00:05, 21.43it/s]\u001b[A\n",
      " 44%|████▎     | 94/216 [00:04<00:05, 20.56it/s]\u001b[A\n",
      " 45%|████▍     | 97/216 [00:04<00:05, 20.22it/s]\u001b[A\n",
      " 46%|████▋     | 100/216 [00:05<00:06, 16.99it/s]\u001b[A\n",
      " 47%|████▋     | 102/216 [00:05<00:07, 16.06it/s]\u001b[A\n",
      " 49%|████▊     | 105/216 [00:05<00:06, 17.92it/s]\u001b[A\n",
      " 50%|████▉     | 107/216 [00:05<00:06, 17.47it/s]\u001b[A\n",
      " 50%|█████     | 109/216 [00:05<00:06, 17.57it/s]\u001b[A\n",
      " 51%|█████▏    | 111/216 [00:05<00:06, 17.48it/s]\u001b[A\n",
      " 53%|█████▎    | 114/216 [00:05<00:05, 19.41it/s]\u001b[A\n",
      " 54%|█████▎    | 116/216 [00:06<00:05, 19.40it/s]\u001b[A\n",
      " 55%|█████▌    | 119/216 [00:06<00:04, 20.80it/s]\u001b[A\n",
      " 56%|█████▋    | 122/216 [00:06<00:04, 19.95it/s]\u001b[A\n",
      " 58%|█████▊    | 125/216 [00:06<00:04, 18.55it/s]\u001b[A\n",
      " 59%|█████▉    | 127/216 [00:06<00:04, 18.15it/s]\u001b[A\n",
      " 60%|██████    | 130/216 [00:06<00:04, 19.15it/s]\u001b[A\n",
      " 61%|██████    | 132/216 [00:06<00:04, 17.63it/s]\u001b[A\n",
      " 62%|██████▏   | 134/216 [00:07<00:05, 16.26it/s]\u001b[A\n",
      " 63%|██████▎   | 136/216 [00:07<00:04, 16.47it/s]\u001b[A\n",
      " 64%|██████▍   | 138/216 [00:07<00:05, 15.51it/s]\u001b[A\n",
      " 65%|██████▍   | 140/216 [00:07<00:04, 15.80it/s]\u001b[A\n",
      " 66%|██████▌   | 143/216 [00:07<00:04, 18.16it/s]\u001b[A\n",
      " 68%|██████▊   | 146/216 [00:07<00:03, 19.95it/s]\u001b[A\n",
      " 69%|██████▉   | 149/216 [00:07<00:03, 20.97it/s]\u001b[A\n",
      " 70%|███████   | 152/216 [00:08<00:03, 20.24it/s]\u001b[A\n",
      " 72%|███████▏  | 155/216 [00:08<00:02, 21.34it/s]\u001b[A\n",
      " 73%|███████▎  | 158/216 [00:08<00:02, 19.68it/s]\u001b[A\n",
      " 75%|███████▍  | 161/216 [00:08<00:02, 20.81it/s]\u001b[A\n",
      " 76%|███████▌  | 164/216 [00:08<00:02, 21.56it/s]\u001b[A\n",
      " 77%|███████▋  | 167/216 [00:08<00:02, 20.72it/s]\u001b[A\n",
      " 79%|███████▊  | 170/216 [00:08<00:02, 19.82it/s]\u001b[A\n",
      " 80%|████████  | 173/216 [00:09<00:02, 19.76it/s]\u001b[A\n",
      " 81%|████████  | 175/216 [00:09<00:02, 19.34it/s]\u001b[A\n",
      " 82%|████████▏ | 178/216 [00:09<00:01, 19.36it/s]\u001b[A\n",
      " 83%|████████▎ | 180/216 [00:09<00:01, 18.48it/s]\u001b[A\n",
      " 85%|████████▍ | 183/216 [00:09<00:01, 18.64it/s]\u001b[A\n",
      " 86%|████████▌ | 186/216 [00:09<00:01, 20.13it/s]\u001b[A\n",
      " 88%|████████▊ | 189/216 [00:09<00:01, 19.16it/s]\u001b[A\n",
      " 89%|████████▉ | 192/216 [00:10<00:01, 20.44it/s]\u001b[A\n",
      " 90%|█████████ | 195/216 [00:10<00:00, 21.49it/s]\u001b[A\n",
      " 92%|█████████▏| 198/216 [00:10<00:00, 20.90it/s]\u001b[A\n",
      " 93%|█████████▎| 201/216 [00:10<00:00, 21.94it/s]\u001b[A\n",
      " 94%|█████████▍| 204/216 [00:10<00:00, 22.36it/s]\u001b[A\n",
      " 96%|█████████▌| 207/216 [00:10<00:00, 20.41it/s]\u001b[A\n",
      " 97%|█████████▋| 210/216 [00:10<00:00, 18.97it/s]\u001b[A\n",
      " 99%|█████████▊| 213/216 [00:11<00:00, 20.11it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:11<00:00, 19.25it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:11<01:41, 11.23s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 3/216 [00:00<00:08, 24.41it/s]\u001b[A\n",
      "  3%|▎         | 6/216 [00:00<00:11, 18.28it/s]\u001b[A\n",
      "  4%|▎         | 8/216 [00:00<00:11, 17.57it/s]\u001b[A\n",
      "  5%|▌         | 11/216 [00:00<00:10, 19.87it/s]\u001b[A\n",
      "  6%|▋         | 14/216 [00:00<00:11, 17.90it/s]\u001b[A\n",
      "  7%|▋         | 16/216 [00:00<00:11, 17.09it/s]\u001b[A\n",
      "  9%|▉         | 19/216 [00:01<00:10, 19.20it/s]\u001b[A\n",
      " 10%|▉         | 21/216 [00:01<00:10, 19.22it/s]\u001b[A\n",
      " 11%|█         | 23/216 [00:01<00:11, 17.13it/s]\u001b[A\n",
      " 12%|█▏        | 25/216 [00:01<00:11, 16.97it/s]\u001b[A\n",
      " 12%|█▎        | 27/216 [00:01<00:11, 16.96it/s]\u001b[A\n",
      " 14%|█▍        | 30/216 [00:01<00:09, 18.83it/s]\u001b[A\n",
      " 15%|█▍        | 32/216 [00:01<00:10, 17.39it/s]\u001b[A\n",
      " 16%|█▌        | 35/216 [00:01<00:10, 17.40it/s]\u001b[A\n",
      " 18%|█▊        | 38/216 [00:02<00:10, 17.66it/s]\u001b[A\n",
      " 19%|█▊        | 40/216 [00:02<00:09, 17.95it/s]\u001b[A\n",
      " 20%|█▉        | 43/216 [00:02<00:08, 19.67it/s]\u001b[A\n",
      " 21%|██        | 45/216 [00:02<00:09, 18.71it/s]\u001b[A\n",
      " 22%|██▏       | 47/216 [00:02<00:09, 18.18it/s]\u001b[A\n",
      " 23%|██▎       | 50/216 [00:02<00:08, 19.88it/s]\u001b[A\n",
      " 24%|██▍       | 52/216 [00:02<00:08, 19.68it/s]\u001b[A\n",
      " 25%|██▌       | 54/216 [00:02<00:09, 17.29it/s]\u001b[A\n",
      " 26%|██▌       | 56/216 [00:03<00:09, 17.25it/s]\u001b[A\n",
      " 27%|██▋       | 58/216 [00:03<00:09, 17.40it/s]\u001b[A\n",
      " 28%|██▊       | 60/216 [00:03<00:08, 17.86it/s]\u001b[A\n",
      " 29%|██▊       | 62/216 [00:03<00:08, 17.84it/s]\u001b[A\n",
      " 30%|███       | 65/216 [00:03<00:08, 18.40it/s]\u001b[A\n",
      " 31%|███       | 67/216 [00:03<00:08, 17.95it/s]\u001b[A\n",
      " 32%|███▏      | 69/216 [00:03<00:08, 17.72it/s]\u001b[A\n",
      " 33%|███▎      | 71/216 [00:03<00:08, 16.37it/s]\u001b[A\n",
      " 34%|███▍      | 73/216 [00:04<00:08, 16.68it/s]\u001b[A\n",
      " 35%|███▌      | 76/216 [00:04<00:07, 17.95it/s]\u001b[A\n",
      " 37%|███▋      | 79/216 [00:04<00:07, 17.60it/s]\u001b[A\n",
      " 38%|███▊      | 81/216 [00:04<00:07, 17.43it/s]\u001b[A\n",
      " 39%|███▉      | 84/216 [00:04<00:07, 18.39it/s]\u001b[A\n",
      " 40%|███▉      | 86/216 [00:04<00:07, 17.45it/s]\u001b[A\n",
      " 41%|████      | 88/216 [00:04<00:07, 16.69it/s]\u001b[A\n",
      " 42%|████▏     | 90/216 [00:05<00:07, 16.83it/s]\u001b[A\n",
      " 43%|████▎     | 93/216 [00:05<00:07, 17.24it/s]\u001b[A\n",
      " 44%|████▍     | 95/216 [00:05<00:07, 16.43it/s]\u001b[A\n",
      " 45%|████▌     | 98/216 [00:05<00:06, 17.64it/s]\u001b[A\n",
      " 47%|████▋     | 101/216 [00:05<00:05, 19.32it/s]\u001b[A\n",
      " 48%|████▊     | 103/216 [00:05<00:06, 17.94it/s]\u001b[A\n",
      " 49%|████▊     | 105/216 [00:05<00:06, 17.75it/s]\u001b[A\n",
      " 50%|█████     | 108/216 [00:05<00:05, 19.42it/s]\u001b[A\n",
      " 51%|█████▏    | 111/216 [00:06<00:05, 20.66it/s]\u001b[A\n",
      " 53%|█████▎    | 114/216 [00:06<00:05, 20.21it/s]\u001b[A\n",
      " 54%|█████▍    | 117/216 [00:06<00:04, 20.10it/s]\u001b[A\n",
      " 56%|█████▌    | 120/216 [00:06<00:04, 21.09it/s]\u001b[A\n",
      " 57%|█████▋    | 123/216 [00:06<00:04, 20.58it/s]\u001b[A\n",
      " 58%|█████▊    | 126/216 [00:06<00:04, 21.27it/s]\u001b[A\n",
      " 60%|█████▉    | 129/216 [00:07<00:04, 17.95it/s]\u001b[A\n",
      " 61%|██████    | 132/216 [00:07<00:04, 19.27it/s]\u001b[A\n",
      " 62%|██████▎   | 135/216 [00:07<00:04, 18.68it/s]\u001b[A\n",
      " 63%|██████▎   | 137/216 [00:07<00:04, 18.53it/s]\u001b[A\n",
      " 65%|██████▍   | 140/216 [00:07<00:03, 19.90it/s]\u001b[A\n",
      " 66%|██████▌   | 143/216 [00:07<00:03, 20.37it/s]\u001b[A\n",
      " 68%|██████▊   | 146/216 [00:07<00:03, 18.98it/s]\u001b[A\n",
      " 69%|██████▊   | 148/216 [00:08<00:03, 18.99it/s]\u001b[A\n",
      " 69%|██████▉   | 150/216 [00:08<00:03, 18.20it/s]\u001b[A\n",
      " 70%|███████   | 152/216 [00:08<00:03, 17.63it/s]\u001b[A\n",
      " 71%|███████▏  | 154/216 [00:08<00:03, 16.84it/s]\u001b[A\n",
      " 72%|███████▏  | 156/216 [00:08<00:03, 17.20it/s]\u001b[A\n",
      " 74%|███████▎  | 159/216 [00:08<00:02, 19.26it/s]\u001b[A\n",
      " 75%|███████▍  | 161/216 [00:08<00:02, 18.64it/s]\u001b[A\n",
      " 75%|███████▌  | 163/216 [00:08<00:02, 18.23it/s]\u001b[A\n",
      " 76%|███████▋  | 165/216 [00:09<00:02, 17.62it/s]\u001b[A\n",
      " 77%|███████▋  | 167/216 [00:09<00:03, 16.33it/s]\u001b[A\n",
      " 79%|███████▊  | 170/216 [00:09<00:02, 18.02it/s]\u001b[A\n",
      " 80%|███████▉  | 172/216 [00:09<00:02, 16.27it/s]\u001b[A\n",
      " 81%|████████  | 174/216 [00:09<00:02, 15.43it/s]\u001b[A\n",
      " 81%|████████▏ | 176/216 [00:09<00:02, 15.73it/s]\u001b[A\n",
      " 82%|████████▏ | 178/216 [00:09<00:02, 15.14it/s]\u001b[A\n",
      " 84%|████████▍ | 181/216 [00:10<00:02, 16.50it/s]\u001b[A\n",
      " 85%|████████▍ | 183/216 [00:10<00:02, 15.63it/s]\u001b[A\n",
      " 86%|████████▌ | 186/216 [00:10<00:01, 17.89it/s]\u001b[A\n",
      " 88%|████████▊ | 189/216 [00:10<00:01, 19.45it/s]\u001b[A\n",
      " 88%|████████▊ | 191/216 [00:10<00:01, 19.01it/s]\u001b[A\n",
      " 89%|████████▉ | 193/216 [00:10<00:01, 17.18it/s]\u001b[A\n",
      " 90%|█████████ | 195/216 [00:10<00:01, 17.26it/s]\u001b[A\n",
      " 91%|█████████ | 197/216 [00:10<00:01, 16.20it/s]\u001b[A\n",
      " 92%|█████████▏| 199/216 [00:11<00:01, 16.43it/s]\u001b[A\n",
      " 93%|█████████▎| 201/216 [00:11<00:00, 16.46it/s]\u001b[A\n",
      " 94%|█████████▍| 203/216 [00:11<00:00, 16.72it/s]\u001b[A\n",
      " 95%|█████████▌| 206/216 [00:11<00:00, 18.80it/s]\u001b[A\n",
      " 97%|█████████▋| 209/216 [00:11<00:00, 20.43it/s]\u001b[A\n",
      " 98%|█████████▊| 212/216 [00:11<00:00, 20.39it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:11<00:00, 18.09it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:23<01:33, 11.65s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 3/216 [00:00<00:09, 23.31it/s]\u001b[A\n",
      "  3%|▎         | 6/216 [00:00<00:10, 20.03it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:10, 19.50it/s]\u001b[A\n",
      "  5%|▌         | 11/216 [00:00<00:10, 18.85it/s]\u001b[A\n",
      "  6%|▌         | 13/216 [00:00<00:12, 16.47it/s]\u001b[A\n",
      "  7%|▋         | 15/216 [00:00<00:12, 15.98it/s]\u001b[A\n",
      "  8%|▊         | 18/216 [00:01<00:11, 16.98it/s]\u001b[A\n",
      "  9%|▉         | 20/216 [00:01<00:11, 16.72it/s]\u001b[A\n",
      " 11%|█         | 23/216 [00:01<00:10, 18.32it/s]\u001b[A\n",
      " 12%|█▏        | 26/216 [00:01<00:09, 19.20it/s]\u001b[A\n",
      " 13%|█▎        | 28/216 [00:01<00:09, 19.09it/s]\u001b[A\n",
      " 14%|█▍        | 31/216 [00:01<00:08, 20.64it/s]\u001b[A\n",
      " 16%|█▌        | 34/216 [00:01<00:09, 19.44it/s]\u001b[A\n",
      " 17%|█▋        | 36/216 [00:01<00:09, 18.71it/s]\u001b[A\n",
      " 18%|█▊        | 38/216 [00:02<00:09, 18.27it/s]\u001b[A\n",
      " 19%|█▉        | 41/216 [00:02<00:09, 17.80it/s]\u001b[A\n",
      " 20%|█▉        | 43/216 [00:02<00:09, 17.90it/s]\u001b[A\n",
      " 21%|██        | 45/216 [00:02<00:10, 16.43it/s]\u001b[A\n",
      " 22%|██▏       | 48/216 [00:02<00:09, 17.84it/s]\u001b[A\n",
      " 23%|██▎       | 50/216 [00:02<00:09, 17.26it/s]\u001b[A\n",
      " 24%|██▍       | 52/216 [00:02<00:10, 16.22it/s]\u001b[A\n",
      " 25%|██▌       | 54/216 [00:03<00:10, 15.88it/s]\u001b[A\n",
      " 26%|██▌       | 56/216 [00:03<00:10, 15.53it/s]\u001b[A\n",
      " 27%|██▋       | 58/216 [00:03<00:09, 16.52it/s]\u001b[A\n",
      " 28%|██▊       | 61/216 [00:03<00:08, 18.65it/s]\u001b[A\n",
      " 30%|██▉       | 64/216 [00:03<00:07, 20.33it/s]\u001b[A\n",
      " 31%|███       | 67/216 [00:03<00:06, 21.47it/s]\u001b[A\n",
      " 32%|███▏      | 70/216 [00:03<00:07, 18.61it/s]\u001b[A\n",
      " 33%|███▎      | 72/216 [00:03<00:07, 18.52it/s]\u001b[A\n",
      " 34%|███▍      | 74/216 [00:04<00:08, 17.19it/s]\u001b[A\n",
      " 36%|███▌      | 77/216 [00:04<00:07, 18.65it/s]\u001b[A\n",
      " 37%|███▋      | 79/216 [00:04<00:07, 18.91it/s]\u001b[A\n",
      " 38%|███▊      | 81/216 [00:04<00:07, 18.34it/s]\u001b[A\n",
      " 38%|███▊      | 83/216 [00:04<00:07, 18.25it/s]\u001b[A\n",
      " 39%|███▉      | 85/216 [00:04<00:07, 16.57it/s]\u001b[A\n",
      " 40%|████      | 87/216 [00:04<00:07, 16.94it/s]\u001b[A\n",
      " 42%|████▏     | 90/216 [00:04<00:06, 18.51it/s]\u001b[A\n",
      " 43%|████▎     | 92/216 [00:05<00:06, 17.86it/s]\u001b[A\n",
      " 44%|████▎     | 94/216 [00:05<00:07, 16.21it/s]\u001b[A\n",
      " 44%|████▍     | 96/216 [00:05<00:07, 16.55it/s]\u001b[A\n",
      " 46%|████▌     | 99/216 [00:05<00:06, 18.80it/s]\u001b[A\n",
      " 47%|████▋     | 101/216 [00:05<00:06, 18.07it/s]\u001b[A\n",
      " 48%|████▊     | 104/216 [00:05<00:05, 19.99it/s]\u001b[A\n",
      " 50%|████▉     | 107/216 [00:05<00:05, 19.43it/s]\u001b[A\n",
      " 50%|█████     | 109/216 [00:06<00:05, 19.27it/s]\u001b[A\n",
      " 51%|█████▏    | 111/216 [00:06<00:06, 17.30it/s]\u001b[A\n",
      " 52%|█████▏    | 113/216 [00:06<00:05, 17.89it/s]\u001b[A\n",
      " 53%|█████▎    | 115/216 [00:06<00:05, 17.72it/s]\u001b[A\n",
      " 54%|█████▍    | 117/216 [00:06<00:05, 17.34it/s]\u001b[A\n",
      " 55%|█████▌    | 119/216 [00:06<00:05, 17.09it/s]\u001b[A\n",
      " 56%|█████▌    | 121/216 [00:06<00:05, 16.60it/s]\u001b[A\n",
      " 57%|█████▋    | 123/216 [00:06<00:05, 17.44it/s]\u001b[A\n",
      " 58%|█████▊    | 125/216 [00:06<00:05, 17.89it/s]\u001b[A\n",
      " 59%|█████▉    | 127/216 [00:07<00:05, 17.53it/s]\u001b[A\n",
      " 60%|█████▉    | 129/216 [00:07<00:05, 16.76it/s]\u001b[A\n",
      " 61%|██████    | 131/216 [00:07<00:05, 16.78it/s]\u001b[A\n",
      " 62%|██████▏   | 134/216 [00:07<00:04, 18.69it/s]\u001b[A\n",
      " 63%|██████▎   | 136/216 [00:07<00:04, 18.38it/s]\u001b[A\n",
      " 64%|██████▍   | 139/216 [00:07<00:04, 18.96it/s]\u001b[A\n",
      " 65%|██████▌   | 141/216 [00:07<00:03, 18.80it/s]\u001b[A\n",
      " 67%|██████▋   | 144/216 [00:07<00:03, 20.36it/s]\u001b[A\n",
      " 68%|██████▊   | 147/216 [00:08<00:03, 18.70it/s]\u001b[A\n",
      " 69%|██████▉   | 150/216 [00:08<00:03, 20.00it/s]\u001b[A\n",
      " 71%|███████   | 153/216 [00:08<00:03, 19.40it/s]\u001b[A\n",
      " 72%|███████▏  | 156/216 [00:08<00:02, 20.50it/s]\u001b[A\n",
      " 74%|███████▎  | 159/216 [00:08<00:02, 20.24it/s]\u001b[A\n",
      " 75%|███████▌  | 162/216 [00:08<00:02, 19.34it/s]\u001b[A\n",
      " 76%|███████▌  | 164/216 [00:08<00:02, 18.59it/s]\u001b[A\n",
      " 77%|███████▋  | 166/216 [00:09<00:02, 18.29it/s]\u001b[A\n",
      " 78%|███████▊  | 168/216 [00:09<00:02, 17.87it/s]\u001b[A\n",
      " 79%|███████▊  | 170/216 [00:09<00:02, 17.79it/s]\u001b[A\n",
      " 80%|███████▉  | 172/216 [00:09<00:02, 17.32it/s]\u001b[A\n",
      " 81%|████████  | 174/216 [00:09<00:02, 17.66it/s]\u001b[A\n",
      " 82%|████████▏ | 177/216 [00:09<00:01, 19.62it/s]\u001b[A\n",
      " 83%|████████▎ | 179/216 [00:09<00:02, 17.22it/s]\u001b[A\n",
      " 84%|████████▍ | 182/216 [00:09<00:01, 18.92it/s]\u001b[A\n",
      " 85%|████████▌ | 184/216 [00:10<00:01, 18.10it/s]\u001b[A\n",
      " 86%|████████▌ | 186/216 [00:10<00:01, 16.54it/s]\u001b[A\n",
      " 87%|████████▋ | 188/216 [00:10<00:01, 15.23it/s]\u001b[A\n",
      " 88%|████████▊ | 191/216 [00:10<00:01, 17.52it/s]\u001b[A\n",
      " 90%|████████▉ | 194/216 [00:10<00:01, 19.37it/s]\u001b[A\n",
      " 91%|█████████ | 196/216 [00:10<00:01, 18.43it/s]\u001b[A\n",
      " 92%|█████████▏| 198/216 [00:10<00:01, 17.55it/s]\u001b[A\n",
      " 93%|█████████▎| 201/216 [00:11<00:00, 19.16it/s]\u001b[A\n",
      " 94%|█████████▍| 204/216 [00:11<00:00, 19.31it/s]\u001b[A\n",
      " 95%|█████████▌| 206/216 [00:11<00:00, 19.21it/s]\u001b[A\n",
      " 96%|█████████▋| 208/216 [00:11<00:00, 19.39it/s]\u001b[A\n",
      " 98%|█████████▊| 211/216 [00:11<00:00, 20.93it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:11<00:00, 18.28it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:34<01:22, 11.73s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 3/216 [00:00<00:08, 24.92it/s]\u001b[A\n",
      "  3%|▎         | 6/216 [00:00<00:08, 24.00it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:09, 22.72it/s]\u001b[A\n",
      "  6%|▌         | 12/216 [00:00<00:10, 19.47it/s]\u001b[A\n",
      "  7%|▋         | 15/216 [00:00<00:09, 20.87it/s]\u001b[A\n",
      "  8%|▊         | 18/216 [00:00<00:09, 21.88it/s]\u001b[A\n",
      " 10%|▉         | 21/216 [00:00<00:08, 22.27it/s]\u001b[A\n",
      " 11%|█         | 24/216 [00:01<00:09, 21.12it/s]\u001b[A\n",
      " 12%|█▎        | 27/216 [00:01<00:09, 20.59it/s]\u001b[A\n",
      " 14%|█▍        | 30/216 [00:01<00:09, 20.17it/s]\u001b[A\n",
      " 15%|█▌        | 33/216 [00:01<00:09, 19.92it/s]\u001b[A\n",
      " 17%|█▋        | 36/216 [00:01<00:08, 20.90it/s]\u001b[A\n",
      " 18%|█▊        | 39/216 [00:01<00:08, 21.11it/s]\u001b[A\n",
      " 19%|█▉        | 42/216 [00:01<00:07, 22.01it/s]\u001b[A\n",
      " 21%|██        | 45/216 [00:02<00:07, 22.47it/s]\u001b[A\n",
      " 22%|██▏       | 48/216 [00:02<00:07, 22.98it/s]\u001b[A\n",
      " 24%|██▎       | 51/216 [00:02<00:07, 23.40it/s]\u001b[A\n",
      " 25%|██▌       | 54/216 [00:02<00:07, 21.44it/s]\u001b[A\n",
      " 26%|██▋       | 57/216 [00:02<00:07, 20.95it/s]\u001b[A\n",
      " 28%|██▊       | 60/216 [00:02<00:07, 21.60it/s]\u001b[A\n",
      " 29%|██▉       | 63/216 [00:02<00:07, 20.82it/s]\u001b[A\n",
      " 31%|███       | 66/216 [00:03<00:07, 20.96it/s]\u001b[A\n",
      " 32%|███▏      | 69/216 [00:03<00:07, 19.61it/s]\u001b[A\n",
      " 33%|███▎      | 71/216 [00:03<00:07, 18.90it/s]\u001b[A\n",
      " 34%|███▍      | 74/216 [00:03<00:06, 20.40it/s]\u001b[A\n",
      " 36%|███▌      | 77/216 [00:03<00:07, 18.43it/s]\u001b[A\n",
      " 37%|███▋      | 79/216 [00:03<00:07, 17.94it/s]\u001b[A\n",
      " 38%|███▊      | 81/216 [00:03<00:07, 17.76it/s]\u001b[A\n",
      " 38%|███▊      | 83/216 [00:04<00:08, 16.60it/s]\u001b[A\n",
      " 39%|███▉      | 85/216 [00:04<00:07, 17.11it/s]\u001b[A\n",
      " 41%|████      | 88/216 [00:04<00:06, 19.24it/s]\u001b[A\n",
      " 42%|████▏     | 90/216 [00:04<00:06, 18.59it/s]\u001b[A\n",
      " 43%|████▎     | 92/216 [00:04<00:06, 18.25it/s]\u001b[A\n",
      " 44%|████▎     | 94/216 [00:04<00:07, 16.71it/s]\u001b[A\n",
      " 45%|████▍     | 97/216 [00:04<00:06, 18.89it/s]\u001b[A\n",
      " 46%|████▋     | 100/216 [00:04<00:05, 20.28it/s]\u001b[A\n",
      " 48%|████▊     | 103/216 [00:05<00:05, 19.76it/s]\u001b[A\n",
      " 49%|████▊     | 105/216 [00:05<00:06, 18.40it/s]\u001b[A\n",
      " 50%|█████     | 108/216 [00:05<00:05, 18.52it/s]\u001b[A\n",
      " 51%|█████     | 110/216 [00:05<00:05, 18.04it/s]\u001b[A\n",
      " 52%|█████▏    | 112/216 [00:05<00:05, 17.85it/s]\u001b[A\n",
      " 53%|█████▎    | 114/216 [00:05<00:05, 18.00it/s]\u001b[A\n",
      " 54%|█████▎    | 116/216 [00:05<00:05, 18.42it/s]\u001b[A\n",
      " 55%|█████▍    | 118/216 [00:05<00:05, 18.17it/s]\u001b[A\n",
      " 56%|█████▌    | 121/216 [00:06<00:04, 20.04it/s]\u001b[A\n",
      " 57%|█████▋    | 123/216 [00:06<00:04, 19.02it/s]\u001b[A\n",
      " 58%|█████▊    | 125/216 [00:06<00:07, 11.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 127/216 [00:06<00:07, 12.55it/s]\u001b[A\n",
      " 60%|█████▉    | 129/216 [00:06<00:06, 12.87it/s]\u001b[A\n",
      " 61%|██████    | 131/216 [00:06<00:06, 13.94it/s]\u001b[A\n",
      " 62%|██████▏   | 133/216 [00:07<00:05, 13.89it/s]\u001b[A\n",
      " 62%|██████▎   | 135/216 [00:07<00:05, 14.38it/s]\u001b[A\n",
      " 63%|██████▎   | 137/216 [00:07<00:05, 15.07it/s]\u001b[A\n",
      " 64%|██████▍   | 139/216 [00:07<00:05, 15.30it/s]\u001b[A\n",
      " 65%|██████▌   | 141/216 [00:07<00:04, 16.43it/s]\u001b[A\n",
      " 67%|██████▋   | 144/216 [00:07<00:04, 16.68it/s]\u001b[A\n",
      " 68%|██████▊   | 147/216 [00:07<00:03, 18.16it/s]\u001b[A\n",
      " 69%|██████▉   | 149/216 [00:07<00:03, 18.48it/s]\u001b[A\n",
      " 70%|███████   | 152/216 [00:08<00:03, 19.49it/s]\u001b[A\n",
      " 72%|███████▏  | 155/216 [00:08<00:03, 20.21it/s]\u001b[A\n",
      " 73%|███████▎  | 158/216 [00:08<00:03, 19.14it/s]\u001b[A\n",
      " 74%|███████▍  | 160/216 [00:08<00:02, 19.04it/s]\u001b[A\n",
      " 75%|███████▌  | 162/216 [00:08<00:03, 17.04it/s]\u001b[A\n",
      " 76%|███████▋  | 165/216 [00:08<00:02, 18.29it/s]\u001b[A\n",
      " 78%|███████▊  | 168/216 [00:08<00:02, 19.39it/s]\u001b[A\n",
      " 79%|███████▊  | 170/216 [00:09<00:02, 17.12it/s]\u001b[A\n",
      " 80%|███████▉  | 172/216 [00:09<00:02, 16.75it/s]\u001b[A\n",
      " 81%|████████  | 174/216 [00:09<00:02, 16.74it/s]\u001b[A\n",
      " 81%|████████▏ | 176/216 [00:09<00:02, 16.45it/s]\u001b[A\n",
      " 82%|████████▏ | 178/216 [00:09<00:02, 15.16it/s]\u001b[A\n",
      " 83%|████████▎ | 180/216 [00:09<00:02, 15.15it/s]\u001b[A\n",
      " 84%|████████▍ | 182/216 [00:09<00:02, 15.98it/s]\u001b[A\n",
      " 85%|████████▌ | 184/216 [00:10<00:02, 15.84it/s]\u001b[A\n",
      " 87%|████████▋ | 187/216 [00:10<00:01, 17.99it/s]\u001b[A\n",
      " 88%|████████▊ | 189/216 [00:10<00:01, 17.39it/s]\u001b[A\n",
      " 88%|████████▊ | 191/216 [00:10<00:01, 17.15it/s]\u001b[A\n",
      " 89%|████████▉ | 193/216 [00:10<00:01, 16.88it/s]\u001b[A\n",
      " 90%|█████████ | 195/216 [00:10<00:01, 17.02it/s]\u001b[A\n",
      " 92%|█████████▏| 198/216 [00:10<00:01, 17.10it/s]\u001b[A\n",
      " 93%|█████████▎| 201/216 [00:10<00:00, 18.83it/s]\u001b[A\n",
      " 94%|█████████▍| 203/216 [00:11<00:00, 18.05it/s]\u001b[A\n",
      " 95%|█████████▌| 206/216 [00:11<00:00, 17.94it/s]\u001b[A\n",
      " 97%|█████████▋| 209/216 [00:11<00:00, 18.02it/s]\u001b[A\n",
      " 98%|█████████▊| 212/216 [00:11<00:00, 19.26it/s]\u001b[A\n",
      " 99%|█████████▉| 214/216 [00:11<00:00, 18.70it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:11<00:00, 18.38it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:46<01:10, 11.74s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 3/216 [00:00<00:12, 17.37it/s]\u001b[A\n",
      "  3%|▎         | 6/216 [00:00<00:10, 20.17it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:09, 21.63it/s]\u001b[A\n",
      "  6%|▌         | 12/216 [00:00<00:10, 18.61it/s]\u001b[A\n",
      "  7%|▋         | 15/216 [00:00<00:10, 20.10it/s]\u001b[A\n",
      "  8%|▊         | 18/216 [00:00<00:09, 21.35it/s]\u001b[A\n",
      " 10%|▉         | 21/216 [00:01<00:09, 20.98it/s]\u001b[A\n",
      " 11%|█         | 24/216 [00:01<00:09, 19.75it/s]\u001b[A\n",
      " 12%|█▎        | 27/216 [00:01<00:10, 18.10it/s]\u001b[A\n",
      " 14%|█▍        | 30/216 [00:01<00:09, 19.30it/s]\u001b[A\n",
      " 15%|█▍        | 32/216 [00:01<00:10, 18.29it/s]\u001b[A\n",
      " 16%|█▌        | 34/216 [00:01<00:09, 18.37it/s]\u001b[A\n",
      " 17%|█▋        | 37/216 [00:01<00:09, 19.78it/s]\u001b[A\n",
      " 18%|█▊        | 39/216 [00:02<00:09, 18.64it/s]\u001b[A\n",
      " 19%|█▉        | 42/216 [00:02<00:08, 20.17it/s]\u001b[A\n",
      " 21%|██        | 45/216 [00:02<00:08, 21.18it/s]\u001b[A\n",
      " 22%|██▏       | 48/216 [00:02<00:07, 21.12it/s]\u001b[A\n",
      " 24%|██▎       | 51/216 [00:02<00:08, 20.30it/s]\u001b[A\n",
      " 25%|██▌       | 54/216 [00:02<00:08, 19.80it/s]\u001b[A\n",
      " 26%|██▌       | 56/216 [00:02<00:08, 18.40it/s]\u001b[A\n",
      " 27%|██▋       | 58/216 [00:03<00:09, 17.42it/s]\u001b[A\n",
      " 28%|██▊       | 60/216 [00:03<00:09, 15.85it/s]\u001b[A\n",
      " 29%|██▊       | 62/216 [00:03<00:09, 15.85it/s]\u001b[A\n",
      " 30%|███       | 65/216 [00:03<00:09, 16.16it/s]\u001b[A\n",
      " 31%|███       | 67/216 [00:03<00:09, 16.45it/s]\u001b[A\n",
      " 32%|███▏      | 70/216 [00:03<00:07, 18.38it/s]\u001b[A\n",
      " 34%|███▍      | 73/216 [00:03<00:07, 20.05it/s]\u001b[A\n",
      " 35%|███▌      | 76/216 [00:04<00:07, 18.63it/s]\u001b[A\n",
      " 36%|███▌      | 78/216 [00:04<00:07, 18.58it/s]\u001b[A\n",
      " 37%|███▋      | 80/216 [00:04<00:07, 18.01it/s]\u001b[A\n",
      " 38%|███▊      | 82/216 [00:04<00:07, 16.94it/s]\u001b[A\n",
      " 39%|███▉      | 84/216 [00:04<00:07, 17.47it/s]\u001b[A\n",
      " 40%|████      | 87/216 [00:04<00:06, 19.52it/s]\u001b[A\n",
      " 42%|████▏     | 90/216 [00:04<00:06, 19.97it/s]\u001b[A\n",
      " 43%|████▎     | 93/216 [00:04<00:06, 19.85it/s]\u001b[A\n",
      " 44%|████▍     | 96/216 [00:05<00:05, 21.07it/s]\u001b[A\n",
      " 46%|████▌     | 99/216 [00:05<00:05, 21.99it/s]\u001b[A\n",
      " 47%|████▋     | 102/216 [00:05<00:05, 22.43it/s]\u001b[A\n",
      " 49%|████▊     | 105/216 [00:05<00:04, 22.91it/s]\u001b[A\n",
      " 50%|█████     | 108/216 [00:05<00:05, 21.23it/s]\u001b[A\n",
      " 51%|█████▏    | 111/216 [00:05<00:05, 18.81it/s]\u001b[A\n",
      " 53%|█████▎    | 114/216 [00:05<00:05, 19.99it/s]\u001b[A\n",
      " 54%|█████▍    | 117/216 [00:06<00:05, 19.42it/s]\u001b[A\n",
      " 56%|█████▌    | 120/216 [00:06<00:04, 19.24it/s]\u001b[A\n",
      " 56%|█████▋    | 122/216 [00:06<00:05, 18.00it/s]\u001b[A\n",
      " 57%|█████▋    | 124/216 [00:06<00:05, 18.23it/s]\u001b[A\n",
      " 58%|█████▊    | 126/216 [00:06<00:05, 16.86it/s]\u001b[A\n",
      " 59%|█████▉    | 128/216 [00:06<00:05, 15.83it/s]\u001b[A\n",
      " 61%|██████    | 131/216 [00:06<00:04, 18.11it/s]\u001b[A\n",
      " 62%|██████▏   | 134/216 [00:07<00:04, 19.63it/s]\u001b[A\n",
      " 63%|██████▎   | 137/216 [00:07<00:04, 19.40it/s]\u001b[A\n",
      " 65%|██████▍   | 140/216 [00:07<00:03, 19.09it/s]\u001b[A\n",
      " 66%|██████▌   | 142/216 [00:07<00:03, 18.63it/s]\u001b[A\n",
      " 67%|██████▋   | 145/216 [00:07<00:03, 19.91it/s]\u001b[A\n",
      " 68%|██████▊   | 147/216 [00:07<00:03, 19.16it/s]\u001b[A\n",
      " 69%|██████▉   | 150/216 [00:07<00:03, 20.40it/s]\u001b[A\n",
      " 71%|███████   | 153/216 [00:07<00:03, 20.22it/s]\u001b[A\n",
      " 72%|███████▏  | 156/216 [00:08<00:03, 19.04it/s]\u001b[A\n",
      " 73%|███████▎  | 158/216 [00:08<00:03, 18.70it/s]\u001b[A\n",
      " 75%|███████▍  | 161/216 [00:08<00:02, 20.29it/s]\u001b[A\n",
      " 76%|███████▌  | 164/216 [00:08<00:02, 18.80it/s]\u001b[A\n",
      " 77%|███████▋  | 166/216 [00:08<00:02, 17.09it/s]\u001b[A\n",
      " 78%|███████▊  | 168/216 [00:08<00:02, 16.95it/s]\u001b[A\n",
      " 79%|███████▊  | 170/216 [00:08<00:02, 17.01it/s]\u001b[A\n",
      " 80%|███████▉  | 172/216 [00:09<00:02, 16.80it/s]\u001b[A\n",
      " 81%|████████  | 174/216 [00:09<00:02, 16.16it/s]\u001b[A\n",
      " 82%|████████▏ | 177/216 [00:09<00:02, 18.48it/s]\u001b[A\n",
      " 83%|████████▎ | 179/216 [00:09<00:02, 17.95it/s]\u001b[A\n",
      " 84%|████████▍ | 181/216 [00:09<00:01, 17.59it/s]\u001b[A\n",
      " 85%|████████▍ | 183/216 [00:09<00:01, 17.42it/s]\u001b[A\n",
      " 86%|████████▌ | 185/216 [00:09<00:01, 17.29it/s]\u001b[A\n",
      " 87%|████████▋ | 187/216 [00:09<00:01, 16.59it/s]\u001b[A\n",
      " 88%|████████▊ | 190/216 [00:10<00:01, 18.59it/s]\u001b[A\n",
      " 89%|████████▉ | 192/216 [00:10<00:01, 18.15it/s]\u001b[A\n",
      " 90%|█████████ | 195/216 [00:10<00:01, 19.95it/s]\u001b[A\n",
      " 92%|█████████▏| 198/216 [00:10<00:00, 20.09it/s]\u001b[A\n",
      " 93%|█████████▎| 201/216 [00:10<00:00, 17.48it/s]\u001b[A\n",
      " 94%|█████████▍| 204/216 [00:10<00:00, 18.99it/s]\u001b[A\n",
      " 95%|█████████▌| 206/216 [00:10<00:00, 17.30it/s]\u001b[A\n",
      " 97%|█████████▋| 209/216 [00:11<00:00, 19.07it/s]\u001b[A\n",
      " 98%|█████████▊| 211/216 [00:11<00:00, 17.36it/s]\u001b[A\n",
      " 99%|█████████▊| 213/216 [00:11<00:00, 17.17it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:11<00:00, 18.80it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:58<00:58, 11.65s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 3/216 [00:00<00:10, 20.23it/s]\u001b[A\n",
      "  3%|▎         | 6/216 [00:00<00:09, 21.76it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:10, 18.99it/s]\u001b[A\n",
      "  5%|▌         | 11/216 [00:00<00:10, 19.24it/s]\u001b[A\n",
      "  6%|▌         | 13/216 [00:00<00:11, 18.31it/s]\u001b[A\n",
      "  7%|▋         | 15/216 [00:00<00:11, 17.97it/s]\u001b[A\n",
      "  8%|▊         | 17/216 [00:00<00:10, 18.23it/s]\u001b[A\n",
      "  9%|▉         | 20/216 [00:01<00:09, 19.91it/s]\u001b[A\n",
      " 10%|█         | 22/216 [00:01<00:09, 19.74it/s]\u001b[A\n",
      " 11%|█         | 24/216 [00:01<00:10, 18.99it/s]\u001b[A\n",
      " 12%|█▏        | 26/216 [00:01<00:10, 18.17it/s]\u001b[A\n",
      " 13%|█▎        | 28/216 [00:01<00:10, 17.66it/s]\u001b[A\n",
      " 14%|█▍        | 30/216 [00:01<00:10, 17.56it/s]\u001b[A\n",
      " 15%|█▌        | 33/216 [00:01<00:10, 18.27it/s]\u001b[A\n",
      " 17%|█▋        | 36/216 [00:01<00:09, 19.81it/s]\u001b[A\n",
      " 18%|█▊        | 39/216 [00:02<00:08, 21.14it/s]\u001b[A\n",
      " 19%|█▉        | 42/216 [00:02<00:08, 21.03it/s]\u001b[A\n",
      " 21%|██        | 45/216 [00:02<00:08, 20.35it/s]\u001b[A\n",
      " 22%|██▏       | 48/216 [00:02<00:09, 18.39it/s]\u001b[A\n",
      " 23%|██▎       | 50/216 [00:02<00:09, 18.38it/s]\u001b[A\n",
      " 25%|██▍       | 53/216 [00:02<00:08, 19.76it/s]\u001b[A\n",
      " 26%|██▌       | 56/216 [00:02<00:08, 19.80it/s]\u001b[A\n",
      " 27%|██▋       | 59/216 [00:03<00:07, 21.04it/s]\u001b[A\n",
      " 29%|██▊       | 62/216 [00:03<00:07, 21.84it/s]\u001b[A\n",
      " 30%|███       | 65/216 [00:03<00:06, 22.56it/s]\u001b[A\n",
      " 31%|███▏      | 68/216 [00:03<00:06, 22.95it/s]\u001b[A\n",
      " 33%|███▎      | 71/216 [00:03<00:06, 23.36it/s]\u001b[A\n",
      " 34%|███▍      | 74/216 [00:03<00:06, 21.94it/s]\u001b[A\n",
      " 36%|███▌      | 77/216 [00:03<00:06, 22.34it/s]\u001b[A\n",
      " 37%|███▋      | 80/216 [00:03<00:06, 20.77it/s]\u001b[A\n",
      " 38%|███▊      | 83/216 [00:04<00:06, 19.53it/s]\u001b[A\n",
      " 39%|███▉      | 85/216 [00:04<00:07, 18.11it/s]\u001b[A\n",
      " 41%|████      | 88/216 [00:04<00:06, 19.63it/s]\u001b[A\n",
      " 42%|████▏     | 90/216 [00:04<00:06, 19.19it/s]\u001b[A\n",
      " 43%|████▎     | 92/216 [00:04<00:06, 18.29it/s]\u001b[A\n",
      " 44%|████▍     | 95/216 [00:04<00:06, 19.99it/s]\u001b[A\n",
      " 45%|████▌     | 98/216 [00:04<00:05, 21.27it/s]\u001b[A\n",
      " 47%|████▋     | 101/216 [00:05<00:05, 20.22it/s]\u001b[A\n",
      " 48%|████▊     | 104/216 [00:05<00:05, 19.95it/s]\u001b[A\n",
      " 50%|████▉     | 107/216 [00:05<00:05, 20.29it/s]\u001b[A\n",
      " 51%|█████     | 110/216 [00:05<00:05, 18.00it/s]\u001b[A\n",
      " 52%|█████▏    | 113/216 [00:05<00:05, 19.56it/s]\u001b[A\n",
      " 54%|█████▎    | 116/216 [00:05<00:05, 19.31it/s]\u001b[A\n",
      " 55%|█████▌    | 119/216 [00:05<00:04, 20.47it/s]\u001b[A\n",
      " 56%|█████▋    | 122/216 [00:06<00:04, 19.24it/s]\u001b[A\n",
      " 58%|█████▊    | 125/216 [00:06<00:04, 20.20it/s]\u001b[A\n",
      " 59%|█████▉    | 128/216 [00:06<00:04, 21.29it/s]\u001b[A\n",
      " 61%|██████    | 131/216 [00:06<00:04, 19.94it/s]\u001b[A\n",
      " 62%|██████▏   | 134/216 [00:06<00:04, 18.40it/s]\u001b[A\n",
      " 63%|██████▎   | 136/216 [00:06<00:04, 17.98it/s]\u001b[A\n",
      " 64%|██████▍   | 138/216 [00:07<00:04, 17.88it/s]\u001b[A\n",
      " 65%|██████▌   | 141/216 [00:07<00:04, 18.14it/s]\u001b[A\n",
      " 67%|██████▋   | 144/216 [00:07<00:03, 19.69it/s]\u001b[A\n",
      " 68%|██████▊   | 147/216 [00:07<00:03, 20.97it/s]\u001b[A\n",
      " 69%|██████▉   | 150/216 [00:07<00:03, 21.66it/s]\u001b[A\n",
      " 71%|███████   | 153/216 [00:07<00:03, 20.20it/s]\u001b[A\n",
      " 72%|███████▏  | 156/216 [00:07<00:02, 21.09it/s]\u001b[A\n",
      " 74%|███████▎  | 159/216 [00:08<00:02, 19.09it/s]\u001b[A\n",
      " 75%|███████▌  | 162/216 [00:08<00:02, 20.36it/s]\u001b[A\n",
      " 76%|███████▋  | 165/216 [00:08<00:02, 21.12it/s]\u001b[A\n",
      " 78%|███████▊  | 168/216 [00:08<00:02, 22.01it/s]\u001b[A\n",
      " 79%|███████▉  | 171/216 [00:08<00:02, 21.74it/s]\u001b[A\n",
      " 81%|████████  | 174/216 [00:08<00:02, 20.62it/s]\u001b[A\n",
      " 82%|████████▏ | 177/216 [00:08<00:01, 19.99it/s]\u001b[A\n",
      " 83%|████████▎ | 180/216 [00:09<00:01, 20.96it/s]\u001b[A\n",
      " 85%|████████▍ | 183/216 [00:09<00:01, 21.87it/s]\u001b[A\n",
      " 86%|████████▌ | 186/216 [00:09<00:01, 22.51it/s]\u001b[A\n",
      " 88%|████████▊ | 189/216 [00:09<00:01, 22.75it/s]\u001b[A\n",
      " 89%|████████▉ | 192/216 [00:09<00:01, 21.36it/s]\u001b[A\n",
      " 90%|█████████ | 195/216 [00:09<00:01, 19.40it/s]\u001b[A\n",
      " 91%|█████████ | 197/216 [00:09<00:01, 18.80it/s]\u001b[A\n",
      " 92%|█████████▏| 199/216 [00:09<00:00, 18.30it/s]\u001b[A\n",
      " 94%|█████████▎| 202/216 [00:10<00:00, 20.02it/s]\u001b[A\n",
      " 95%|█████████▍| 205/216 [00:10<00:00, 21.14it/s]\u001b[A\n",
      " 96%|█████████▋| 208/216 [00:10<00:00, 19.65it/s]\u001b[A\n",
      " 98%|█████████▊| 211/216 [00:10<00:00, 20.15it/s]\u001b[A\n",
      " 99%|█████████▉| 214/216 [00:10<00:00, 18.88it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:10<00:00, 19.96it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [01:09<00:45, 11.37s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 3/216 [00:00<00:08, 24.44it/s]\u001b[A\n",
      "  3%|▎         | 6/216 [00:00<00:08, 23.79it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:09, 22.47it/s]\u001b[A\n",
      "  6%|▌         | 12/216 [00:00<00:09, 20.90it/s]\u001b[A\n",
      "  7%|▋         | 15/216 [00:00<00:09, 20.31it/s]\u001b[A\n",
      "  8%|▊         | 18/216 [00:00<00:09, 19.90it/s]\u001b[A\n",
      " 10%|▉         | 21/216 [00:01<00:09, 19.96it/s]\u001b[A\n",
      " 11%|█         | 24/216 [00:01<00:09, 19.64it/s]\u001b[A\n",
      " 12%|█▏        | 26/216 [00:01<00:09, 19.14it/s]\u001b[A\n",
      " 13%|█▎        | 28/216 [00:01<00:10, 18.50it/s]\u001b[A\n",
      " 14%|█▍        | 30/216 [00:01<00:10, 17.87it/s]\u001b[A\n",
      " 15%|█▌        | 33/216 [00:01<00:09, 19.77it/s]\u001b[A\n",
      " 17%|█▋        | 36/216 [00:01<00:08, 20.89it/s]\u001b[A\n",
      " 18%|█▊        | 39/216 [00:01<00:08, 20.66it/s]\u001b[A\n",
      " 19%|█▉        | 42/216 [00:02<00:08, 20.79it/s]\u001b[A\n",
      " 21%|██        | 45/216 [00:02<00:08, 20.55it/s]\u001b[A\n",
      " 22%|██▏       | 48/216 [00:02<00:08, 19.66it/s]\u001b[A\n",
      " 23%|██▎       | 50/216 [00:02<00:08, 18.97it/s]\u001b[A\n",
      " 24%|██▍       | 52/216 [00:02<00:09, 17.13it/s]\u001b[A\n",
      " 25%|██▌       | 54/216 [00:02<00:10, 15.81it/s]\u001b[A\n",
      " 26%|██▌       | 56/216 [00:02<00:09, 16.26it/s]\u001b[A\n",
      " 27%|██▋       | 59/216 [00:03<00:08, 17.70it/s]\u001b[A\n",
      " 28%|██▊       | 61/216 [00:03<00:08, 17.76it/s]\u001b[A\n",
      " 30%|██▉       | 64/216 [00:03<00:07, 19.64it/s]\u001b[A\n",
      " 31%|███       | 66/216 [00:03<00:07, 19.30it/s]\u001b[A\n",
      " 31%|███▏      | 68/216 [00:03<00:08, 18.47it/s]\u001b[A\n",
      " 32%|███▏      | 70/216 [00:03<00:08, 16.97it/s]\u001b[A\n",
      " 34%|███▍      | 73/216 [00:03<00:07, 19.13it/s]\u001b[A\n",
      " 35%|███▍      | 75/216 [00:03<00:07, 18.62it/s]\u001b[A\n",
      " 36%|███▌      | 77/216 [00:04<00:07, 18.07it/s]\u001b[A\n",
      " 37%|███▋      | 79/216 [00:04<00:08, 16.92it/s]\u001b[A\n",
      " 38%|███▊      | 81/216 [00:04<00:07, 17.07it/s]\u001b[A\n",
      " 38%|███▊      | 83/216 [00:04<00:07, 17.11it/s]\u001b[A\n",
      " 40%|███▉      | 86/216 [00:04<00:06, 19.02it/s]\u001b[A\n",
      " 41%|████      | 89/216 [00:04<00:06, 20.61it/s]\u001b[A\n",
      " 43%|████▎     | 92/216 [00:04<00:05, 21.45it/s]\u001b[A\n",
      " 44%|████▍     | 95/216 [00:04<00:05, 21.01it/s]\u001b[A\n",
      " 45%|████▌     | 98/216 [00:05<00:05, 22.00it/s]\u001b[A\n",
      " 47%|████▋     | 101/216 [00:05<00:05, 22.42it/s]\u001b[A\n",
      " 48%|████▊     | 104/216 [00:05<00:04, 22.97it/s]\u001b[A\n",
      " 50%|████▉     | 107/216 [00:05<00:04, 23.33it/s]\u001b[A\n",
      " 51%|█████     | 110/216 [00:05<00:04, 23.34it/s]\u001b[A\n",
      " 52%|█████▏    | 113/216 [00:05<00:04, 22.27it/s]\u001b[A\n",
      " 54%|█████▎    | 116/216 [00:05<00:04, 20.36it/s]\u001b[A\n",
      " 55%|█████▌    | 119/216 [00:06<00:04, 20.72it/s]\u001b[A\n",
      " 56%|█████▋    | 122/216 [00:06<00:04, 21.70it/s]\u001b[A\n",
      " 58%|█████▊    | 125/216 [00:06<00:04, 21.77it/s]\u001b[A\n",
      " 59%|█████▉    | 128/216 [00:06<00:04, 20.88it/s]\u001b[A\n",
      " 61%|██████    | 131/216 [00:06<00:04, 18.47it/s]\u001b[A\n",
      " 62%|██████▏   | 133/216 [00:06<00:04, 18.22it/s]\u001b[A\n",
      " 63%|██████▎   | 136/216 [00:06<00:04, 19.73it/s]\u001b[A\n",
      " 64%|██████▍   | 139/216 [00:07<00:03, 19.34it/s]\u001b[A\n",
      " 65%|██████▌   | 141/216 [00:07<00:04, 17.30it/s]\u001b[A\n",
      " 66%|██████▌   | 143/216 [00:07<00:04, 17.85it/s]\u001b[A\n",
      " 67%|██████▋   | 145/216 [00:07<00:04, 17.57it/s]\u001b[A\n",
      " 68%|██████▊   | 147/216 [00:07<00:03, 17.49it/s]\u001b[A\n",
      " 69%|██████▉   | 149/216 [00:07<00:04, 15.73it/s]\u001b[A\n",
      " 70%|██████▉   | 151/216 [00:07<00:04, 16.19it/s]\u001b[A\n",
      " 71%|███████   | 153/216 [00:07<00:03, 16.40it/s]\u001b[A\n",
      " 72%|███████▏  | 155/216 [00:08<00:03, 15.75it/s]\u001b[A\n",
      " 73%|███████▎  | 158/216 [00:08<00:03, 17.93it/s]\u001b[A\n",
      " 75%|███████▍  | 161/216 [00:08<00:02, 18.99it/s]\u001b[A\n",
      " 75%|███████▌  | 163/216 [00:08<00:02, 19.16it/s]\u001b[A\n",
      " 77%|███████▋  | 166/216 [00:08<00:02, 20.41it/s]\u001b[A\n",
      " 78%|███████▊  | 169/216 [00:08<00:02, 21.52it/s]\u001b[A\n",
      " 80%|███████▉  | 172/216 [00:08<00:01, 22.12it/s]\u001b[A\n",
      " 81%|████████  | 175/216 [00:08<00:01, 20.94it/s]\u001b[A\n",
      " 82%|████████▏ | 178/216 [00:09<00:01, 20.29it/s]\u001b[A\n",
      " 84%|████████▍ | 181/216 [00:09<00:01, 21.21it/s]\u001b[A\n",
      " 85%|████████▌ | 184/216 [00:09<00:01, 22.06it/s]\u001b[A\n",
      " 87%|████████▋ | 187/216 [00:09<00:01, 20.75it/s]\u001b[A\n",
      " 88%|████████▊ | 190/216 [00:09<00:01, 19.24it/s]\u001b[A\n",
      " 89%|████████▉ | 192/216 [00:09<00:01, 18.76it/s]\u001b[A\n",
      " 90%|████████▉ | 194/216 [00:09<00:01, 18.48it/s]\u001b[A\n",
      " 91%|█████████ | 196/216 [00:10<00:01, 18.20it/s]\u001b[A\n",
      " 92%|█████████▏| 198/216 [00:10<00:01, 16.75it/s]\u001b[A\n",
      " 93%|█████████▎| 200/216 [00:10<00:00, 16.45it/s]\u001b[A\n",
      " 94%|█████████▍| 203/216 [00:10<00:00, 17.59it/s]\u001b[A\n",
      " 95%|█████████▍| 205/216 [00:10<00:00, 16.03it/s]\u001b[A\n",
      " 96%|█████████▌| 207/216 [00:10<00:00, 16.03it/s]\u001b[A\n",
      " 97%|█████████▋| 209/216 [00:10<00:00, 15.20it/s]\u001b[A\n",
      " 98%|█████████▊| 212/216 [00:11<00:00, 17.46it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:11<00:00, 19.22it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [01:20<00:33, 11.33s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 3/216 [00:00<00:08, 24.52it/s]\u001b[A\n",
      "  3%|▎         | 6/216 [00:00<00:09, 21.88it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:10, 20.52it/s]\u001b[A\n",
      "  6%|▌         | 12/216 [00:00<00:10, 19.87it/s]\u001b[A\n",
      "  7%|▋         | 15/216 [00:00<00:10, 18.79it/s]\u001b[A\n",
      "  8%|▊         | 18/216 [00:00<00:10, 18.89it/s]\u001b[A\n",
      "  9%|▉         | 20/216 [00:01<00:11, 16.78it/s]\u001b[A\n",
      " 10%|█         | 22/216 [00:01<00:11, 17.22it/s]\u001b[A\n",
      " 11%|█         | 24/216 [00:01<00:11, 17.33it/s]\u001b[A\n",
      " 12%|█▎        | 27/216 [00:01<00:09, 19.44it/s]\u001b[A\n",
      " 14%|█▍        | 30/216 [00:01<00:09, 19.54it/s]\u001b[A\n",
      " 15%|█▍        | 32/216 [00:01<00:09, 18.93it/s]\u001b[A\n",
      " 16%|█▌        | 35/216 [00:01<00:08, 20.53it/s]\u001b[A\n",
      " 18%|█▊        | 38/216 [00:01<00:09, 19.09it/s]\u001b[A\n",
      " 19%|█▉        | 41/216 [00:02<00:09, 19.20it/s]\u001b[A\n",
      " 20%|█▉        | 43/216 [00:02<00:09, 18.56it/s]\u001b[A\n",
      " 21%|██        | 45/216 [00:02<00:09, 18.00it/s]\u001b[A\n",
      " 22%|██▏       | 47/216 [00:02<00:10, 16.50it/s]\u001b[A\n",
      " 23%|██▎       | 49/216 [00:02<00:10, 16.58it/s]\u001b[A\n",
      " 24%|██▎       | 51/216 [00:02<00:10, 16.20it/s]\u001b[A\n",
      " 25%|██▍       | 53/216 [00:03<00:16,  9.97it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 55/216 [00:03<00:14, 11.26it/s]\u001b[A\n",
      " 26%|██▋       | 57/216 [00:03<00:13, 12.08it/s]\u001b[A\n",
      " 27%|██▋       | 59/216 [00:03<00:12, 12.25it/s]\u001b[A\n",
      " 28%|██▊       | 61/216 [00:03<00:12, 12.72it/s]\u001b[A\n",
      " 29%|██▉       | 63/216 [00:03<00:11, 13.30it/s]\u001b[A\n",
      " 30%|███       | 65/216 [00:03<00:10, 14.09it/s]\u001b[A\n",
      " 31%|███       | 67/216 [00:04<00:09, 14.95it/s]\u001b[A\n",
      " 32%|███▏      | 69/216 [00:04<00:09, 16.04it/s]\u001b[A\n",
      " 33%|███▎      | 72/216 [00:04<00:08, 17.67it/s]\u001b[A\n",
      " 34%|███▍      | 74/216 [00:04<00:08, 17.19it/s]\u001b[A\n",
      " 35%|███▌      | 76/216 [00:04<00:09, 15.13it/s]\u001b[A\n",
      " 36%|███▌      | 78/216 [00:04<00:08, 15.39it/s]\u001b[A\n",
      " 38%|███▊      | 81/216 [00:04<00:07, 17.13it/s]\u001b[A\n",
      " 38%|███▊      | 83/216 [00:05<00:07, 17.13it/s]\u001b[A\n",
      " 39%|███▉      | 85/216 [00:05<00:07, 17.66it/s]\u001b[A\n",
      " 40%|████      | 87/216 [00:05<00:07, 17.15it/s]\u001b[A\n",
      " 42%|████▏     | 90/216 [00:05<00:06, 18.55it/s]\u001b[A\n",
      " 43%|████▎     | 92/216 [00:05<00:07, 17.45it/s]\u001b[A\n",
      " 44%|████▍     | 95/216 [00:05<00:06, 18.76it/s]\u001b[A\n",
      " 45%|████▌     | 98/216 [00:05<00:06, 19.67it/s]\u001b[A\n",
      " 46%|████▋     | 100/216 [00:05<00:06, 18.32it/s]\u001b[A\n",
      " 48%|████▊     | 103/216 [00:06<00:05, 19.41it/s]\u001b[A\n",
      " 49%|████▊     | 105/216 [00:06<00:06, 17.10it/s]\u001b[A\n",
      " 50%|████▉     | 107/216 [00:06<00:06, 17.27it/s]\u001b[A\n",
      " 50%|█████     | 109/216 [00:06<00:06, 16.48it/s]\u001b[A\n",
      " 51%|█████▏    | 111/216 [00:06<00:06, 16.44it/s]\u001b[A\n",
      " 52%|█████▏    | 113/216 [00:06<00:06, 16.31it/s]\u001b[A\n",
      " 54%|█████▎    | 116/216 [00:06<00:05, 17.17it/s]\u001b[A\n",
      " 55%|█████▌    | 119/216 [00:07<00:05, 18.48it/s]\u001b[A\n",
      " 56%|█████▌    | 121/216 [00:07<00:05, 18.63it/s]\u001b[A\n",
      " 57%|█████▋    | 124/216 [00:07<00:04, 19.52it/s]\u001b[A\n",
      " 58%|█████▊    | 126/216 [00:07<00:05, 17.43it/s]\u001b[A\n",
      " 59%|█████▉    | 128/216 [00:07<00:05, 17.07it/s]\u001b[A\n",
      " 60%|██████    | 130/216 [00:07<00:04, 17.21it/s]\u001b[A\n",
      " 62%|██████▏   | 133/216 [00:07<00:04, 17.31it/s]\u001b[A\n",
      " 63%|██████▎   | 136/216 [00:08<00:04, 17.75it/s]\u001b[A\n",
      " 64%|██████▍   | 138/216 [00:08<00:04, 16.24it/s]\u001b[A\n",
      " 65%|██████▍   | 140/216 [00:08<00:04, 16.18it/s]\u001b[A\n",
      " 66%|██████▌   | 142/216 [00:08<00:04, 16.07it/s]\u001b[A\n",
      " 67%|██████▋   | 145/216 [00:08<00:03, 18.10it/s]\u001b[A\n",
      " 69%|██████▊   | 148/216 [00:08<00:03, 19.38it/s]\u001b[A\n",
      " 69%|██████▉   | 150/216 [00:08<00:03, 18.04it/s]\u001b[A\n",
      " 70%|███████   | 152/216 [00:08<00:03, 17.44it/s]\u001b[A\n",
      " 71%|███████▏  | 154/216 [00:09<00:03, 17.55it/s]\u001b[A\n",
      " 72%|███████▏  | 156/216 [00:09<00:03, 17.29it/s]\u001b[A\n",
      " 73%|███████▎  | 158/216 [00:09<00:03, 17.58it/s]\u001b[A\n",
      " 74%|███████▍  | 160/216 [00:09<00:03, 17.30it/s]\u001b[A\n",
      " 75%|███████▌  | 162/216 [00:09<00:03, 16.64it/s]\u001b[A\n",
      " 76%|███████▌  | 164/216 [00:09<00:03, 16.55it/s]\u001b[A\n",
      " 77%|███████▋  | 166/216 [00:09<00:03, 16.25it/s]\u001b[A\n",
      " 78%|███████▊  | 169/216 [00:09<00:02, 18.35it/s]\u001b[A\n",
      " 79%|███████▉  | 171/216 [00:10<00:02, 16.37it/s]\u001b[A\n",
      " 81%|████████  | 174/216 [00:10<00:02, 17.46it/s]\u001b[A\n",
      " 81%|████████▏ | 176/216 [00:10<00:02, 16.48it/s]\u001b[A\n",
      " 82%|████████▏ | 178/216 [00:10<00:02, 16.64it/s]\u001b[A\n",
      " 84%|████████▍ | 181/216 [00:10<00:01, 18.57it/s]\u001b[A\n",
      " 85%|████████▌ | 184/216 [00:10<00:01, 18.39it/s]\u001b[A\n",
      " 86%|████████▌ | 186/216 [00:10<00:01, 18.42it/s]\u001b[A\n",
      " 87%|████████▋ | 188/216 [00:11<00:01, 17.93it/s]\u001b[A\n",
      " 88%|████████▊ | 190/216 [00:11<00:01, 17.56it/s]\u001b[A\n",
      " 89%|████████▉ | 192/216 [00:11<00:01, 17.81it/s]\u001b[A\n",
      " 90%|█████████ | 195/216 [00:11<00:01, 19.78it/s]\u001b[A\n",
      " 91%|█████████ | 197/216 [00:11<00:01, 18.94it/s]\u001b[A\n",
      " 92%|█████████▏| 199/216 [00:11<00:00, 18.58it/s]\u001b[A\n",
      " 94%|█████████▎| 202/216 [00:11<00:00, 18.75it/s]\u001b[A\n",
      " 94%|█████████▍| 204/216 [00:11<00:00, 18.04it/s]\u001b[A\n",
      " 95%|█████████▌| 206/216 [00:11<00:00, 18.07it/s]\u001b[A\n",
      " 96%|█████████▋| 208/216 [00:12<00:00, 16.97it/s]\u001b[A\n",
      " 97%|█████████▋| 210/216 [00:12<00:00, 17.48it/s]\u001b[A\n",
      " 98%|█████████▊| 212/216 [00:12<00:00, 17.11it/s]\u001b[A\n",
      " 99%|█████████▉| 214/216 [00:12<00:00, 17.08it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:12<00:00, 17.15it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [01:32<00:23, 11.73s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 2/216 [00:00<00:12, 16.56it/s]\u001b[A\n",
      "  2%|▏         | 5/216 [00:00<00:11, 17.92it/s]\u001b[A\n",
      "  3%|▎         | 7/216 [00:00<00:12, 16.64it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:12, 17.17it/s]\u001b[A\n",
      "  5%|▌         | 11/216 [00:00<00:13, 15.17it/s]\u001b[A\n",
      "  6%|▌         | 13/216 [00:00<00:12, 15.93it/s]\u001b[A\n",
      "  7%|▋         | 15/216 [00:00<00:12, 16.24it/s]\u001b[A\n",
      "  8%|▊         | 18/216 [00:01<00:10, 18.72it/s]\u001b[A\n",
      " 10%|▉         | 21/216 [00:01<00:09, 20.21it/s]\u001b[A\n",
      " 11%|█         | 24/216 [00:01<00:09, 20.58it/s]\u001b[A\n",
      " 12%|█▎        | 27/216 [00:01<00:09, 20.06it/s]\u001b[A\n",
      " 14%|█▍        | 30/216 [00:01<00:10, 18.25it/s]\u001b[A\n",
      " 15%|█▍        | 32/216 [00:01<00:10, 18.19it/s]\u001b[A\n",
      " 16%|█▌        | 34/216 [00:01<00:10, 17.37it/s]\u001b[A\n",
      " 17%|█▋        | 36/216 [00:02<00:10, 17.66it/s]\u001b[A\n",
      " 18%|█▊        | 38/216 [00:02<00:10, 17.76it/s]\u001b[A\n",
      " 19%|█▉        | 41/216 [00:02<00:08, 19.58it/s]\u001b[A\n",
      " 20%|█▉        | 43/216 [00:02<00:09, 19.12it/s]\u001b[A\n",
      " 21%|██        | 45/216 [00:02<00:09, 18.43it/s]\u001b[A\n",
      " 22%|██▏       | 47/216 [00:02<00:09, 17.23it/s]\u001b[A\n",
      " 23%|██▎       | 50/216 [00:02<00:08, 19.30it/s]\u001b[A\n",
      " 24%|██▍       | 52/216 [00:02<00:08, 18.31it/s]\u001b[A\n",
      " 25%|██▌       | 55/216 [00:02<00:08, 20.06it/s]\u001b[A\n",
      " 27%|██▋       | 58/216 [00:03<00:07, 21.27it/s]\u001b[A\n",
      " 28%|██▊       | 61/216 [00:03<00:07, 19.55it/s]\u001b[A\n",
      " 30%|██▉       | 64/216 [00:03<00:07, 19.24it/s]\u001b[A\n",
      " 31%|███       | 67/216 [00:03<00:07, 20.52it/s]\u001b[A\n",
      " 32%|███▏      | 70/216 [00:03<00:07, 18.29it/s]\u001b[A\n",
      " 33%|███▎      | 72/216 [00:03<00:08, 17.96it/s]\u001b[A\n",
      " 35%|███▍      | 75/216 [00:04<00:07, 19.60it/s]\u001b[A\n",
      " 36%|███▌      | 78/216 [00:04<00:07, 18.70it/s]\u001b[A\n",
      " 38%|███▊      | 81/216 [00:04<00:06, 20.15it/s]\u001b[A\n",
      " 39%|███▉      | 84/216 [00:04<00:06, 21.06it/s]\u001b[A\n",
      " 40%|████      | 87/216 [00:04<00:05, 21.92it/s]\u001b[A\n",
      " 42%|████▏     | 90/216 [00:04<00:06, 20.88it/s]\u001b[A\n",
      " 43%|████▎     | 93/216 [00:04<00:06, 18.41it/s]\u001b[A\n",
      " 44%|████▍     | 95/216 [00:05<00:06, 18.08it/s]\u001b[A\n",
      " 45%|████▍     | 97/216 [00:05<00:06, 17.90it/s]\u001b[A\n",
      " 46%|████▋     | 100/216 [00:05<00:05, 19.45it/s]\u001b[A\n",
      " 48%|████▊     | 103/216 [00:05<00:05, 20.77it/s]\u001b[A\n",
      " 49%|████▉     | 106/216 [00:05<00:05, 19.86it/s]\u001b[A\n",
      " 50%|█████     | 109/216 [00:05<00:05, 18.52it/s]\u001b[A\n",
      " 52%|█████▏    | 112/216 [00:05<00:05, 19.93it/s]\u001b[A\n",
      " 53%|█████▎    | 115/216 [00:06<00:04, 21.08it/s]\u001b[A\n",
      " 55%|█████▍    | 118/216 [00:06<00:05, 19.21it/s]\u001b[A\n",
      " 56%|█████▌    | 120/216 [00:06<00:05, 18.36it/s]\u001b[A\n",
      " 56%|█████▋    | 122/216 [00:06<00:05, 17.54it/s]\u001b[A\n",
      " 57%|█████▋    | 124/216 [00:06<00:05, 17.08it/s]\u001b[A\n",
      " 58%|█████▊    | 126/216 [00:06<00:05, 16.99it/s]\u001b[A\n",
      " 60%|█████▉    | 129/216 [00:06<00:04, 18.44it/s]\u001b[A\n",
      " 61%|██████    | 132/216 [00:06<00:04, 19.69it/s]\u001b[A\n",
      " 62%|██████▎   | 135/216 [00:07<00:04, 20.13it/s]\u001b[A\n",
      " 64%|██████▍   | 138/216 [00:07<00:04, 18.75it/s]\u001b[A\n",
      " 65%|██████▍   | 140/216 [00:07<00:04, 18.98it/s]\u001b[A\n",
      " 66%|██████▌   | 143/216 [00:07<00:03, 20.36it/s]\u001b[A\n",
      " 68%|██████▊   | 146/216 [00:07<00:03, 21.46it/s]\u001b[A\n",
      " 69%|██████▉   | 149/216 [00:07<00:03, 20.36it/s]\u001b[A\n",
      " 70%|███████   | 152/216 [00:07<00:03, 19.82it/s]\u001b[A\n",
      " 72%|███████▏  | 155/216 [00:08<00:02, 20.36it/s]\u001b[A\n",
      " 73%|███████▎  | 158/216 [00:08<00:02, 19.82it/s]\u001b[A\n",
      " 75%|███████▍  | 161/216 [00:08<00:02, 20.97it/s]\u001b[A\n",
      " 76%|███████▌  | 164/216 [00:08<00:02, 20.20it/s]\u001b[A\n",
      " 77%|███████▋  | 167/216 [00:08<00:02, 21.25it/s]\u001b[A\n",
      " 79%|███████▊  | 170/216 [00:08<00:02, 20.62it/s]\u001b[A\n",
      " 80%|████████  | 173/216 [00:09<00:02, 19.55it/s]\u001b[A\n",
      " 81%|████████  | 175/216 [00:09<00:02, 18.99it/s]\u001b[A\n",
      " 82%|████████▏ | 177/216 [00:09<00:02, 18.64it/s]\u001b[A\n",
      " 83%|████████▎ | 180/216 [00:09<00:01, 18.43it/s]\u001b[A\n",
      " 84%|████████▍ | 182/216 [00:09<00:01, 18.73it/s]\u001b[A\n",
      " 86%|████████▌ | 185/216 [00:09<00:01, 20.32it/s]\u001b[A\n",
      " 87%|████████▋ | 188/216 [00:09<00:01, 21.31it/s]\u001b[A\n",
      " 88%|████████▊ | 191/216 [00:09<00:01, 20.52it/s]\u001b[A\n",
      " 90%|████████▉ | 194/216 [00:10<00:01, 19.68it/s]\u001b[A\n",
      " 91%|█████████ | 196/216 [00:10<00:01, 17.91it/s]\u001b[A\n",
      " 92%|█████████▏| 199/216 [00:10<00:00, 19.61it/s]\u001b[A\n",
      " 94%|█████████▎| 202/216 [00:10<00:00, 20.91it/s]\u001b[A\n",
      " 95%|█████████▍| 205/216 [00:10<00:00, 21.61it/s]\u001b[A\n",
      " 96%|█████████▋| 208/216 [00:10<00:00, 22.36it/s]\u001b[A\n",
      " 98%|█████████▊| 211/216 [00:10<00:00, 21.97it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:11<00:00, 19.42it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [01:44<00:11, 11.54s/it]\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 2/216 [00:00<00:16, 13.36it/s]\u001b[A\n",
      "  2%|▏         | 5/216 [00:00<00:11, 18.71it/s]\u001b[A\n",
      "  3%|▎         | 7/216 [00:00<00:11, 18.08it/s]\u001b[A\n",
      "  4%|▍         | 9/216 [00:00<00:11, 17.64it/s]\u001b[A\n",
      "  5%|▌         | 11/216 [00:00<00:11, 18.16it/s]\u001b[A\n",
      "  6%|▌         | 13/216 [00:00<00:11, 17.76it/s]\u001b[A\n",
      "  7%|▋         | 15/216 [00:00<00:11, 18.27it/s]\u001b[A\n",
      "  8%|▊         | 17/216 [00:00<00:11, 18.06it/s]\u001b[A\n",
      "  9%|▉         | 19/216 [00:01<00:10, 18.42it/s]\u001b[A\n",
      " 10%|█         | 22/216 [00:01<00:09, 20.09it/s]\u001b[A\n",
      " 11%|█         | 24/216 [00:01<00:10, 19.11it/s]\u001b[A\n",
      " 12%|█▏        | 26/216 [00:01<00:10, 18.58it/s]\u001b[A\n",
      " 13%|█▎        | 29/216 [00:01<00:09, 20.21it/s]\u001b[A\n",
      " 15%|█▍        | 32/216 [00:01<00:08, 21.35it/s]\u001b[A\n",
      " 16%|█▌        | 35/216 [00:01<00:09, 19.05it/s]\u001b[A\n",
      " 17%|█▋        | 37/216 [00:01<00:09, 19.07it/s]\u001b[A\n",
      " 18%|█▊        | 39/216 [00:02<00:09, 18.72it/s]\u001b[A\n",
      " 19%|█▉        | 41/216 [00:02<00:09, 18.49it/s]\u001b[A\n",
      " 20%|█▉        | 43/216 [00:02<00:09, 17.75it/s]\u001b[A\n",
      " 21%|██▏       | 46/216 [00:02<00:08, 19.54it/s]\u001b[A\n",
      " 23%|██▎       | 49/216 [00:02<00:08, 19.81it/s]\u001b[A\n",
      " 24%|██▎       | 51/216 [00:02<00:08, 18.92it/s]\u001b[A\n",
      " 25%|██▌       | 54/216 [00:02<00:08, 18.63it/s]\u001b[A\n",
      " 26%|██▌       | 56/216 [00:02<00:08, 18.25it/s]\u001b[A\n",
      " 27%|██▋       | 59/216 [00:03<00:07, 20.00it/s]\u001b[A\n",
      " 29%|██▊       | 62/216 [00:03<00:07, 21.02it/s]\u001b[A\n",
      " 30%|███       | 65/216 [00:03<00:07, 20.29it/s]\u001b[A\n",
      " 31%|███▏      | 68/216 [00:03<00:07, 20.33it/s]\u001b[A\n",
      " 33%|███▎      | 71/216 [00:03<00:07, 20.38it/s]\u001b[A\n",
      " 34%|███▍      | 74/216 [00:03<00:07, 20.12it/s]\u001b[A\n",
      " 36%|███▌      | 77/216 [00:04<00:07, 19.54it/s]\u001b[A\n",
      " 37%|███▋      | 79/216 [00:04<00:07, 19.11it/s]\u001b[A\n",
      " 38%|███▊      | 82/216 [00:04<00:06, 20.38it/s]\u001b[A\n",
      " 39%|███▉      | 85/216 [00:04<00:06, 19.50it/s]\u001b[A\n",
      " 40%|████      | 87/216 [00:04<00:06, 18.67it/s]\u001b[A\n",
      " 42%|████▏     | 90/216 [00:04<00:06, 18.98it/s]\u001b[A\n",
      " 43%|████▎     | 92/216 [00:04<00:06, 17.92it/s]\u001b[A\n",
      " 44%|████▎     | 94/216 [00:04<00:06, 17.54it/s]\u001b[A\n",
      " 44%|████▍     | 96/216 [00:05<00:07, 16.16it/s]\u001b[A\n",
      " 46%|████▌     | 99/216 [00:05<00:06, 18.34it/s]\u001b[A\n",
      " 47%|████▋     | 102/216 [00:05<00:05, 19.82it/s]\u001b[A\n",
      " 49%|████▊     | 105/216 [00:05<00:05, 21.05it/s]\u001b[A\n",
      " 50%|█████     | 108/216 [00:05<00:05, 19.58it/s]\u001b[A\n",
      " 51%|█████▏    | 111/216 [00:05<00:05, 20.75it/s]\u001b[A\n",
      " 53%|█████▎    | 114/216 [00:05<00:04, 21.66it/s]\u001b[A\n",
      " 54%|█████▍    | 117/216 [00:06<00:04, 21.15it/s]\u001b[A\n",
      " 56%|█████▌    | 120/216 [00:06<00:04, 20.31it/s]\u001b[A\n",
      " 57%|█████▋    | 123/216 [00:06<00:04, 20.68it/s]\u001b[A\n",
      " 58%|█████▊    | 126/216 [00:06<00:04, 20.16it/s]\u001b[A\n",
      " 60%|█████▉    | 129/216 [00:06<00:04, 18.51it/s]\u001b[A\n",
      " 61%|██████    | 131/216 [00:06<00:04, 18.66it/s]\u001b[A\n",
      " 62%|██████▏   | 133/216 [00:06<00:04, 18.10it/s]\u001b[A\n",
      " 62%|██████▎   | 135/216 [00:07<00:04, 17.95it/s]\u001b[A\n",
      " 64%|██████▍   | 138/216 [00:07<00:04, 18.48it/s]\u001b[A\n",
      " 65%|██████▍   | 140/216 [00:07<00:04, 16.60it/s]\u001b[A\n",
      " 66%|██████▌   | 142/216 [00:07<00:04, 16.45it/s]\u001b[A\n",
      " 67%|██████▋   | 144/216 [00:07<00:04, 16.45it/s]\u001b[A\n",
      " 68%|██████▊   | 146/216 [00:07<00:04, 16.61it/s]\u001b[A\n",
      " 69%|██████▉   | 149/216 [00:07<00:03, 18.66it/s]\u001b[A\n",
      " 70%|██████▉   | 151/216 [00:07<00:03, 17.98it/s]\u001b[A\n",
      " 71%|███████   | 153/216 [00:08<00:03, 18.08it/s]\u001b[A\n",
      " 72%|███████▏  | 156/216 [00:08<00:03, 18.24it/s]\u001b[A\n",
      " 74%|███████▎  | 159/216 [00:08<00:02, 19.81it/s]\u001b[A\n",
      " 75%|███████▌  | 162/216 [00:08<00:02, 21.10it/s]\u001b[A\n",
      " 76%|███████▋  | 165/216 [00:08<00:02, 21.75it/s]\u001b[A\n",
      " 78%|███████▊  | 168/216 [00:08<00:02, 21.25it/s]\u001b[A\n",
      " 79%|███████▉  | 171/216 [00:08<00:02, 19.14it/s]\u001b[A\n",
      " 81%|████████  | 174/216 [00:09<00:02, 18.82it/s]\u001b[A\n",
      " 81%|████████▏ | 176/216 [00:09<00:02, 18.31it/s]\u001b[A\n",
      " 83%|████████▎ | 179/216 [00:09<00:01, 19.89it/s]\u001b[A\n",
      " 84%|████████▍ | 182/216 [00:09<00:01, 20.93it/s]\u001b[A\n",
      " 86%|████████▌ | 185/216 [00:09<00:01, 21.86it/s]\u001b[A\n",
      " 87%|████████▋ | 188/216 [00:09<00:01, 20.46it/s]\u001b[A\n",
      " 88%|████████▊ | 191/216 [00:09<00:01, 20.54it/s]\u001b[A\n",
      " 90%|████████▉ | 194/216 [00:10<00:01, 20.36it/s]\u001b[A\n",
      " 91%|█████████ | 197/216 [00:10<00:00, 21.15it/s]\u001b[A\n",
      " 93%|█████████▎| 200/216 [00:10<00:00, 21.92it/s]\u001b[A\n",
      " 94%|█████████▍| 203/216 [00:10<00:00, 22.61it/s]\u001b[A\n",
      " 95%|█████████▌| 206/216 [00:10<00:00, 21.34it/s]\u001b[A\n",
      " 97%|█████████▋| 209/216 [00:10<00:00, 20.78it/s]\u001b[A\n",
      " 98%|█████████▊| 212/216 [00:10<00:00, 21.53it/s]\u001b[A\n",
      "100%|██████████| 216/216 [00:11<00:00, 19.45it/s]\u001b[A\n",
      "100%|██████████| 10/10 [01:55<00:00, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.14943027496338\n",
      "0.053309921423594156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for j in tqdm(range(10)):\n",
    "    for i in tqdm(range(216)):\n",
    "        prediction = model.predict([np.expand_dims(x_test1[i], axis=0), np.expand_dims(x_test2[i], axis=0), \n",
    "                                    np.expand_dims(x_test3[i], axis=0), np.expand_dims(x_test4[i], axis=0)], verbose=0)\n",
    "\n",
    "total = time.time() - start\n",
    "print(total)\n",
    "print(total/2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
